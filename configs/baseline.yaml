# Simplified Configuration - Less is More
# Philosophy: Smaller model, cleaner training, let the data speak

# Model architecture - SMALLER
model:
  nz: 100
  ngf: 256              # REDUCED from 512 - halves generator size (~900K params vs 3.6M)
  ndf: 64               # Keep discriminator the same
  nc: 3
  use_spectral_norm: true

# Data configuration
data:
  train_dir: "data/pokemon-dataset-1000/train"
  val_dir: "data/pokemon-dataset-1000/val"
  test_dir: "data/pokemon-dataset-1000/test"
  num_workers: 4
  augment: true         # Keep augmentation - it helps with limited data

# Training configuration - SIMPLER
training:
  seed: 42
  deterministic: false
  batch_size: 64        # Keep at 64 - stable
  epochs: 300           # More epochs since simpler model may need more time
  
  # Slightly lower, more balanced learning rates
  lr_g: 0.0002
  lr_d: 0.0002          # SAME as G - simpler, often works fine
  beta1: 0.5
  beta2: 0.999
  
  loss_type: lsgan      # Keep LSGAN - it's working fine
  label_smoothing: 0.1  # REDUCED from 0.2 - less smoothing needed
  one_sided_label_smoothing: true
  
  # REMOVE complex tricks
  d_steps_per_g_step: 1
  #grad_clip_g: 10.0     # DISABLED - let gradients flow naturally  
  #grad_clip_d: 10.0     # DISABLED
  
  # Simpler scheduler
  lr_scheduler:
    enabled: true
    type: step           # Simple step decay instead of plateau
    step_size: 50        # Decay every 50 epochs
    gamma: 0.5           # Halve LR
  
  # Early stopping - more patient
  early_stopping:
    enabled: true
    patience: 30         # Give it time
    min_delta: 1.0
  
  # Logging
  log_interval: 50
  save_interval: 5
  checkpoint_interval: 25
  val_samples: 64
  
  checkpoint_dir: "checkpoints_simple"
  output_dir: "outputs_simple"
  log_dir: "logs_simple"

