{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEQxwTGzZq5G"
      },
      "source": [
        "# PokéGAN Training Notebook\n",
        "**CSC 487 Final Project Stage 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi54RJcWZq5N"
      },
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35-C8cIHZq5O",
        "outputId": "92cf9672-1438-4aea-b8ac-bbb8fb82f6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Collecting torchmetrics[image]\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics[image]) (25.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics[image])\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: scipy>1.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics[image]) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-fidelity) (4.67.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, torch-fidelity\n",
            "Successfully installed lightning-utilities-0.15.2 torch-fidelity-0.3.0 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchmetrics[image] pyyaml matplotlib tensorboard torch-fidelity kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9wOrCvQZq5Q"
      },
      "source": [
        "## Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB-b2N3oZq5R",
        "outputId": "b230d9cb-ffec-412b-ec57-476a650a3c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository...\n",
            "Cloning into 'CSC487-Project'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 172 (delta 87), reused 141 (delta 57), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (172/172), 4.88 MiB | 19.06 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "/content/CSC487-Project\n",
            "Current directory: /content/CSC487-Project\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "repo_name = 'CSC487-Project'\n",
        "repo_url = 'https://github.com/BraedenAlonge/CSC487-Project.git'\n",
        "\n",
        "# Clone or pull repository\n",
        "if not os.path.exists(repo_name):\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(\"Repository already exists. Updating...\")\n",
        "    %cd {repo_name}\n",
        "    !git pull\n",
        "    %cd ..\n",
        "\n",
        "# Move into project directory\n",
        "if repo_name in os.listdir('.'):\n",
        "    %cd {repo_name}\n",
        "\n",
        "print(f\"Current directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewJJz4i2Zq5S"
      },
      "source": [
        "## Verify GPU for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4z3G28LZq5S",
        "outputId": "c198375b-f938-498e-de97-029e312bcff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 42.47 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCwbNS_4cL7Z"
      },
      "source": [
        "## Download Dataset (Kaggle Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "6B0SwYi-Zq5U",
        "outputId": "208be9a5-4e98-4ca6-a257-10e962696b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your kaggle.json file (from Kaggle Account -> API):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17d243dd-bf8b-470d-99b6-8caefab8ce15\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-17d243dd-bf8b-470d-99b6-8caefab8ce15\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading from Kaggle...\n",
            "Dataset URL: https://www.kaggle.com/datasets/noodulz/pokemon-dataset-1000\n",
            "License(s): CC0-1.0\n",
            "Downloading pokemon-dataset-1000.zip to /content/CSC487-Project\n",
            " 84% 661M/785M [00:00<00:00, 1.73GB/s]\n",
            "100% 785M/785M [00:00<00:00, 1.73GB/s]\n",
            "Extracting dataset...\n",
            "Organizing dataset...\n",
            "Found 53078 images total.\n",
            "Moving files to train/val/test folders...\n",
            "✓ Dataset prepared!\n",
            "  Train: 47770\n",
            "  Val: 2653\n",
            "  Test: 2655\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your kaggle.json file (from Kaggle Account -> API):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Setup Kaggle Auth\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/ 2>/dev/null\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Downloading from Kaggle...\")\n",
        "!kaggle datasets download -d noodulz/pokemon-dataset-1000 --force\n",
        "\n",
        "# Clean previous data\n",
        "if os.path.exists('data/pokemon-dataset-1000'):\n",
        "    shutil.rmtree('data/pokemon-dataset-1000')\n",
        "!mkdir -p data\n",
        "\n",
        "print(\"Extracting dataset...\")\n",
        "if os.path.exists('pokemon-dataset-1000.zip'):\n",
        "    # Unzip to a temporary location first to inspect structure\n",
        "    temp_extract_dir = 'data/temp_extract'\n",
        "    if os.path.exists(temp_extract_dir): shutil.rmtree(temp_extract_dir)\n",
        "    !unzip -q pokemon-dataset-1000.zip -d {temp_extract_dir}\n",
        "\n",
        "    print(\"Organizing dataset...\")\n",
        "    # Target directories\n",
        "    base_data_dir = 'data/pokemon-dataset-1000'\n",
        "    train_dir = os.path.join(base_data_dir, 'train')\n",
        "    val_dir = os.path.join(base_data_dir, 'val')\n",
        "    test_dir = os.path.join(base_data_dir, 'test')\n",
        "\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Find ALL images recursively\n",
        "    all_images = []\n",
        "    for root, dirs, files in os.walk(temp_extract_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                all_images.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(all_images)} images total.\")\n",
        "\n",
        "    # Shuffle and Split\n",
        "    random.shuffle(all_images)\n",
        "    train_split = int(0.9 * len(all_images))\n",
        "    val_split = int(0.05 * len(all_images))\n",
        "\n",
        "    train_imgs = all_images[:train_split]\n",
        "    val_imgs = all_images[train_split:train_split+val_split]\n",
        "    test_imgs = all_images[train_split+val_split:]\n",
        "\n",
        "    print(\"Moving files to train/val/test folders...\")\n",
        "    # Helper to move files\n",
        "    def move_files(file_list, target_folder):\n",
        "        for src in file_list:\n",
        "            dst = os.path.join(target_folder, os.path.basename(src))\n",
        "            # Handle duplicate filenames if flattened\n",
        "            if os.path.exists(dst):\n",
        "                base, ext = os.path.splitext(os.path.basename(src))\n",
        "                dst = os.path.join(target_folder, f\"{base}_{random.randint(0,9999)}{ext}\")\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "    move_files(train_imgs, train_dir)\n",
        "    move_files(val_imgs, val_dir)\n",
        "    move_files(test_imgs, test_dir)\n",
        "\n",
        "    # Cleanup temp\n",
        "    shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "    print(f\"✓ Dataset prepared!\")\n",
        "    print(f\"  Train: {len(os.listdir(train_dir))}\")\n",
        "    print(f\"  Val: {len(os.listdir(val_dir))}\")\n",
        "    print(f\"  Test: {len(os.listdir(test_dir))}\")\n",
        "else:\n",
        "    print(\"Error: pokemon-dataset-1000.zip not found! Upload failed...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1v1_hl1Zq5X"
      },
      "source": [
        "## Training the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiQP-sFbZq5X",
        "outputId": "632d7747-19ce-46d4-85c0-dd2bf042a743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-10 00:48:34.228268: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-10 00:48:34.245363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765327714.267465    4309 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765327714.274121    4309 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765327714.291156    4309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765327714.291196    4309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765327714.291198    4309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765327714.291201    4309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-10 00:48:34.296185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Set random seed to 42.\n",
            "Using device: cuda\n",
            "Generator parameters: 3,605,457\n",
            "Discriminator parameters: 2,768,978\n",
            "Found 47770 images in data/pokemon-dataset-1000/train\n",
            "Found 2653 images in data/pokemon-dataset-1000/val\n",
            "Training samples: 47770\n",
            "Validation samples: 2653\n",
            "Using BCE loss\n",
            "Starting training...\n",
            "Epoch [0/200] Batch [0/374] Loss_D: 1.4057 Loss_G: 0.6477 D(x): 0.4836 D(G(z)): 0.5000/0.5232\n",
            "Epoch [0/200] Batch [50/374] Loss_D: 1.2635 Loss_G: 1.1008 D(x): 0.8586 D(G(z)): 0.4991/0.3328\n",
            "Epoch [0/200] Batch [100/374] Loss_D: 1.2156 Loss_G: 1.1141 D(x): 0.7433 D(G(z)): 0.3864/0.3282\n",
            "Epoch [0/200] Batch [150/374] Loss_D: 1.2424 Loss_G: 0.6849 D(x): 0.7282 D(G(z)): 0.5044/0.5041\n",
            "Epoch [0/200] Batch [200/374] Loss_D: 1.1399 Loss_G: 1.1395 D(x): 0.7025 D(G(z)): 0.2596/0.3200\n",
            "Epoch [0/200] Batch [250/374] Loss_D: 1.1339 Loss_G: 1.4001 D(x): 0.7375 D(G(z)): 0.2974/0.2468\n",
            "Epoch [0/200] Batch [300/374] Loss_D: 1.1922 Loss_G: 0.8433 D(x): 0.6353 D(G(z)): 0.1394/0.4303\n",
            "Epoch [0/200] Batch [350/374] Loss_D: 1.2752 Loss_G: 0.8938 D(x): 0.7139 D(G(z)): 0.5012/0.4091\n",
            "Calculating FID...\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:00<00:00, 183MB/s]\n",
            "Epoch [0/200] Val Loss_D: 1.1222 Val Loss_G: 1.0182 FID: 435.1602\n",
            "Saved checkpoint to checkpoints/epoch_0_model_best.pt\n",
            "New best FID: 435.1602 at epoch 0\n",
            "Saved image grid to outputs/epoch_0_fake.png.\n",
            "Saved image grid to outputs/epoch_0_real.png.\n",
            "Epoch [1/200] Batch [0/374] Loss_D: 1.1493 Loss_G: 1.0998 D(x): 0.6488 D(G(z)): 0.3570/0.3330\n",
            "Epoch [1/200] Batch [50/374] Loss_D: 1.1531 Loss_G: 1.2405 D(x): 0.7213 D(G(z)): 0.2407/0.2893\n",
            "Epoch [1/200] Batch [100/374] Loss_D: 1.1469 Loss_G: 1.1067 D(x): 0.6763 D(G(z)): 0.2851/0.3307\n",
            "Epoch [1/200] Batch [150/374] Loss_D: 1.1163 Loss_G: 1.5454 D(x): 0.7548 D(G(z)): 0.3260/0.2139\n",
            "Epoch [1/200] Batch [200/374] Loss_D: 1.2379 Loss_G: 1.2813 D(x): 0.6530 D(G(z)): 0.4145/0.2784\n",
            "Epoch [1/200] Batch [250/374] Loss_D: 1.1955 Loss_G: 1.1403 D(x): 0.6049 D(G(z)): 0.3061/0.3221\n",
            "Epoch [1/200] Batch [300/374] Loss_D: 1.1805 Loss_G: 1.4451 D(x): 0.6556 D(G(z)): 0.3472/0.2383\n",
            "Epoch [1/200] Batch [350/374] Loss_D: 1.2964 Loss_G: 1.1478 D(x): 0.5494 D(G(z)): 0.3440/0.3180\n",
            "Calculating FID...\n",
            "Epoch [1/200] Val Loss_D: 1.2048 Val Loss_G: 1.2289 FID: 364.1451\n",
            "Saved checkpoint to checkpoints/epoch_1_model_best.pt\n",
            "New best FID: 364.1451 at epoch 1\n",
            "Saved image grid to outputs/epoch_1_fake.png.\n",
            "Saved image grid to outputs/epoch_1_real.png.\n",
            "Epoch [2/200] Batch [0/374] Loss_D: 1.3269 Loss_G: 0.7421 D(x): 0.5826 D(G(z)): 0.2953/0.4780\n",
            "Epoch [2/200] Batch [50/374] Loss_D: 1.2724 Loss_G: 0.9693 D(x): 0.6325 D(G(z)): 0.4588/0.3814\n",
            "Epoch [2/200] Batch [100/374] Loss_D: 1.3041 Loss_G: 1.3504 D(x): 0.6063 D(G(z)): 0.4678/0.2748\n",
            "Epoch [2/200] Batch [150/374] Loss_D: 1.3004 Loss_G: 1.1287 D(x): 0.6077 D(G(z)): 0.3573/0.3241\n",
            "Epoch [2/200] Batch [200/374] Loss_D: 1.1700 Loss_G: 1.2412 D(x): 0.6795 D(G(z)): 0.3816/0.2920\n",
            "Epoch [2/200] Batch [250/374] Loss_D: 1.2467 Loss_G: 1.1538 D(x): 0.6174 D(G(z)): 0.2962/0.3186\n",
            "Epoch [2/200] Batch [300/374] Loss_D: 1.1943 Loss_G: 1.3724 D(x): 0.6836 D(G(z)): 0.3853/0.2545\n",
            "Epoch [2/200] Batch [350/374] Loss_D: 1.2736 Loss_G: 1.1719 D(x): 0.5947 D(G(z)): 0.3761/0.3236\n",
            "Calculating FID...\n",
            "Epoch [2/200] Val Loss_D: 1.2142 Val Loss_G: 1.1533 FID: 322.8440\n",
            "Saved checkpoint to checkpoints/epoch_2_model_best.pt\n",
            "New best FID: 322.8440 at epoch 2\n",
            "Saved image grid to outputs/epoch_2_fake.png.\n",
            "Saved image grid to outputs/epoch_2_real.png.\n",
            "Epoch [3/200] Batch [0/374] Loss_D: 1.3124 Loss_G: 1.0728 D(x): 0.5721 D(G(z)): 0.3268/0.3432\n",
            "Epoch [3/200] Batch [50/374] Loss_D: 1.2511 Loss_G: 1.4148 D(x): 0.6802 D(G(z)): 0.4254/0.2454\n",
            "Epoch [3/200] Batch [100/374] Loss_D: 1.3043 Loss_G: 1.0512 D(x): 0.5878 D(G(z)): 0.4320/0.3570\n",
            "Epoch [3/200] Batch [150/374] Loss_D: 1.2536 Loss_G: 1.2063 D(x): 0.6150 D(G(z)): 0.4047/0.3049\n",
            "Epoch [3/200] Batch [200/374] Loss_D: 1.2595 Loss_G: 1.1680 D(x): 0.6207 D(G(z)): 0.3974/0.3179\n",
            "Epoch [3/200] Batch [250/374] Loss_D: 1.3063 Loss_G: 0.9135 D(x): 0.5577 D(G(z)): 0.3812/0.4031\n",
            "Epoch [3/200] Batch [300/374] Loss_D: 1.2485 Loss_G: 1.1238 D(x): 0.6461 D(G(z)): 0.3692/0.3287\n",
            "Epoch [3/200] Batch [350/374] Loss_D: 1.2530 Loss_G: 1.0827 D(x): 0.5978 D(G(z)): 0.4064/0.3412\n",
            "Calculating FID...\n",
            "Epoch [3/200] Val Loss_D: 1.2875 Val Loss_G: 0.7111 FID: 223.8942\n",
            "Saved checkpoint to checkpoints/epoch_3_model_best.pt\n",
            "New best FID: 223.8942 at epoch 3\n",
            "Saved image grid to outputs/epoch_3_fake.png.\n",
            "Saved image grid to outputs/epoch_3_real.png.\n",
            "Epoch [4/200] Batch [0/374] Loss_D: 1.2985 Loss_G: 1.0886 D(x): 0.6288 D(G(z)): 0.4822/0.3385\n",
            "Epoch [4/200] Batch [50/374] Loss_D: 1.2701 Loss_G: 1.0189 D(x): 0.5828 D(G(z)): 0.3855/0.3668\n",
            "Epoch [4/200] Batch [100/374] Loss_D: 1.2534 Loss_G: 1.2712 D(x): 0.5666 D(G(z)): 0.3735/0.2853\n",
            "Epoch [4/200] Batch [150/374] Loss_D: 1.2851 Loss_G: 1.1352 D(x): 0.5728 D(G(z)): 0.4128/0.3244\n",
            "Epoch [4/200] Batch [200/374] Loss_D: 1.2378 Loss_G: 1.2496 D(x): 0.6111 D(G(z)): 0.4106/0.2906\n",
            "Epoch [4/200] Batch [250/374] Loss_D: 1.1828 Loss_G: 1.1352 D(x): 0.6012 D(G(z)): 0.2953/0.3282\n",
            "Epoch [4/200] Batch [300/374] Loss_D: 1.2282 Loss_G: 1.2186 D(x): 0.6500 D(G(z)): 0.3998/0.3000\n",
            "Epoch [4/200] Batch [350/374] Loss_D: 1.2807 Loss_G: 1.3121 D(x): 0.5986 D(G(z)): 0.3688/0.2713\n",
            "Calculating FID...\n",
            "Epoch [4/200] Val Loss_D: 1.1823 Val Loss_G: 1.2528 FID: 242.1703\n",
            "Epoch [5/200] Batch [0/374] Loss_D: 1.2374 Loss_G: 1.0388 D(x): 0.5712 D(G(z)): 0.3020/0.3564\n",
            "Epoch [5/200] Batch [50/374] Loss_D: 1.2755 Loss_G: 1.7124 D(x): 0.6722 D(G(z)): 0.4416/0.1849\n",
            "Epoch [5/200] Batch [100/374] Loss_D: 1.2489 Loss_G: 1.1107 D(x): 0.6302 D(G(z)): 0.3466/0.3322\n",
            "Epoch [5/200] Batch [150/374] Loss_D: 1.1599 Loss_G: 1.3886 D(x): 0.7003 D(G(z)): 0.3680/0.2566\n",
            "Epoch [5/200] Batch [200/374] Loss_D: 1.2525 Loss_G: 1.7346 D(x): 0.7192 D(G(z)): 0.4670/0.1803\n",
            "Epoch [5/200] Batch [250/374] Loss_D: 1.3089 Loss_G: 1.2648 D(x): 0.6555 D(G(z)): 0.5096/0.2875\n",
            "Epoch [5/200] Batch [300/374] Loss_D: 1.2626 Loss_G: 1.5674 D(x): 0.6893 D(G(z)): 0.4588/0.2154\n",
            "Epoch [5/200] Batch [350/374] Loss_D: 1.2713 Loss_G: 1.1275 D(x): 0.5945 D(G(z)): 0.4171/0.3291\n",
            "Calculating FID...\n",
            "Epoch [5/200] Val Loss_D: 1.1711 Val Loss_G: 0.8353 FID: 251.5190\n",
            "Epoch [6/200] Batch [0/374] Loss_D: 1.2530 Loss_G: 1.2300 D(x): 0.7337 D(G(z)): 0.4427/0.2961\n",
            "Epoch [6/200] Batch [50/374] Loss_D: 1.1742 Loss_G: 1.4094 D(x): 0.6447 D(G(z)): 0.3428/0.2494\n",
            "Epoch [6/200] Batch [100/374] Loss_D: 1.2508 Loss_G: 1.0380 D(x): 0.5761 D(G(z)): 0.3771/0.3577\n",
            "Epoch [6/200] Batch [150/374] Loss_D: 1.2643 Loss_G: 1.0702 D(x): 0.5966 D(G(z)): 0.4177/0.3483\n",
            "Epoch [6/200] Batch [200/374] Loss_D: 1.2314 Loss_G: 1.1035 D(x): 0.6195 D(G(z)): 0.3522/0.3368\n",
            "Epoch [6/200] Batch [250/374] Loss_D: 1.2942 Loss_G: 1.1530 D(x): 0.6133 D(G(z)): 0.4552/0.3221\n",
            "Epoch [6/200] Batch [300/374] Loss_D: 1.2739 Loss_G: 1.0510 D(x): 0.5477 D(G(z)): 0.3259/0.3574\n",
            "Epoch [6/200] Batch [350/374] Loss_D: 1.2954 Loss_G: 1.3387 D(x): 0.6087 D(G(z)): 0.3898/0.2701\n",
            "Calculating FID...\n",
            "Epoch [6/200] Val Loss_D: 1.1867 Val Loss_G: 1.0133 FID: 253.0326\n",
            "Epoch [7/200] Batch [0/374] Loss_D: 1.2662 Loss_G: 1.1577 D(x): 0.6181 D(G(z)): 0.3855/0.3196\n",
            "Epoch [7/200] Batch [50/374] Loss_D: 1.2927 Loss_G: 0.8351 D(x): 0.5225 D(G(z)): 0.3281/0.4422\n",
            "Epoch [7/200] Batch [100/374] Loss_D: 1.2979 Loss_G: 1.0501 D(x): 0.5436 D(G(z)): 0.4116/0.3554\n",
            "Epoch [7/200] Batch [150/374] Loss_D: 1.2597 Loss_G: 1.0758 D(x): 0.6002 D(G(z)): 0.4086/0.3501\n",
            "Epoch [7/200] Batch [200/374] Loss_D: 1.2727 Loss_G: 1.2006 D(x): 0.6023 D(G(z)): 0.4344/0.3102\n",
            "Epoch [7/200] Batch [250/374] Loss_D: 1.2994 Loss_G: 1.6764 D(x): 0.6761 D(G(z)): 0.4993/0.1948\n",
            "Epoch [7/200] Batch [300/374] Loss_D: 1.2606 Loss_G: 1.2488 D(x): 0.6215 D(G(z)): 0.4196/0.2925\n",
            "Epoch [7/200] Batch [350/374] Loss_D: 1.2535 Loss_G: 0.9309 D(x): 0.5620 D(G(z)): 0.3637/0.3985\n",
            "Calculating FID...\n",
            "Epoch [7/200] Val Loss_D: 1.2073 Val Loss_G: 0.8229 FID: 220.4771\n",
            "Saved checkpoint to checkpoints/epoch_7_model_best.pt\n",
            "New best FID: 220.4771 at epoch 7\n",
            "Saved image grid to outputs/epoch_7_fake.png.\n",
            "Saved image grid to outputs/epoch_7_real.png.\n",
            "Epoch [8/200] Batch [0/374] Loss_D: 1.2169 Loss_G: 1.2172 D(x): 0.6430 D(G(z)): 0.4309/0.2981\n",
            "Epoch [8/200] Batch [50/374] Loss_D: 1.2887 Loss_G: 0.9306 D(x): 0.5730 D(G(z)): 0.4240/0.3982\n",
            "Epoch [8/200] Batch [100/374] Loss_D: 1.2657 Loss_G: 1.2549 D(x): 0.5993 D(G(z)): 0.3985/0.2909\n",
            "Epoch [8/200] Batch [150/374] Loss_D: 1.2682 Loss_G: 1.4160 D(x): 0.6852 D(G(z)): 0.4750/0.2480\n",
            "Epoch [8/200] Batch [200/374] Loss_D: 1.2571 Loss_G: 1.0254 D(x): 0.6369 D(G(z)): 0.4052/0.3620\n",
            "Epoch [8/200] Batch [250/374] Loss_D: 1.2947 Loss_G: 0.8708 D(x): 0.4900 D(G(z)): 0.2839/0.4228\n",
            "Epoch [8/200] Batch [300/374] Loss_D: 1.3169 Loss_G: 1.0399 D(x): 0.5799 D(G(z)): 0.4255/0.3579\n",
            "Epoch [8/200] Batch [350/374] Loss_D: 1.2710 Loss_G: 1.2557 D(x): 0.6171 D(G(z)): 0.4303/0.2872\n",
            "Calculating FID...\n",
            "Epoch [8/200] Val Loss_D: 1.2006 Val Loss_G: 1.3663 FID: 257.8637\n",
            "Epoch [9/200] Batch [0/374] Loss_D: 1.2084 Loss_G: 1.2679 D(x): 0.5852 D(G(z)): 0.2554/0.2909\n",
            "Epoch [9/200] Batch [50/374] Loss_D: 1.3100 Loss_G: 1.5538 D(x): 0.6397 D(G(z)): 0.5097/0.2195\n",
            "Epoch [9/200] Batch [100/374] Loss_D: 1.2998 Loss_G: 1.5502 D(x): 0.6679 D(G(z)): 0.4561/0.2166\n",
            "Epoch [9/200] Batch [150/374] Loss_D: 1.2319 Loss_G: 1.2802 D(x): 0.6787 D(G(z)): 0.4280/0.2907\n",
            "Epoch [9/200] Batch [200/374] Loss_D: 1.2623 Loss_G: 1.0841 D(x): 0.5834 D(G(z)): 0.3603/0.3428\n",
            "Epoch [9/200] Batch [250/374] Loss_D: 1.2001 Loss_G: 1.3652 D(x): 0.6333 D(G(z)): 0.3701/0.2601\n",
            "Epoch [9/200] Batch [300/374] Loss_D: 1.2791 Loss_G: 1.2631 D(x): 0.6036 D(G(z)): 0.3892/0.2879\n",
            "Epoch [9/200] Batch [350/374] Loss_D: 1.2499 Loss_G: 1.1445 D(x): 0.5912 D(G(z)): 0.3996/0.3213\n",
            "Calculating FID...\n",
            "Epoch [9/200] Val Loss_D: 1.2124 Val Loss_G: 1.0479 FID: 238.7716\n",
            "Epoch [10/200] Batch [0/374] Loss_D: 1.2538 Loss_G: 0.9711 D(x): 0.5604 D(G(z)): 0.3500/0.3919\n",
            "Epoch [10/200] Batch [50/374] Loss_D: 1.2258 Loss_G: 1.1032 D(x): 0.5938 D(G(z)): 0.3358/0.3335\n",
            "Epoch [10/200] Batch [100/374] Loss_D: 1.2175 Loss_G: 1.1124 D(x): 0.6277 D(G(z)): 0.4101/0.3322\n",
            "Epoch [10/200] Batch [150/374] Loss_D: 1.2986 Loss_G: 1.0705 D(x): 0.5584 D(G(z)): 0.3817/0.3458\n",
            "Epoch [10/200] Batch [200/374] Loss_D: 1.2303 Loss_G: 1.1535 D(x): 0.6384 D(G(z)): 0.3941/0.3188\n",
            "Epoch [10/200] Batch [250/374] Loss_D: 1.2830 Loss_G: 1.4334 D(x): 0.6031 D(G(z)): 0.4296/0.2429\n",
            "Epoch [10/200] Batch [300/374] Loss_D: 1.2996 Loss_G: 1.3069 D(x): 0.6338 D(G(z)): 0.4603/0.2745\n",
            "Epoch [10/200] Batch [350/374] Loss_D: 1.2735 Loss_G: 0.8005 D(x): 0.5550 D(G(z)): 0.3202/0.4541\n",
            "Calculating FID...\n",
            "Epoch [10/200] Val Loss_D: 1.2784 Val Loss_G: 1.0838 FID: 259.4381\n",
            "Epoch [11/200] Batch [0/374] Loss_D: 1.3077 Loss_G: 0.7475 D(x): 0.5282 D(G(z)): 0.3466/0.4774\n",
            "Epoch [11/200] Batch [50/374] Loss_D: 1.2837 Loss_G: 1.0300 D(x): 0.5681 D(G(z)): 0.3514/0.3621\n",
            "Epoch [11/200] Batch [100/374] Loss_D: 1.2322 Loss_G: 1.1567 D(x): 0.6269 D(G(z)): 0.4092/0.3243\n",
            "Epoch [11/200] Batch [150/374] Loss_D: 1.2528 Loss_G: 1.1389 D(x): 0.6703 D(G(z)): 0.3537/0.3274\n",
            "Epoch [11/200] Batch [200/374] Loss_D: 1.2192 Loss_G: 1.1784 D(x): 0.6230 D(G(z)): 0.3952/0.3220\n",
            "Epoch [11/200] Batch [250/374] Loss_D: 1.2553 Loss_G: 1.1505 D(x): 0.5839 D(G(z)): 0.3609/0.3191\n",
            "Epoch [11/200] Batch [300/374] Loss_D: 1.3148 Loss_G: 1.2717 D(x): 0.5903 D(G(z)): 0.4435/0.2862\n",
            "Epoch [11/200] Batch [350/374] Loss_D: 1.2566 Loss_G: 0.9976 D(x): 0.6157 D(G(z)): 0.3647/0.3737\n",
            "Calculating FID...\n",
            "Epoch [11/200] Val Loss_D: 1.2038 Val Loss_G: 1.1824 FID: 244.1044\n",
            "Epoch [12/200] Batch [0/374] Loss_D: 1.2489 Loss_G: 0.9484 D(x): 0.5638 D(G(z)): 0.3048/0.3913\n",
            "Epoch [12/200] Batch [50/374] Loss_D: 1.2623 Loss_G: 1.2321 D(x): 0.5780 D(G(z)): 0.3774/0.2982\n",
            "Epoch [12/200] Batch [100/374] Loss_D: 1.2301 Loss_G: 1.2329 D(x): 0.6288 D(G(z)): 0.3439/0.2946\n",
            "Epoch [12/200] Batch [150/374] Loss_D: 1.2677 Loss_G: 1.2945 D(x): 0.5643 D(G(z)): 0.3518/0.2778\n",
            "Epoch [12/200] Batch [200/374] Loss_D: 1.2910 Loss_G: 1.3022 D(x): 0.5989 D(G(z)): 0.4457/0.2757\n",
            "Epoch [12/200] Batch [250/374] Loss_D: 1.2084 Loss_G: 1.1075 D(x): 0.5627 D(G(z)): 0.3055/0.3358\n",
            "Epoch [12/200] Batch [300/374] Loss_D: 1.2893 Loss_G: 0.9923 D(x): 0.5832 D(G(z)): 0.3717/0.3749\n",
            "Epoch [12/200] Batch [350/374] Loss_D: 1.2914 Loss_G: 1.0250 D(x): 0.5783 D(G(z)): 0.4453/0.3638\n",
            "Calculating FID...\n",
            "Epoch [12/200] Val Loss_D: 1.1943 Val Loss_G: 0.8872 FID: 259.4808\n",
            "Epoch [13/200] Batch [0/374] Loss_D: 1.2167 Loss_G: 1.5926 D(x): 0.6361 D(G(z)): 0.4145/0.2071\n",
            "Epoch [13/200] Batch [50/374] Loss_D: 1.2795 Loss_G: 1.1344 D(x): 0.6277 D(G(z)): 0.4509/0.3249\n",
            "Epoch [13/200] Batch [100/374] Loss_D: 1.2535 Loss_G: 1.2195 D(x): 0.5625 D(G(z)): 0.3450/0.3011\n",
            "Epoch [13/200] Batch [150/374] Loss_D: 1.2025 Loss_G: 1.2570 D(x): 0.6687 D(G(z)): 0.3952/0.2910\n",
            "Epoch [13/200] Batch [200/374] Loss_D: 1.3526 Loss_G: 1.0944 D(x): 0.6839 D(G(z)): 0.5693/0.3383\n",
            "Epoch [13/200] Batch [250/374] Loss_D: 1.2544 Loss_G: 1.1523 D(x): 0.5949 D(G(z)): 0.3902/0.3208\n",
            "Epoch [13/200] Batch [300/374] Loss_D: 1.3317 Loss_G: 0.9660 D(x): 0.5301 D(G(z)): 0.3854/0.3857\n",
            "Epoch [13/200] Batch [350/374] Loss_D: 1.2620 Loss_G: 1.1070 D(x): 0.7149 D(G(z)): 0.4723/0.3364\n",
            "Calculating FID...\n",
            "Epoch [13/200] Val Loss_D: 1.2682 Val Loss_G: 1.2367 FID: 251.1824\n",
            "Epoch [14/200] Batch [0/374] Loss_D: 1.3039 Loss_G: 0.8526 D(x): 0.5098 D(G(z)): 0.3094/0.4322\n",
            "Epoch [14/200] Batch [50/374] Loss_D: 1.2989 Loss_G: 0.9910 D(x): 0.5343 D(G(z)): 0.3667/0.3752\n",
            "Epoch [14/200] Batch [100/374] Loss_D: 1.2326 Loss_G: 1.4805 D(x): 0.6752 D(G(z)): 0.4432/0.2323\n",
            "Epoch [14/200] Batch [150/374] Loss_D: 1.2743 Loss_G: 1.1174 D(x): 0.5421 D(G(z)): 0.3900/0.3301\n",
            "Epoch [14/200] Batch [200/374] Loss_D: 1.2707 Loss_G: 1.2213 D(x): 0.6460 D(G(z)): 0.4702/0.3139\n",
            "Epoch [14/200] Batch [250/374] Loss_D: 1.2668 Loss_G: 1.0007 D(x): 0.5533 D(G(z)): 0.3726/0.3773\n",
            "Epoch [14/200] Batch [300/374] Loss_D: 1.2306 Loss_G: 1.3341 D(x): 0.6733 D(G(z)): 0.4023/0.2717\n",
            "Epoch [14/200] Batch [350/374] Loss_D: 1.3153 Loss_G: 0.9261 D(x): 0.5389 D(G(z)): 0.3535/0.4006\n",
            "Calculating FID...\n",
            "Epoch [14/200] Val Loss_D: 1.1788 Val Loss_G: 0.9342 FID: 250.0416\n",
            "Epoch [15/200] Batch [0/374] Loss_D: 1.2158 Loss_G: 1.4125 D(x): 0.6652 D(G(z)): 0.3888/0.2502\n",
            "Epoch [15/200] Batch [50/374] Loss_D: 1.2339 Loss_G: 1.2261 D(x): 0.6605 D(G(z)): 0.4514/0.2990\n",
            "Epoch [15/200] Batch [100/374] Loss_D: 1.3129 Loss_G: 1.0688 D(x): 0.5532 D(G(z)): 0.4373/0.3473\n",
            "Epoch [15/200] Batch [150/374] Loss_D: 1.2518 Loss_G: 1.2270 D(x): 0.6224 D(G(z)): 0.4059/0.2964\n",
            "Epoch [15/200] Batch [200/374] Loss_D: 1.1972 Loss_G: 1.1474 D(x): 0.5951 D(G(z)): 0.3267/0.3285\n",
            "Epoch [15/200] Batch [250/374] Loss_D: 1.2767 Loss_G: 1.0973 D(x): 0.6598 D(G(z)): 0.4536/0.3390\n",
            "Epoch [15/200] Batch [300/374] Loss_D: 1.2318 Loss_G: 1.2631 D(x): 0.6327 D(G(z)): 0.3868/0.2882\n",
            "Epoch [15/200] Batch [350/374] Loss_D: 1.2474 Loss_G: 1.1250 D(x): 0.6271 D(G(z)): 0.4345/0.3297\n",
            "Calculating FID...\n",
            "Epoch [15/200] Val Loss_D: 1.2083 Val Loss_G: 0.8766 FID: 228.0339\n",
            "Epoch [16/200] Batch [0/374] Loss_D: 1.2445 Loss_G: 1.2354 D(x): 0.6338 D(G(z)): 0.4185/0.2971\n",
            "Epoch [16/200] Batch [50/374] Loss_D: 1.2796 Loss_G: 0.9731 D(x): 0.5593 D(G(z)): 0.4051/0.3835\n",
            "Epoch [16/200] Batch [100/374] Loss_D: 1.2987 Loss_G: 0.8167 D(x): 0.5238 D(G(z)): 0.3334/0.4503\n",
            "Epoch [16/200] Batch [150/374] Loss_D: 1.2476 Loss_G: 1.0932 D(x): 0.5979 D(G(z)): 0.4211/0.3390\n",
            "Epoch [16/200] Batch [200/374] Loss_D: 1.2421 Loss_G: 1.0508 D(x): 0.6245 D(G(z)): 0.3999/0.3526\n",
            "Epoch [16/200] Batch [250/374] Loss_D: 1.2043 Loss_G: 1.1764 D(x): 0.6504 D(G(z)): 0.4016/0.3140\n",
            "Epoch [16/200] Batch [300/374] Loss_D: 1.2797 Loss_G: 1.1719 D(x): 0.5597 D(G(z)): 0.3866/0.3138\n",
            "Epoch [16/200] Batch [350/374] Loss_D: 1.2295 Loss_G: 1.3318 D(x): 0.6389 D(G(z)): 0.3966/0.2707\n",
            "Calculating FID...\n",
            "Epoch [16/200] Val Loss_D: 1.2395 Val Loss_G: 0.8952 FID: 239.7241\n",
            "Epoch [17/200] Batch [0/374] Loss_D: 1.2982 Loss_G: 1.3107 D(x): 0.5753 D(G(z)): 0.4101/0.2742\n",
            "Epoch [17/200] Batch [50/374] Loss_D: 1.2202 Loss_G: 1.1966 D(x): 0.6336 D(G(z)): 0.4028/0.3074\n",
            "Epoch [17/200] Batch [100/374] Loss_D: 1.2430 Loss_G: 1.1209 D(x): 0.5858 D(G(z)): 0.3593/0.3323\n",
            "Epoch [17/200] Batch [150/374] Loss_D: 1.2671 Loss_G: 1.3382 D(x): 0.6470 D(G(z)): 0.4519/0.2651\n",
            "Epoch [17/200] Batch [200/374] Loss_D: 1.2297 Loss_G: 1.0827 D(x): 0.6366 D(G(z)): 0.4045/0.3421\n",
            "Epoch [17/200] Batch [250/374] Loss_D: 1.2533 Loss_G: 1.0819 D(x): 0.6325 D(G(z)): 0.4096/0.3458\n",
            "Epoch [17/200] Batch [300/374] Loss_D: 1.3048 Loss_G: 0.9691 D(x): 0.5219 D(G(z)): 0.3982/0.3840\n",
            "Epoch [17/200] Batch [350/374] Loss_D: 1.2384 Loss_G: 1.0495 D(x): 0.6064 D(G(z)): 0.4050/0.3538\n",
            "Calculating FID...\n",
            "Epoch [17/200] Val Loss_D: 1.2729 Val Loss_G: 0.7379 FID: 239.4305\n",
            "Epoch [18/200] Batch [0/374] Loss_D: 1.2978 Loss_G: 1.1751 D(x): 0.6315 D(G(z)): 0.4867/0.3125\n",
            "Epoch [18/200] Batch [50/374] Loss_D: 1.2845 Loss_G: 0.9684 D(x): 0.6051 D(G(z)): 0.4260/0.3844\n",
            "Epoch [18/200] Batch [100/374] Loss_D: 1.2136 Loss_G: 1.0889 D(x): 0.5749 D(G(z)): 0.3490/0.3405\n",
            "Epoch [18/200] Batch [150/374] Loss_D: 1.3170 Loss_G: 1.1688 D(x): 0.6172 D(G(z)): 0.4388/0.3161\n",
            "Epoch [18/200] Batch [200/374] Loss_D: 1.2743 Loss_G: 1.0361 D(x): 0.6147 D(G(z)): 0.4136/0.3657\n",
            "Epoch [18/200] Batch [250/374] Loss_D: 1.2305 Loss_G: 1.0446 D(x): 0.6096 D(G(z)): 0.4113/0.3581\n",
            "Epoch [18/200] Batch [300/374] Loss_D: 1.2647 Loss_G: 1.2606 D(x): 0.5770 D(G(z)): 0.3721/0.2885\n",
            "Epoch [18/200] Batch [350/374] Loss_D: 1.2463 Loss_G: 1.1134 D(x): 0.6443 D(G(z)): 0.4054/0.3327\n",
            "Calculating FID...\n",
            "Epoch [18/200] Val Loss_D: 1.2067 Val Loss_G: 0.9365 FID: 228.5481\n",
            "Epoch [19/200] Batch [0/374] Loss_D: 1.2202 Loss_G: 1.0692 D(x): 0.6148 D(G(z)): 0.3885/0.3467\n",
            "Epoch [19/200] Batch [50/374] Loss_D: 1.2424 Loss_G: 0.9993 D(x): 0.5972 D(G(z)): 0.3726/0.3762\n",
            "Epoch [19/200] Batch [100/374] Loss_D: 1.2633 Loss_G: 1.0729 D(x): 0.5798 D(G(z)): 0.4140/0.3487\n",
            "Epoch [19/200] Batch [150/374] Loss_D: 1.3329 Loss_G: 1.1551 D(x): 0.5571 D(G(z)): 0.4335/0.3186\n",
            "Epoch [19/200] Batch [200/374] Loss_D: 1.2696 Loss_G: 1.0528 D(x): 0.6359 D(G(z)): 0.4581/0.3531\n",
            "Epoch [19/200] Batch [250/374] Loss_D: 1.2992 Loss_G: 0.8524 D(x): 0.5642 D(G(z)): 0.3599/0.4329\n",
            "Epoch [19/200] Batch [300/374] Loss_D: 1.3107 Loss_G: 0.9272 D(x): 0.5765 D(G(z)): 0.4318/0.4017\n",
            "Epoch [19/200] Batch [350/374] Loss_D: 1.2526 Loss_G: 0.9727 D(x): 0.5702 D(G(z)): 0.3790/0.3818\n",
            "Calculating FID...\n",
            "Epoch [19/200] Val Loss_D: 1.2537 Val Loss_G: 0.8226 FID: 242.7219\n",
            "Epoch [20/200] Batch [0/374] Loss_D: 1.2878 Loss_G: 0.8904 D(x): 0.6068 D(G(z)): 0.4409/0.4171\n",
            "Epoch [20/200] Batch [50/374] Loss_D: 1.2715 Loss_G: 1.0880 D(x): 0.5535 D(G(z)): 0.3988/0.3437\n",
            "Epoch [20/200] Batch [100/374] Loss_D: 1.2364 Loss_G: 1.1540 D(x): 0.6485 D(G(z)): 0.4245/0.3201\n",
            "Epoch [20/200] Batch [150/374] Loss_D: 1.2786 Loss_G: 1.0727 D(x): 0.6017 D(G(z)): 0.4533/0.3478\n",
            "Epoch [20/200] Batch [200/374] Loss_D: 1.2279 Loss_G: 1.1671 D(x): 0.6721 D(G(z)): 0.4189/0.3178\n",
            "Epoch [20/200] Batch [250/374] Loss_D: 1.2185 Loss_G: 1.0840 D(x): 0.6364 D(G(z)): 0.3626/0.3440\n",
            "Epoch [20/200] Batch [300/374] Loss_D: 1.2328 Loss_G: 0.9337 D(x): 0.6140 D(G(z)): 0.4096/0.3966\n",
            "Epoch [20/200] Batch [350/374] Loss_D: 1.2724 Loss_G: 1.0627 D(x): 0.6064 D(G(z)): 0.4128/0.3543\n",
            "Calculating FID...\n",
            "Epoch [20/200] Val Loss_D: 1.1634 Val Loss_G: 1.0308 FID: 237.7124\n",
            "Epoch [21/200] Batch [0/374] Loss_D: 1.1874 Loss_G: 1.1353 D(x): 0.6561 D(G(z)): 0.3570/0.3292\n",
            "Epoch [21/200] Batch [50/374] Loss_D: 1.2480 Loss_G: 1.0408 D(x): 0.6463 D(G(z)): 0.4165/0.3589\n",
            "Epoch [21/200] Batch [100/374] Loss_D: 1.2397 Loss_G: 0.9947 D(x): 0.5902 D(G(z)): 0.3991/0.3784\n",
            "Epoch [21/200] Batch [150/374] Loss_D: 1.2590 Loss_G: 1.1986 D(x): 0.6393 D(G(z)): 0.4761/0.3081\n",
            "Epoch [21/200] Batch [200/374] Loss_D: 1.3160 Loss_G: 1.4977 D(x): 0.7395 D(G(z)): 0.5284/0.2277\n",
            "Epoch [21/200] Batch [250/374] Loss_D: 1.2635 Loss_G: 1.0074 D(x): 0.5435 D(G(z)): 0.3345/0.3712\n",
            "Epoch [21/200] Batch [300/374] Loss_D: 1.2460 Loss_G: 1.0711 D(x): 0.6500 D(G(z)): 0.4631/0.3494\n",
            "Epoch [21/200] Batch [350/374] Loss_D: 1.2816 Loss_G: 0.8211 D(x): 0.4958 D(G(z)): 0.2989/0.4439\n",
            "Calculating FID...\n",
            "Epoch [21/200] Val Loss_D: 1.2135 Val Loss_G: 1.1255 FID: 240.7506\n",
            "Epoch [22/200] Batch [0/374] Loss_D: 1.2444 Loss_G: 0.9829 D(x): 0.5433 D(G(z)): 0.3431/0.3798\n",
            "Epoch [22/200] Batch [50/374] Loss_D: 1.2313 Loss_G: 1.1612 D(x): 0.6023 D(G(z)): 0.3898/0.3177\n",
            "Epoch [22/200] Batch [100/374] Loss_D: 1.2552 Loss_G: 1.0547 D(x): 0.6362 D(G(z)): 0.4331/0.3588\n",
            "Epoch [22/200] Batch [150/374] Loss_D: 1.2702 Loss_G: 1.1236 D(x): 0.6021 D(G(z)): 0.4312/0.3299\n",
            "Epoch [22/200] Batch [200/374] Loss_D: 1.2771 Loss_G: 1.0629 D(x): 0.6032 D(G(z)): 0.4086/0.3515\n",
            "Epoch [22/200] Batch [250/374] Loss_D: 1.2003 Loss_G: 1.0934 D(x): 0.6295 D(G(z)): 0.3988/0.3402\n",
            "Epoch [22/200] Batch [300/374] Loss_D: 1.2422 Loss_G: 1.0786 D(x): 0.6093 D(G(z)): 0.4018/0.3454\n",
            "Epoch [22/200] Batch [350/374] Loss_D: 1.2833 Loss_G: 1.1356 D(x): 0.5953 D(G(z)): 0.4136/0.3255\n",
            "Calculating FID...\n",
            "Epoch [22/200] Val Loss_D: 1.2823 Val Loss_G: 1.4248 FID: 243.5762\n",
            "Epoch [23/200] Batch [0/374] Loss_D: 1.3145 Loss_G: 0.7822 D(x): 0.4960 D(G(z)): 0.2537/0.4620\n",
            "Epoch [23/200] Batch [50/374] Loss_D: 1.2394 Loss_G: 1.1682 D(x): 0.6050 D(G(z)): 0.3426/0.3164\n",
            "Epoch [23/200] Batch [100/374] Loss_D: 1.2765 Loss_G: 1.1633 D(x): 0.6338 D(G(z)): 0.4584/0.3168\n",
            "Epoch [23/200] Batch [150/374] Loss_D: 1.2792 Loss_G: 0.8812 D(x): 0.5142 D(G(z)): 0.3277/0.4185\n",
            "Epoch [23/200] Batch [200/374] Loss_D: 1.3282 Loss_G: 0.8655 D(x): 0.4925 D(G(z)): 0.3500/0.4248\n",
            "Epoch [23/200] Batch [250/374] Loss_D: 1.2028 Loss_G: 1.2918 D(x): 0.7137 D(G(z)): 0.4411/0.2817\n",
            "Epoch [23/200] Batch [300/374] Loss_D: 1.2413 Loss_G: 1.0254 D(x): 0.5858 D(G(z)): 0.3659/0.3647\n",
            "Epoch [23/200] Batch [350/374] Loss_D: 1.2947 Loss_G: 1.0443 D(x): 0.5929 D(G(z)): 0.3988/0.3606\n",
            "Calculating FID...\n",
            "Epoch [23/200] Val Loss_D: 1.2652 Val Loss_G: 1.0282 FID: 227.4092\n",
            "Epoch [24/200] Batch [0/374] Loss_D: 1.2969 Loss_G: 0.8930 D(x): 0.5013 D(G(z)): 0.3476/0.4221\n",
            "Epoch [24/200] Batch [50/374] Loss_D: 1.2652 Loss_G: 1.0641 D(x): 0.5861 D(G(z)): 0.3941/0.3508\n",
            "Epoch [24/200] Batch [100/374] Loss_D: 1.2880 Loss_G: 0.9581 D(x): 0.5140 D(G(z)): 0.3041/0.3896\n",
            "Epoch [24/200] Batch [150/374] Loss_D: 1.2534 Loss_G: 1.2272 D(x): 0.5992 D(G(z)): 0.3679/0.2976\n",
            "Epoch [24/200] Batch [200/374] Loss_D: 1.2831 Loss_G: 1.2163 D(x): 0.5861 D(G(z)): 0.3918/0.3025\n",
            "Epoch [24/200] Batch [250/374] Loss_D: 1.2745 Loss_G: 1.0436 D(x): 0.6228 D(G(z)): 0.3861/0.3572\n",
            "Epoch [24/200] Batch [300/374] Loss_D: 1.2035 Loss_G: 1.0967 D(x): 0.6393 D(G(z)): 0.3517/0.3388\n",
            "Epoch [24/200] Batch [350/374] Loss_D: 1.2402 Loss_G: 1.0107 D(x): 0.6589 D(G(z)): 0.4516/0.3709\n",
            "Calculating FID...\n",
            "Epoch [24/200] Val Loss_D: 1.2386 Val Loss_G: 1.1015 FID: 222.5303\n",
            "Epoch [25/200] Batch [0/374] Loss_D: 1.2892 Loss_G: 0.9717 D(x): 0.5437 D(G(z)): 0.3326/0.3838\n",
            "Epoch [25/200] Batch [50/374] Loss_D: 1.2547 Loss_G: 1.1176 D(x): 0.6261 D(G(z)): 0.4424/0.3330\n",
            "Epoch [25/200] Batch [100/374] Loss_D: 1.2204 Loss_G: 1.0749 D(x): 0.5991 D(G(z)): 0.3583/0.3521\n",
            "Epoch [25/200] Batch [150/374] Loss_D: 1.2760 Loss_G: 1.1272 D(x): 0.5898 D(G(z)): 0.3935/0.3276\n",
            "Epoch [25/200] Batch [200/374] Loss_D: 1.3248 Loss_G: 1.0679 D(x): 0.6605 D(G(z)): 0.5132/0.3487\n",
            "Epoch [25/200] Batch [250/374] Loss_D: 1.2624 Loss_G: 1.2417 D(x): 0.5928 D(G(z)): 0.4144/0.2983\n",
            "Epoch [25/200] Batch [300/374] Loss_D: 1.2516 Loss_G: 1.2694 D(x): 0.6173 D(G(z)): 0.3984/0.2865\n",
            "Epoch [25/200] Batch [350/374] Loss_D: 1.2642 Loss_G: 1.1278 D(x): 0.6567 D(G(z)): 0.3734/0.3318\n",
            "Calculating FID...\n",
            "Epoch [25/200] Val Loss_D: 1.2436 Val Loss_G: 1.1608 FID: 236.6581\n",
            "Epoch [26/200] Batch [0/374] Loss_D: 1.2645 Loss_G: 0.9263 D(x): 0.5424 D(G(z)): 0.3255/0.4004\n",
            "Epoch [26/200] Batch [50/374] Loss_D: 1.2406 Loss_G: 1.1450 D(x): 0.5727 D(G(z)): 0.3908/0.3236\n",
            "Epoch [26/200] Batch [100/374] Loss_D: 1.3008 Loss_G: 0.9419 D(x): 0.5301 D(G(z)): 0.3637/0.3974\n",
            "Epoch [26/200] Batch [150/374] Loss_D: 1.3367 Loss_G: 0.8582 D(x): 0.5083 D(G(z)): 0.3287/0.4290\n",
            "Epoch [26/200] Batch [200/374] Loss_D: 1.2475 Loss_G: 1.1096 D(x): 0.5994 D(G(z)): 0.3873/0.3357\n",
            "Epoch [26/200] Batch [250/374] Loss_D: 1.2234 Loss_G: 1.2636 D(x): 0.6119 D(G(z)): 0.3581/0.2885\n",
            "Epoch [26/200] Batch [300/374] Loss_D: 1.2311 Loss_G: 1.1226 D(x): 0.6449 D(G(z)): 0.4102/0.3300\n",
            "Epoch [26/200] Batch [350/374] Loss_D: 1.2186 Loss_G: 1.0977 D(x): 0.6379 D(G(z)): 0.3868/0.3385\n",
            "Calculating FID...\n",
            "Epoch [26/200] Val Loss_D: 1.3101 Val Loss_G: 0.9041 FID: 215.6086\n",
            "Saved checkpoint to checkpoints/epoch_26_model_best.pt\n",
            "New best FID: 215.6086 at epoch 26\n",
            "Saved image grid to outputs/epoch_26_fake.png.\n",
            "Saved image grid to outputs/epoch_26_real.png.\n",
            "Epoch [27/200] Batch [0/374] Loss_D: 1.3463 Loss_G: 0.9138 D(x): 0.5281 D(G(z)): 0.4088/0.4091\n",
            "Epoch [27/200] Batch [50/374] Loss_D: 1.2896 Loss_G: 1.1185 D(x): 0.6450 D(G(z)): 0.4703/0.3333\n",
            "Epoch [27/200] Batch [100/374] Loss_D: 1.2218 Loss_G: 1.2082 D(x): 0.6056 D(G(z)): 0.3681/0.3041\n",
            "Epoch [27/200] Batch [150/374] Loss_D: 1.2835 Loss_G: 1.0956 D(x): 0.5699 D(G(z)): 0.3773/0.3425\n",
            "Epoch [27/200] Batch [200/374] Loss_D: 1.2893 Loss_G: 0.9732 D(x): 0.5441 D(G(z)): 0.3925/0.3833\n",
            "Epoch [27/200] Batch [250/374] Loss_D: 1.2944 Loss_G: 1.2431 D(x): 0.6274 D(G(z)): 0.4310/0.2985\n",
            "Epoch [27/200] Batch [300/374] Loss_D: 1.2707 Loss_G: 1.1396 D(x): 0.5945 D(G(z)): 0.4196/0.3249\n",
            "Epoch [27/200] Batch [350/374] Loss_D: 1.2874 Loss_G: 1.1664 D(x): 0.6359 D(G(z)): 0.4609/0.3190\n",
            "Calculating FID...\n",
            "Epoch [27/200] Val Loss_D: 1.2733 Val Loss_G: 0.7212 FID: 241.7050\n",
            "Epoch [28/200] Batch [0/374] Loss_D: 1.3078 Loss_G: 1.2612 D(x): 0.6529 D(G(z)): 0.5001/0.2896\n",
            "Epoch [28/200] Batch [50/374] Loss_D: 1.2341 Loss_G: 1.1718 D(x): 0.6315 D(G(z)): 0.4139/0.3189\n",
            "Epoch [28/200] Batch [100/374] Loss_D: 1.2314 Loss_G: 0.9579 D(x): 0.5855 D(G(z)): 0.3362/0.3887\n",
            "Epoch [28/200] Batch [150/374] Loss_D: 1.2369 Loss_G: 1.1512 D(x): 0.6585 D(G(z)): 0.3972/0.3213\n",
            "Epoch [28/200] Batch [200/374] Loss_D: 1.2800 Loss_G: 1.0805 D(x): 0.5179 D(G(z)): 0.3490/0.3457\n",
            "Epoch [28/200] Batch [250/374] Loss_D: 1.2616 Loss_G: 0.9002 D(x): 0.5938 D(G(z)): 0.4159/0.4095\n",
            "Epoch [28/200] Batch [300/374] Loss_D: 1.2104 Loss_G: 1.1992 D(x): 0.5795 D(G(z)): 0.3243/0.3073\n",
            "Epoch [28/200] Batch [350/374] Loss_D: 1.2740 Loss_G: 0.9255 D(x): 0.6590 D(G(z)): 0.4432/0.4039\n",
            "Calculating FID...\n",
            "Epoch [28/200] Val Loss_D: 1.2623 Val Loss_G: 0.7820 FID: 231.3099\n",
            "Epoch [29/200] Batch [0/374] Loss_D: 1.2924 Loss_G: 1.1410 D(x): 0.6174 D(G(z)): 0.4642/0.3264\n",
            "Epoch [29/200] Batch [50/374] Loss_D: 1.2667 Loss_G: 1.1869 D(x): 0.5780 D(G(z)): 0.3891/0.3105\n",
            "Epoch [29/200] Batch [100/374] Loss_D: 1.2389 Loss_G: 1.3041 D(x): 0.6439 D(G(z)): 0.4307/0.2772\n",
            "Epoch [29/200] Batch [150/374] Loss_D: 1.2659 Loss_G: 0.9343 D(x): 0.5675 D(G(z)): 0.3479/0.4001\n",
            "Epoch [29/200] Batch [200/374] Loss_D: 1.2840 Loss_G: 1.0482 D(x): 0.5973 D(G(z)): 0.3844/0.3567\n",
            "Epoch [29/200] Batch [250/374] Loss_D: 1.2866 Loss_G: 1.0103 D(x): 0.5316 D(G(z)): 0.3114/0.3706\n",
            "Epoch [29/200] Batch [300/374] Loss_D: 1.2715 Loss_G: 1.0095 D(x): 0.5985 D(G(z)): 0.4207/0.3695\n",
            "Epoch [29/200] Batch [350/374] Loss_D: 1.2507 Loss_G: 1.2446 D(x): 0.6487 D(G(z)): 0.3858/0.2947\n",
            "Calculating FID...\n",
            "Epoch [29/200] Val Loss_D: 1.2064 Val Loss_G: 0.9926 FID: 226.9418\n",
            "Epoch [30/200] Batch [0/374] Loss_D: 1.2475 Loss_G: 1.0078 D(x): 0.6083 D(G(z)): 0.3933/0.3733\n",
            "Epoch [30/200] Batch [50/374] Loss_D: 1.2482 Loss_G: 1.2003 D(x): 0.6178 D(G(z)): 0.3967/0.3064\n",
            "Epoch [30/200] Batch [100/374] Loss_D: 1.2308 Loss_G: 1.1064 D(x): 0.5866 D(G(z)): 0.3781/0.3359\n",
            "Epoch [30/200] Batch [150/374] Loss_D: 1.2347 Loss_G: 1.1197 D(x): 0.6361 D(G(z)): 0.4219/0.3327\n",
            "Epoch [30/200] Batch [200/374] Loss_D: 1.2958 Loss_G: 1.2119 D(x): 0.6645 D(G(z)): 0.4187/0.3058\n",
            "Epoch [30/200] Batch [250/374] Loss_D: 1.3160 Loss_G: 0.7526 D(x): 0.5114 D(G(z)): 0.2901/0.4795\n",
            "Epoch [30/200] Batch [300/374] Loss_D: 1.2032 Loss_G: 1.2262 D(x): 0.6259 D(G(z)): 0.3966/0.2986\n",
            "Epoch [30/200] Batch [350/374] Loss_D: 1.2678 Loss_G: 1.1600 D(x): 0.6770 D(G(z)): 0.4428/0.3180\n",
            "Calculating FID...\n",
            "Epoch [30/200] Val Loss_D: 1.2478 Val Loss_G: 0.7566 FID: 215.1769\n",
            "Epoch [31/200] Batch [0/374] Loss_D: 1.2620 Loss_G: 1.1136 D(x): 0.6696 D(G(z)): 0.4794/0.3364\n",
            "Epoch [31/200] Batch [50/374] Loss_D: 1.2489 Loss_G: 0.8083 D(x): 0.5784 D(G(z)): 0.4036/0.4507\n",
            "Epoch [31/200] Batch [100/374] Loss_D: 1.2547 Loss_G: 1.0343 D(x): 0.6318 D(G(z)): 0.4391/0.3609\n",
            "Epoch [31/200] Batch [150/374] Loss_D: 1.2546 Loss_G: 0.8980 D(x): 0.6088 D(G(z)): 0.3976/0.4134\n",
            "Epoch [31/200] Batch [200/374] Loss_D: 1.2526 Loss_G: 1.0177 D(x): 0.6122 D(G(z)): 0.3994/0.3713\n",
            "Epoch [31/200] Batch [250/374] Loss_D: 1.2355 Loss_G: 0.8800 D(x): 0.5877 D(G(z)): 0.3640/0.4196\n",
            "Epoch [31/200] Batch [300/374] Loss_D: 1.2558 Loss_G: 0.8827 D(x): 0.6095 D(G(z)): 0.3891/0.4188\n",
            "Epoch [31/200] Batch [350/374] Loss_D: 1.2702 Loss_G: 1.0712 D(x): 0.6634 D(G(z)): 0.4752/0.3483\n",
            "Calculating FID...\n",
            "Epoch [31/200] Val Loss_D: 1.3258 Val Loss_G: 1.0632 FID: 221.4726\n",
            "Epoch [32/200] Batch [0/374] Loss_D: 1.3734 Loss_G: 0.8540 D(x): 0.4766 D(G(z)): 0.3608/0.4315\n",
            "Epoch [32/200] Batch [50/374] Loss_D: 1.1808 Loss_G: 1.1776 D(x): 0.6576 D(G(z)): 0.3802/0.3151\n",
            "Epoch [32/200] Batch [100/374] Loss_D: 1.2606 Loss_G: 1.0575 D(x): 0.6202 D(G(z)): 0.4146/0.3574\n",
            "Epoch [32/200] Batch [150/374] Loss_D: 1.2091 Loss_G: 1.1197 D(x): 0.5912 D(G(z)): 0.3158/0.3330\n",
            "Epoch [32/200] Batch [200/374] Loss_D: 1.2575 Loss_G: 1.1331 D(x): 0.6419 D(G(z)): 0.4421/0.3277\n",
            "Epoch [32/200] Batch [250/374] Loss_D: 1.3662 Loss_G: 1.1074 D(x): 0.4833 D(G(z)): 0.3231/0.3387\n",
            "Epoch [32/200] Batch [300/374] Loss_D: 1.2369 Loss_G: 0.9813 D(x): 0.5888 D(G(z)): 0.3697/0.3803\n",
            "Epoch [32/200] Batch [350/374] Loss_D: 1.2414 Loss_G: 1.0246 D(x): 0.6274 D(G(z)): 0.3627/0.3647\n",
            "Calculating FID...\n",
            "Epoch [32/200] Val Loss_D: 1.2054 Val Loss_G: 0.8528 FID: 208.0161\n",
            "Saved checkpoint to checkpoints/epoch_32_model_best.pt\n",
            "New best FID: 208.0161 at epoch 32\n",
            "Saved image grid to outputs/epoch_32_fake.png.\n",
            "Saved image grid to outputs/epoch_32_real.png.\n",
            "Epoch [33/200] Batch [0/374] Loss_D: 1.2549 Loss_G: 1.0522 D(x): 0.6477 D(G(z)): 0.4310/0.3579\n",
            "Epoch [33/200] Batch [50/374] Loss_D: 1.2032 Loss_G: 1.0787 D(x): 0.6495 D(G(z)): 0.4080/0.3469\n",
            "Epoch [33/200] Batch [100/374] Loss_D: 1.2466 Loss_G: 1.1708 D(x): 0.6651 D(G(z)): 0.4177/0.3154\n",
            "Epoch [33/200] Batch [150/374] Loss_D: 1.2565 Loss_G: 1.2401 D(x): 0.6138 D(G(z)): 0.3860/0.2958\n",
            "Epoch [33/200] Batch [200/374] Loss_D: 1.2253 Loss_G: 0.9637 D(x): 0.5925 D(G(z)): 0.3684/0.3874\n",
            "Epoch [33/200] Batch [250/374] Loss_D: 1.2535 Loss_G: 1.0309 D(x): 0.6066 D(G(z)): 0.3972/0.3646\n",
            "Epoch [33/200] Batch [300/374] Loss_D: 1.2375 Loss_G: 1.0686 D(x): 0.5693 D(G(z)): 0.3309/0.3491\n",
            "Epoch [33/200] Batch [350/374] Loss_D: 1.2002 Loss_G: 1.0224 D(x): 0.6327 D(G(z)): 0.3533/0.3673\n",
            "Calculating FID...\n",
            "Epoch [33/200] Val Loss_D: 1.1935 Val Loss_G: 1.0184 FID: 219.9318\n",
            "Epoch [34/200] Batch [0/374] Loss_D: 1.2616 Loss_G: 1.1827 D(x): 0.6563 D(G(z)): 0.3634/0.3181\n",
            "Epoch [34/200] Batch [50/374] Loss_D: 1.2210 Loss_G: 1.0146 D(x): 0.5780 D(G(z)): 0.3715/0.3679\n",
            "Epoch [34/200] Batch [100/374] Loss_D: 1.2123 Loss_G: 1.3561 D(x): 0.6427 D(G(z)): 0.3489/0.2627\n",
            "Epoch [34/200] Batch [150/374] Loss_D: 1.2474 Loss_G: 0.9585 D(x): 0.5920 D(G(z)): 0.3683/0.3898\n",
            "Epoch [34/200] Batch [200/374] Loss_D: 1.2589 Loss_G: 0.9251 D(x): 0.5239 D(G(z)): 0.3122/0.4023\n",
            "Epoch [34/200] Batch [250/374] Loss_D: 1.2513 Loss_G: 1.0464 D(x): 0.6347 D(G(z)): 0.3852/0.3555\n",
            "Epoch [34/200] Batch [300/374] Loss_D: 1.2460 Loss_G: 1.0785 D(x): 0.6519 D(G(z)): 0.3978/0.3515\n",
            "Epoch [34/200] Batch [350/374] Loss_D: 1.2829 Loss_G: 1.1872 D(x): 0.6009 D(G(z)): 0.3979/0.3103\n",
            "Calculating FID...\n",
            "Epoch [34/200] Val Loss_D: 1.2135 Val Loss_G: 1.0346 FID: 221.3160\n",
            "Epoch [35/200] Batch [0/374] Loss_D: 1.2407 Loss_G: 1.0409 D(x): 0.6014 D(G(z)): 0.3627/0.3613\n",
            "Epoch [35/200] Batch [50/374] Loss_D: 1.2494 Loss_G: 1.0425 D(x): 0.5779 D(G(z)): 0.3878/0.3593\n",
            "Epoch [35/200] Batch [100/374] Loss_D: 1.2652 Loss_G: 0.9752 D(x): 0.6157 D(G(z)): 0.3966/0.3825\n",
            "Epoch [35/200] Batch [150/374] Loss_D: 1.2238 Loss_G: 1.0406 D(x): 0.6311 D(G(z)): 0.4070/0.3577\n",
            "Epoch [35/200] Batch [200/374] Loss_D: 1.2322 Loss_G: 1.0396 D(x): 0.6273 D(G(z)): 0.3930/0.3604\n",
            "Epoch [35/200] Batch [250/374] Loss_D: 1.2422 Loss_G: 1.0009 D(x): 0.6148 D(G(z)): 0.3925/0.3715\n",
            "Epoch [35/200] Batch [300/374] Loss_D: 1.2667 Loss_G: 0.8645 D(x): 0.5644 D(G(z)): 0.3444/0.4300\n",
            "Epoch [35/200] Batch [350/374] Loss_D: 1.2352 Loss_G: 1.1417 D(x): 0.6226 D(G(z)): 0.4083/0.3250\n",
            "Calculating FID...\n",
            "Epoch [35/200] Val Loss_D: 1.2173 Val Loss_G: 0.9598 FID: 214.0143\n",
            "Epoch [36/200] Batch [0/374] Loss_D: 1.2590 Loss_G: 1.0472 D(x): 0.5936 D(G(z)): 0.3881/0.3577\n",
            "Epoch [36/200] Batch [50/374] Loss_D: 1.2326 Loss_G: 1.1540 D(x): 0.6042 D(G(z)): 0.3773/0.3198\n",
            "Epoch [36/200] Batch [100/374] Loss_D: 1.2627 Loss_G: 1.0222 D(x): 0.6698 D(G(z)): 0.4537/0.3657\n",
            "Epoch [36/200] Batch [150/374] Loss_D: 1.2696 Loss_G: 0.9916 D(x): 0.6256 D(G(z)): 0.4242/0.3775\n",
            "Epoch [36/200] Batch [200/374] Loss_D: 1.2413 Loss_G: 1.3164 D(x): 0.6344 D(G(z)): 0.4294/0.2744\n",
            "Epoch [36/200] Batch [250/374] Loss_D: 1.3198 Loss_G: 1.0326 D(x): 0.5861 D(G(z)): 0.3459/0.3602\n",
            "Epoch [36/200] Batch [300/374] Loss_D: 1.2199 Loss_G: 0.8587 D(x): 0.5996 D(G(z)): 0.3387/0.4290\n",
            "Epoch [36/200] Batch [350/374] Loss_D: 1.2287 Loss_G: 1.0996 D(x): 0.6010 D(G(z)): 0.3730/0.3387\n",
            "Calculating FID...\n",
            "Epoch [36/200] Val Loss_D: 1.2482 Val Loss_G: 0.8170 FID: 218.8816\n",
            "Epoch [37/200] Batch [0/374] Loss_D: 1.2858 Loss_G: 1.3097 D(x): 0.6967 D(G(z)): 0.4547/0.2762\n",
            "Epoch [37/200] Batch [50/374] Loss_D: 1.2416 Loss_G: 1.0906 D(x): 0.5912 D(G(z)): 0.3748/0.3417\n",
            "Epoch [37/200] Batch [100/374] Loss_D: 1.2415 Loss_G: 1.1206 D(x): 0.6594 D(G(z)): 0.4457/0.3347\n",
            "Epoch [37/200] Batch [150/374] Loss_D: 1.2323 Loss_G: 0.9460 D(x): 0.6086 D(G(z)): 0.3975/0.3932\n",
            "Epoch [37/200] Batch [200/374] Loss_D: 1.2320 Loss_G: 1.0550 D(x): 0.5919 D(G(z)): 0.3519/0.3538\n",
            "Epoch [37/200] Batch [250/374] Loss_D: 1.2439 Loss_G: 1.0787 D(x): 0.6026 D(G(z)): 0.3504/0.3484\n",
            "Epoch [37/200] Batch [300/374] Loss_D: 1.2512 Loss_G: 1.0884 D(x): 0.5640 D(G(z)): 0.3220/0.3466\n",
            "Epoch [37/200] Batch [350/374] Loss_D: 1.2291 Loss_G: 1.1861 D(x): 0.6224 D(G(z)): 0.3941/0.3120\n",
            "Calculating FID...\n",
            "Epoch [37/200] Val Loss_D: 1.2387 Val Loss_G: 1.0282 FID: 206.7608\n",
            "Saved checkpoint to checkpoints/epoch_37_model_best.pt\n",
            "New best FID: 206.7608 at epoch 37\n",
            "Saved image grid to outputs/epoch_37_fake.png.\n",
            "Saved image grid to outputs/epoch_37_real.png.\n",
            "Epoch [38/200] Batch [0/374] Loss_D: 1.2844 Loss_G: 1.0534 D(x): 0.5630 D(G(z)): 0.3905/0.3542\n",
            "Epoch [38/200] Batch [50/374] Loss_D: 1.2204 Loss_G: 1.1963 D(x): 0.6557 D(G(z)): 0.3863/0.3095\n",
            "Epoch [38/200] Batch [100/374] Loss_D: 1.2783 Loss_G: 1.0156 D(x): 0.5422 D(G(z)): 0.3166/0.3720\n",
            "Epoch [38/200] Batch [150/374] Loss_D: 1.2766 Loss_G: 1.0772 D(x): 0.6462 D(G(z)): 0.4356/0.3464\n",
            "Epoch [38/200] Batch [200/374] Loss_D: 1.2273 Loss_G: 1.1355 D(x): 0.6570 D(G(z)): 0.4086/0.3277\n",
            "Epoch [38/200] Batch [250/374] Loss_D: 1.2112 Loss_G: 1.0035 D(x): 0.6335 D(G(z)): 0.3632/0.3701\n",
            "Epoch [38/200] Batch [300/374] Loss_D: 1.2707 Loss_G: 0.8913 D(x): 0.6007 D(G(z)): 0.3958/0.4171\n",
            "Epoch [38/200] Batch [350/374] Loss_D: 1.2466 Loss_G: 0.9769 D(x): 0.6769 D(G(z)): 0.4739/0.3855\n",
            "Calculating FID...\n",
            "Epoch [38/200] Val Loss_D: 1.1897 Val Loss_G: 0.9691 FID: 223.3121\n",
            "Epoch [39/200] Batch [0/374] Loss_D: 1.2404 Loss_G: 0.9855 D(x): 0.5953 D(G(z)): 0.3802/0.3794\n",
            "Epoch [39/200] Batch [50/374] Loss_D: 1.2755 Loss_G: 0.9000 D(x): 0.5973 D(G(z)): 0.4120/0.4131\n",
            "Epoch [39/200] Batch [100/374] Loss_D: 1.2589 Loss_G: 0.7536 D(x): 0.5679 D(G(z)): 0.3001/0.4760\n",
            "Epoch [39/200] Batch [150/374] Loss_D: 1.2322 Loss_G: 1.1368 D(x): 0.6184 D(G(z)): 0.4119/0.3264\n",
            "Epoch [39/200] Batch [200/374] Loss_D: 1.2639 Loss_G: 1.0013 D(x): 0.6004 D(G(z)): 0.4168/0.3735\n",
            "Epoch [39/200] Batch [250/374] Loss_D: 1.2339 Loss_G: 1.2611 D(x): 0.6271 D(G(z)): 0.3784/0.2910\n",
            "Epoch [39/200] Batch [300/374] Loss_D: 1.2192 Loss_G: 1.0567 D(x): 0.6139 D(G(z)): 0.3871/0.3533\n",
            "Epoch [39/200] Batch [350/374] Loss_D: 1.2729 Loss_G: 0.9358 D(x): 0.5341 D(G(z)): 0.3467/0.3969\n",
            "Calculating FID...\n",
            "Epoch [39/200] Val Loss_D: 1.2452 Val Loss_G: 0.8775 FID: 219.6907\n",
            "Epoch [40/200] Batch [0/374] Loss_D: 1.2589 Loss_G: 1.0824 D(x): 0.5944 D(G(z)): 0.4146/0.3483\n",
            "Epoch [40/200] Batch [50/374] Loss_D: 1.2311 Loss_G: 1.1627 D(x): 0.6200 D(G(z)): 0.3867/0.3173\n",
            "Epoch [40/200] Batch [100/374] Loss_D: 1.2369 Loss_G: 1.1837 D(x): 0.6095 D(G(z)): 0.3801/0.3105\n",
            "Epoch [40/200] Batch [150/374] Loss_D: 1.2874 Loss_G: 0.8022 D(x): 0.5437 D(G(z)): 0.3702/0.4571\n",
            "Epoch [40/200] Batch [200/374] Loss_D: 1.2537 Loss_G: 0.8805 D(x): 0.5771 D(G(z)): 0.3578/0.4219\n",
            "Epoch [40/200] Batch [250/374] Loss_D: 1.2592 Loss_G: 1.1044 D(x): 0.6040 D(G(z)): 0.4128/0.3368\n",
            "Epoch [40/200] Batch [300/374] Loss_D: 1.2755 Loss_G: 0.8901 D(x): 0.5321 D(G(z)): 0.2935/0.4189\n",
            "Epoch [40/200] Batch [350/374] Loss_D: 1.2676 Loss_G: 1.0286 D(x): 0.5390 D(G(z)): 0.3465/0.3627\n",
            "Calculating FID...\n",
            "Epoch [40/200] Val Loss_D: 1.2172 Val Loss_G: 0.9801 FID: 195.6899\n",
            "Saved checkpoint to checkpoints/epoch_40_model_best.pt\n",
            "New best FID: 195.6899 at epoch 40\n",
            "Saved image grid to outputs/epoch_40_fake.png.\n",
            "Saved image grid to outputs/epoch_40_real.png.\n",
            "Epoch [41/200] Batch [0/374] Loss_D: 1.2767 Loss_G: 1.0940 D(x): 0.5820 D(G(z)): 0.3927/0.3413\n",
            "Epoch [41/200] Batch [50/374] Loss_D: 1.2622 Loss_G: 0.7515 D(x): 0.5417 D(G(z)): 0.3345/0.4784\n",
            "Epoch [41/200] Batch [100/374] Loss_D: 1.2311 Loss_G: 1.2077 D(x): 0.6672 D(G(z)): 0.4323/0.3030\n",
            "Epoch [41/200] Batch [150/374] Loss_D: 1.2445 Loss_G: 0.9523 D(x): 0.6206 D(G(z)): 0.4369/0.3904\n",
            "Epoch [41/200] Batch [200/374] Loss_D: 1.2731 Loss_G: 1.0142 D(x): 0.5933 D(G(z)): 0.3915/0.3685\n",
            "Epoch [41/200] Batch [250/374] Loss_D: 1.2211 Loss_G: 0.9908 D(x): 0.6483 D(G(z)): 0.4300/0.3777\n",
            "Epoch [41/200] Batch [300/374] Loss_D: 1.2264 Loss_G: 0.9512 D(x): 0.6468 D(G(z)): 0.4136/0.3908\n",
            "Epoch [41/200] Batch [350/374] Loss_D: 1.2860 Loss_G: 1.3198 D(x): 0.6705 D(G(z)): 0.4680/0.2710\n",
            "Calculating FID...\n",
            "Epoch [41/200] Val Loss_D: 1.2489 Val Loss_G: 0.8312 FID: 206.8227\n",
            "Epoch [42/200] Batch [0/374] Loss_D: 1.2936 Loss_G: 1.1395 D(x): 0.6456 D(G(z)): 0.4423/0.3298\n",
            "Epoch [42/200] Batch [50/374] Loss_D: 1.2420 Loss_G: 1.3296 D(x): 0.6779 D(G(z)): 0.4363/0.2735\n",
            "Epoch [42/200] Batch [100/374] Loss_D: 1.2412 Loss_G: 1.1820 D(x): 0.6113 D(G(z)): 0.3881/0.3119\n",
            "Epoch [42/200] Batch [150/374] Loss_D: 1.2404 Loss_G: 0.9954 D(x): 0.6062 D(G(z)): 0.3951/0.3748\n",
            "Epoch [42/200] Batch [200/374] Loss_D: 1.2686 Loss_G: 1.1937 D(x): 0.6309 D(G(z)): 0.4255/0.3146\n",
            "Epoch [42/200] Batch [250/374] Loss_D: 1.2632 Loss_G: 0.9398 D(x): 0.5669 D(G(z)): 0.3979/0.3976\n",
            "Epoch [42/200] Batch [300/374] Loss_D: 1.2534 Loss_G: 1.0173 D(x): 0.5639 D(G(z)): 0.3715/0.3665\n",
            "Epoch [42/200] Batch [350/374] Loss_D: 1.2379 Loss_G: 1.1136 D(x): 0.6313 D(G(z)): 0.4136/0.3344\n",
            "Calculating FID...\n",
            "Epoch [42/200] Val Loss_D: 1.2104 Val Loss_G: 0.9197 FID: 198.3903\n",
            "Epoch [43/200] Batch [0/374] Loss_D: 1.2618 Loss_G: 0.9982 D(x): 0.5970 D(G(z)): 0.4022/0.3776\n",
            "Epoch [43/200] Batch [50/374] Loss_D: 1.2232 Loss_G: 1.2281 D(x): 0.6008 D(G(z)): 0.3750/0.2996\n",
            "Epoch [43/200] Batch [100/374] Loss_D: 1.2425 Loss_G: 0.9428 D(x): 0.5713 D(G(z)): 0.3664/0.3952\n",
            "Epoch [43/200] Batch [150/374] Loss_D: 1.2182 Loss_G: 1.1352 D(x): 0.6037 D(G(z)): 0.3636/0.3291\n",
            "Epoch [43/200] Batch [200/374] Loss_D: 1.2892 Loss_G: 0.9384 D(x): 0.5640 D(G(z)): 0.3861/0.4000\n",
            "Epoch [43/200] Batch [250/374] Loss_D: 1.2446 Loss_G: 1.0462 D(x): 0.5695 D(G(z)): 0.3404/0.3566\n",
            "Epoch [43/200] Batch [300/374] Loss_D: 1.2454 Loss_G: 1.0495 D(x): 0.6352 D(G(z)): 0.4369/0.3542\n",
            "Epoch [43/200] Batch [350/374] Loss_D: 1.2861 Loss_G: 1.0571 D(x): 0.6535 D(G(z)): 0.4823/0.3522\n",
            "Calculating FID...\n",
            "Epoch [43/200] Val Loss_D: 1.2605 Val Loss_G: 1.1070 FID: 200.8307\n",
            "Epoch [44/200] Batch [0/374] Loss_D: 1.2808 Loss_G: 0.8167 D(x): 0.5388 D(G(z)): 0.3314/0.4527\n",
            "Epoch [44/200] Batch [50/374] Loss_D: 1.3132 Loss_G: 1.0630 D(x): 0.5739 D(G(z)): 0.4362/0.3521\n",
            "Epoch [44/200] Batch [100/374] Loss_D: 1.2822 Loss_G: 0.9310 D(x): 0.6062 D(G(z)): 0.4040/0.3999\n",
            "Epoch [44/200] Batch [150/374] Loss_D: 1.2467 Loss_G: 0.9552 D(x): 0.5606 D(G(z)): 0.3780/0.3952\n",
            "Epoch [44/200] Batch [200/374] Loss_D: 1.2466 Loss_G: 0.8985 D(x): 0.5707 D(G(z)): 0.3449/0.4158\n",
            "Epoch [44/200] Batch [250/374] Loss_D: 1.2536 Loss_G: 1.0057 D(x): 0.5988 D(G(z)): 0.4239/0.3727\n",
            "Epoch [44/200] Batch [300/374] Loss_D: 1.2433 Loss_G: 1.0911 D(x): 0.5490 D(G(z)): 0.3104/0.3442\n",
            "Epoch [44/200] Batch [350/374] Loss_D: 1.2715 Loss_G: 0.9746 D(x): 0.5639 D(G(z)): 0.3827/0.3830\n",
            "Calculating FID...\n",
            "Epoch [44/200] Val Loss_D: 1.2087 Val Loss_G: 0.9535 FID: 203.1235\n",
            "Epoch [45/200] Batch [0/374] Loss_D: 1.2568 Loss_G: 1.0040 D(x): 0.6124 D(G(z)): 0.4011/0.3753\n",
            "Epoch [45/200] Batch [50/374] Loss_D: 1.2585 Loss_G: 0.9161 D(x): 0.5491 D(G(z)): 0.3592/0.4055\n",
            "Epoch [45/200] Batch [100/374] Loss_D: 1.2431 Loss_G: 1.0445 D(x): 0.5956 D(G(z)): 0.4027/0.3582\n",
            "Epoch [45/200] Batch [150/374] Loss_D: 1.2065 Loss_G: 1.1018 D(x): 0.6483 D(G(z)): 0.3734/0.3479\n",
            "Epoch [45/200] Batch [200/374] Loss_D: 1.2358 Loss_G: 0.8690 D(x): 0.5590 D(G(z)): 0.3089/0.4263\n",
            "Epoch [45/200] Batch [250/374] Loss_D: 1.2590 Loss_G: 0.8187 D(x): 0.6106 D(G(z)): 0.4012/0.4468\n",
            "Epoch [45/200] Batch [300/374] Loss_D: 1.2514 Loss_G: 1.0473 D(x): 0.6079 D(G(z)): 0.4014/0.3558\n",
            "Epoch [45/200] Batch [350/374] Loss_D: 1.2716 Loss_G: 0.8765 D(x): 0.5303 D(G(z)): 0.3345/0.4219\n",
            "Calculating FID...\n",
            "Epoch [45/200] Val Loss_D: 1.2258 Val Loss_G: 1.1768 FID: 195.9044\n",
            "Epoch [46/200] Batch [0/374] Loss_D: 1.2945 Loss_G: 0.8941 D(x): 0.5374 D(G(z)): 0.3151/0.4139\n",
            "Epoch [46/200] Batch [50/374] Loss_D: 1.2376 Loss_G: 0.9986 D(x): 0.6038 D(G(z)): 0.3681/0.3786\n",
            "Epoch [46/200] Batch [100/374] Loss_D: 1.2373 Loss_G: 0.9685 D(x): 0.5902 D(G(z)): 0.3773/0.3845\n",
            "Epoch [46/200] Batch [150/374] Loss_D: 1.2530 Loss_G: 1.1304 D(x): 0.6209 D(G(z)): 0.4434/0.3280\n",
            "Epoch [46/200] Batch [200/374] Loss_D: 1.2250 Loss_G: 1.0137 D(x): 0.5781 D(G(z)): 0.3696/0.3690\n",
            "Epoch [46/200] Batch [250/374] Loss_D: 1.2731 Loss_G: 0.9124 D(x): 0.5964 D(G(z)): 0.3903/0.4063\n",
            "Epoch [46/200] Batch [300/374] Loss_D: 1.2864 Loss_G: 0.9472 D(x): 0.5234 D(G(z)): 0.3547/0.3970\n",
            "Epoch [46/200] Batch [350/374] Loss_D: 1.2392 Loss_G: 1.1761 D(x): 0.6358 D(G(z)): 0.4293/0.3151\n",
            "Calculating FID...\n",
            "Epoch [46/200] Val Loss_D: 1.2090 Val Loss_G: 0.9430 FID: 201.3663\n",
            "Epoch [47/200] Batch [0/374] Loss_D: 1.2609 Loss_G: 1.2098 D(x): 0.6044 D(G(z)): 0.3976/0.3064\n",
            "Epoch [47/200] Batch [50/374] Loss_D: 1.2245 Loss_G: 1.0869 D(x): 0.6023 D(G(z)): 0.4022/0.3430\n",
            "Epoch [47/200] Batch [100/374] Loss_D: 1.2542 Loss_G: 0.8301 D(x): 0.5456 D(G(z)): 0.3300/0.4410\n",
            "Epoch [47/200] Batch [150/374] Loss_D: 1.2667 Loss_G: 0.9900 D(x): 0.5629 D(G(z)): 0.3585/0.3756\n",
            "Epoch [47/200] Batch [200/374] Loss_D: 1.2932 Loss_G: 0.8435 D(x): 0.5374 D(G(z)): 0.3290/0.4350\n",
            "Epoch [47/200] Batch [250/374] Loss_D: 1.2330 Loss_G: 1.0554 D(x): 0.6159 D(G(z)): 0.3994/0.3553\n",
            "Epoch [47/200] Batch [300/374] Loss_D: 1.2420 Loss_G: 1.0234 D(x): 0.6075 D(G(z)): 0.3986/0.3647\n",
            "Epoch [47/200] Batch [350/374] Loss_D: 1.2686 Loss_G: 0.9768 D(x): 0.6904 D(G(z)): 0.4657/0.3845\n",
            "Calculating FID...\n",
            "Epoch [47/200] Val Loss_D: 1.2676 Val Loss_G: 0.8860 FID: 181.9288\n",
            "Saved checkpoint to checkpoints/epoch_47_model_best.pt\n",
            "New best FID: 181.9288 at epoch 47\n",
            "Saved image grid to outputs/epoch_47_fake.png.\n",
            "Saved image grid to outputs/epoch_47_real.png.\n",
            "Epoch [48/200] Batch [0/374] Loss_D: 1.3187 Loss_G: 1.1608 D(x): 0.6013 D(G(z)): 0.4402/0.3188\n",
            "Epoch [48/200] Batch [50/374] Loss_D: 1.2560 Loss_G: 0.9040 D(x): 0.5764 D(G(z)): 0.3914/0.4099\n",
            "Epoch [48/200] Batch [100/374] Loss_D: 1.2472 Loss_G: 1.0132 D(x): 0.5632 D(G(z)): 0.3676/0.3738\n",
            "Epoch [48/200] Batch [150/374] Loss_D: 1.2652 Loss_G: 1.3416 D(x): 0.7102 D(G(z)): 0.4956/0.2681\n",
            "Epoch [48/200] Batch [200/374] Loss_D: 1.2540 Loss_G: 0.9526 D(x): 0.5602 D(G(z)): 0.3621/0.3902\n",
            "Epoch [48/200] Batch [250/374] Loss_D: 1.2478 Loss_G: 1.0275 D(x): 0.6171 D(G(z)): 0.4233/0.3612\n",
            "Epoch [48/200] Batch [300/374] Loss_D: 1.2703 Loss_G: 1.0786 D(x): 0.5984 D(G(z)): 0.4068/0.3466\n",
            "Epoch [48/200] Batch [350/374] Loss_D: 1.2746 Loss_G: 1.2758 D(x): 0.6680 D(G(z)): 0.4698/0.2862\n",
            "Calculating FID...\n",
            "Epoch [48/200] Val Loss_D: 1.2072 Val Loss_G: 0.9326 FID: 196.2075\n",
            "Epoch [49/200] Batch [0/374] Loss_D: 1.2317 Loss_G: 1.0386 D(x): 0.6232 D(G(z)): 0.4025/0.3609\n",
            "Epoch [49/200] Batch [50/374] Loss_D: 1.2515 Loss_G: 1.0536 D(x): 0.6223 D(G(z)): 0.4324/0.3530\n",
            "Epoch [49/200] Batch [100/374] Loss_D: 1.2645 Loss_G: 1.0799 D(x): 0.5892 D(G(z)): 0.4117/0.3447\n",
            "Epoch [49/200] Batch [150/374] Loss_D: 1.2125 Loss_G: 1.0780 D(x): 0.6167 D(G(z)): 0.3879/0.3487\n",
            "Epoch [49/200] Batch [200/374] Loss_D: 1.2551 Loss_G: 0.9506 D(x): 0.5873 D(G(z)): 0.3680/0.3900\n",
            "Epoch [49/200] Batch [250/374] Loss_D: 1.2384 Loss_G: 1.0707 D(x): 0.6277 D(G(z)): 0.3954/0.3470\n",
            "Epoch [49/200] Batch [300/374] Loss_D: 1.2885 Loss_G: 1.0405 D(x): 0.5229 D(G(z)): 0.3125/0.3589\n",
            "Epoch [49/200] Batch [350/374] Loss_D: 1.2776 Loss_G: 0.9348 D(x): 0.5156 D(G(z)): 0.3222/0.4011\n",
            "Calculating FID...\n",
            "Epoch [49/200] Val Loss_D: 1.2297 Val Loss_G: 1.0155 FID: 195.4447\n",
            "Epoch [50/200] Batch [0/374] Loss_D: 1.2359 Loss_G: 1.0916 D(x): 0.5556 D(G(z)): 0.3611/0.3392\n",
            "Epoch [50/200] Batch [50/374] Loss_D: 1.2385 Loss_G: 1.0797 D(x): 0.6004 D(G(z)): 0.3757/0.3485\n",
            "Epoch [50/200] Batch [100/374] Loss_D: 1.2637 Loss_G: 0.9362 D(x): 0.5944 D(G(z)): 0.4078/0.3970\n",
            "Epoch [50/200] Batch [150/374] Loss_D: 1.2769 Loss_G: 1.0218 D(x): 0.6432 D(G(z)): 0.4693/0.3656\n",
            "Epoch [50/200] Batch [200/374] Loss_D: 1.2789 Loss_G: 0.9967 D(x): 0.5835 D(G(z)): 0.3459/0.3756\n",
            "Epoch [50/200] Batch [250/374] Loss_D: 1.2517 Loss_G: 1.1694 D(x): 0.6539 D(G(z)): 0.4423/0.3152\n",
            "Epoch [50/200] Batch [300/374] Loss_D: 1.2812 Loss_G: 1.1138 D(x): 0.6623 D(G(z)): 0.5026/0.3368\n",
            "Epoch [50/200] Batch [350/374] Loss_D: 1.2577 Loss_G: 0.8617 D(x): 0.5912 D(G(z)): 0.3688/0.4300\n",
            "Calculating FID...\n",
            "Epoch [50/200] Val Loss_D: 1.2333 Val Loss_G: 0.9253 FID: 193.5398\n",
            "Epoch [51/200] Batch [0/374] Loss_D: 1.2682 Loss_G: 0.8757 D(x): 0.5951 D(G(z)): 0.3974/0.4238\n",
            "Epoch [51/200] Batch [50/374] Loss_D: 1.2219 Loss_G: 1.0918 D(x): 0.6208 D(G(z)): 0.4227/0.3401\n",
            "Epoch [51/200] Batch [100/374] Loss_D: 1.2479 Loss_G: 1.2337 D(x): 0.6240 D(G(z)): 0.4034/0.2974\n",
            "Epoch [51/200] Batch [150/374] Loss_D: 1.2416 Loss_G: 1.0197 D(x): 0.5875 D(G(z)): 0.3714/0.3663\n",
            "Epoch [51/200] Batch [200/374] Loss_D: 1.2570 Loss_G: 1.2141 D(x): 0.6736 D(G(z)): 0.4481/0.3014\n",
            "Epoch [51/200] Batch [250/374] Loss_D: 1.2518 Loss_G: 1.0739 D(x): 0.6099 D(G(z)): 0.3751/0.3477\n",
            "Epoch [51/200] Batch [300/374] Loss_D: 1.2433 Loss_G: 0.9938 D(x): 0.6239 D(G(z)): 0.4133/0.3826\n",
            "Epoch [51/200] Batch [350/374] Loss_D: 1.2650 Loss_G: 1.0817 D(x): 0.6014 D(G(z)): 0.3867/0.3441\n",
            "Calculating FID...\n",
            "Epoch [51/200] Val Loss_D: 1.2128 Val Loss_G: 1.0977 FID: 197.3953\n",
            "Epoch [52/200] Batch [0/374] Loss_D: 1.2241 Loss_G: 1.0054 D(x): 0.5672 D(G(z)): 0.3468/0.3720\n",
            "Epoch [52/200] Batch [50/374] Loss_D: 1.2813 Loss_G: 0.8924 D(x): 0.5544 D(G(z)): 0.3729/0.4150\n",
            "Epoch [52/200] Batch [100/374] Loss_D: 1.2546 Loss_G: 0.8760 D(x): 0.5509 D(G(z)): 0.3356/0.4233\n",
            "Epoch [52/200] Batch [150/374] Loss_D: 1.2580 Loss_G: 1.0456 D(x): 0.5789 D(G(z)): 0.3661/0.3573\n",
            "Epoch [52/200] Batch [200/374] Loss_D: 1.2674 Loss_G: 0.9040 D(x): 0.5309 D(G(z)): 0.3602/0.4102\n",
            "Epoch [52/200] Batch [250/374] Loss_D: 1.2485 Loss_G: 0.9008 D(x): 0.6034 D(G(z)): 0.4242/0.4119\n",
            "Epoch [52/200] Batch [300/374] Loss_D: 1.2563 Loss_G: 0.8399 D(x): 0.5754 D(G(z)): 0.3518/0.4373\n",
            "Epoch [52/200] Batch [350/374] Loss_D: 1.2541 Loss_G: 0.8979 D(x): 0.5888 D(G(z)): 0.4134/0.4103\n",
            "Calculating FID...\n",
            "Epoch [52/200] Val Loss_D: 1.2290 Val Loss_G: 0.7985 FID: 203.9534\n",
            "Epoch [53/200] Batch [0/374] Loss_D: 1.2640 Loss_G: 1.1550 D(x): 0.6506 D(G(z)): 0.4630/0.3202\n",
            "Epoch [53/200] Batch [50/374] Loss_D: 1.2299 Loss_G: 0.8775 D(x): 0.5796 D(G(z)): 0.3864/0.4209\n",
            "Epoch [53/200] Batch [100/374] Loss_D: 1.2990 Loss_G: 0.8635 D(x): 0.5549 D(G(z)): 0.3580/0.4261\n",
            "Epoch [53/200] Batch [150/374] Loss_D: 1.2377 Loss_G: 1.1037 D(x): 0.6475 D(G(z)): 0.4229/0.3387\n",
            "Epoch [53/200] Batch [200/374] Loss_D: 1.2325 Loss_G: 1.0431 D(x): 0.6053 D(G(z)): 0.3980/0.3561\n",
            "Epoch [53/200] Batch [250/374] Loss_D: 1.2480 Loss_G: 0.8795 D(x): 0.5700 D(G(z)): 0.3522/0.4221\n",
            "Epoch [53/200] Batch [300/374] Loss_D: 1.2693 Loss_G: 1.0946 D(x): 0.6790 D(G(z)): 0.4847/0.3383\n",
            "Epoch [53/200] Batch [350/374] Loss_D: 1.2358 Loss_G: 1.0662 D(x): 0.5651 D(G(z)): 0.3645/0.3499\n",
            "Calculating FID...\n",
            "Epoch [53/200] Val Loss_D: 1.2510 Val Loss_G: 1.1451 FID: 194.8376\n",
            "Epoch [54/200] Batch [0/374] Loss_D: 1.2668 Loss_G: 0.8080 D(x): 0.5166 D(G(z)): 0.3223/0.4525\n",
            "Epoch [54/200] Batch [50/374] Loss_D: 1.2800 Loss_G: 0.9438 D(x): 0.5521 D(G(z)): 0.3628/0.3952\n",
            "Epoch [54/200] Batch [100/374] Loss_D: 1.2233 Loss_G: 1.0803 D(x): 0.6023 D(G(z)): 0.3721/0.3463\n",
            "Epoch [54/200] Batch [150/374] Loss_D: 1.2790 Loss_G: 0.9928 D(x): 0.6102 D(G(z)): 0.4113/0.3780\n",
            "Epoch [54/200] Batch [200/374] Loss_D: 1.2516 Loss_G: 0.9612 D(x): 0.5765 D(G(z)): 0.3680/0.3885\n",
            "Epoch [54/200] Batch [250/374] Loss_D: 1.2916 Loss_G: 0.8040 D(x): 0.5141 D(G(z)): 0.3204/0.4534\n",
            "Epoch [54/200] Batch [300/374] Loss_D: 1.2874 Loss_G: 1.0898 D(x): 0.6248 D(G(z)): 0.4384/0.3430\n",
            "Epoch [54/200] Batch [350/374] Loss_D: 1.2340 Loss_G: 0.9435 D(x): 0.5996 D(G(z)): 0.3951/0.3938\n",
            "Calculating FID...\n",
            "Epoch [54/200] Val Loss_D: 1.2273 Val Loss_G: 0.8074 FID: 193.8883\n",
            "Epoch [55/200] Batch [0/374] Loss_D: 1.2647 Loss_G: 1.1598 D(x): 0.6555 D(G(z)): 0.4550/0.3212\n",
            "Epoch [55/200] Batch [50/374] Loss_D: 1.2328 Loss_G: 1.0428 D(x): 0.5777 D(G(z)): 0.3737/0.3569\n",
            "Epoch [55/200] Batch [100/374] Loss_D: 1.2580 Loss_G: 0.8662 D(x): 0.5262 D(G(z)): 0.3283/0.4278\n",
            "Epoch [55/200] Batch [150/374] Loss_D: 1.2415 Loss_G: 0.8839 D(x): 0.6513 D(G(z)): 0.4452/0.4173\n",
            "Epoch [55/200] Batch [200/374] Loss_D: 1.2501 Loss_G: 1.1634 D(x): 0.6476 D(G(z)): 0.4213/0.3228\n",
            "Epoch [55/200] Batch [250/374] Loss_D: 1.2512 Loss_G: 0.9811 D(x): 0.6143 D(G(z)): 0.3990/0.3801\n",
            "Epoch [55/200] Batch [300/374] Loss_D: 1.2448 Loss_G: 0.9507 D(x): 0.6038 D(G(z)): 0.3988/0.3923\n",
            "Epoch [55/200] Batch [350/374] Loss_D: 1.2762 Loss_G: 0.8107 D(x): 0.5128 D(G(z)): 0.3264/0.4517\n",
            "Calculating FID...\n",
            "Epoch [55/200] Val Loss_D: 1.2410 Val Loss_G: 1.1316 FID: 189.4386\n",
            "Epoch [56/200] Batch [0/374] Loss_D: 1.2814 Loss_G: 0.8794 D(x): 0.5294 D(G(z)): 0.3361/0.4222\n",
            "Epoch [56/200] Batch [50/374] Loss_D: 1.2393 Loss_G: 0.8551 D(x): 0.5796 D(G(z)): 0.3582/0.4292\n",
            "Epoch [56/200] Batch [100/374] Loss_D: 1.2515 Loss_G: 1.0657 D(x): 0.6227 D(G(z)): 0.4360/0.3499\n",
            "Epoch [56/200] Batch [150/374] Loss_D: 1.2776 Loss_G: 0.9590 D(x): 0.6150 D(G(z)): 0.4236/0.3899\n",
            "Epoch [56/200] Batch [200/374] Loss_D: 1.2857 Loss_G: 1.1919 D(x): 0.6641 D(G(z)): 0.4881/0.3109\n",
            "Epoch [56/200] Batch [250/374] Loss_D: 1.2647 Loss_G: 1.1868 D(x): 0.5680 D(G(z)): 0.3787/0.3101\n",
            "Epoch [56/200] Batch [300/374] Loss_D: 1.2538 Loss_G: 1.0847 D(x): 0.5915 D(G(z)): 0.4079/0.3438\n",
            "Epoch [56/200] Batch [350/374] Loss_D: 1.2797 Loss_G: 0.8762 D(x): 0.5472 D(G(z)): 0.3918/0.4221\n",
            "Calculating FID...\n",
            "Epoch [56/200] Val Loss_D: 1.2256 Val Loss_G: 0.8260 FID: 187.9768\n",
            "Epoch [57/200] Batch [0/374] Loss_D: 1.2647 Loss_G: 1.2367 D(x): 0.6523 D(G(z)): 0.4529/0.2983\n",
            "Epoch [57/200] Batch [50/374] Loss_D: 1.2361 Loss_G: 0.9922 D(x): 0.5719 D(G(z)): 0.3755/0.3751\n",
            "Epoch [57/200] Batch [100/374] Loss_D: 1.2745 Loss_G: 1.1694 D(x): 0.5688 D(G(z)): 0.3923/0.3179\n",
            "Epoch [57/200] Batch [150/374] Loss_D: 1.2537 Loss_G: 1.0561 D(x): 0.6594 D(G(z)): 0.4503/0.3559\n",
            "Epoch [57/200] Batch [200/374] Loss_D: 1.2644 Loss_G: 1.0167 D(x): 0.5836 D(G(z)): 0.3886/0.3662\n",
            "Epoch [57/200] Batch [250/374] Loss_D: 1.2498 Loss_G: 1.0954 D(x): 0.6112 D(G(z)): 0.3754/0.3431\n",
            "Epoch [57/200] Batch [300/374] Loss_D: 1.2519 Loss_G: 1.0427 D(x): 0.5978 D(G(z)): 0.4043/0.3575\n",
            "Epoch [57/200] Batch [350/374] Loss_D: 1.2511 Loss_G: 1.1667 D(x): 0.6359 D(G(z)): 0.4321/0.3151\n",
            "Calculating FID...\n",
            "Epoch [57/200] Val Loss_D: 1.2726 Val Loss_G: 0.6821 FID: 197.9502\n",
            "Epoch [58/200] Batch [0/374] Loss_D: 1.3320 Loss_G: 1.2950 D(x): 0.6781 D(G(z)): 0.5123/0.2803\n",
            "Epoch [58/200] Batch [50/374] Loss_D: 1.2699 Loss_G: 0.9279 D(x): 0.5607 D(G(z)): 0.3745/0.4000\n",
            "Epoch [58/200] Batch [100/374] Loss_D: 1.3478 Loss_G: 1.0424 D(x): 0.7005 D(G(z)): 0.5518/0.3569\n",
            "Epoch [58/200] Batch [150/374] Loss_D: 1.2692 Loss_G: 0.9972 D(x): 0.5580 D(G(z)): 0.3510/0.3741\n",
            "Epoch [58/200] Batch [200/374] Loss_D: 1.2552 Loss_G: 1.0807 D(x): 0.6478 D(G(z)): 0.4383/0.3475\n",
            "Epoch [58/200] Batch [250/374] Loss_D: 1.2562 Loss_G: 1.0738 D(x): 0.6289 D(G(z)): 0.4161/0.3500\n",
            "Epoch [58/200] Batch [300/374] Loss_D: 1.2487 Loss_G: 0.9728 D(x): 0.5639 D(G(z)): 0.3714/0.3835\n",
            "Epoch [58/200] Batch [350/374] Loss_D: 1.2714 Loss_G: 1.0846 D(x): 0.6171 D(G(z)): 0.4299/0.3471\n",
            "Calculating FID...\n",
            "Epoch [58/200] Val Loss_D: 1.2685 Val Loss_G: 0.7638 FID: 198.9305\n",
            "Epoch [59/200] Batch [0/374] Loss_D: 1.2753 Loss_G: 0.8949 D(x): 0.6608 D(G(z)): 0.4734/0.4171\n",
            "Epoch [59/200] Batch [50/374] Loss_D: 1.2689 Loss_G: 1.1407 D(x): 0.6429 D(G(z)): 0.4132/0.3256\n",
            "Epoch [59/200] Batch [100/374] Loss_D: 1.2503 Loss_G: 1.0554 D(x): 0.5718 D(G(z)): 0.3687/0.3534\n",
            "Epoch [59/200] Batch [150/374] Loss_D: 1.2946 Loss_G: 0.8909 D(x): 0.5607 D(G(z)): 0.3955/0.4146\n",
            "Epoch [59/200] Batch [200/374] Loss_D: 1.2746 Loss_G: 0.8313 D(x): 0.5246 D(G(z)): 0.3479/0.4407\n",
            "Epoch [59/200] Batch [250/374] Loss_D: 1.2358 Loss_G: 1.0855 D(x): 0.6227 D(G(z)): 0.4071/0.3453\n",
            "Epoch [59/200] Batch [300/374] Loss_D: 1.2823 Loss_G: 0.9056 D(x): 0.5301 D(G(z)): 0.3574/0.4082\n",
            "Epoch [59/200] Batch [350/374] Loss_D: 1.2322 Loss_G: 0.9064 D(x): 0.6004 D(G(z)): 0.3979/0.4123\n",
            "Calculating FID...\n",
            "Epoch [59/200] Val Loss_D: 1.2501 Val Loss_G: 1.0892 FID: 190.5870\n",
            "Epoch [60/200] Batch [0/374] Loss_D: 1.2885 Loss_G: 0.9188 D(x): 0.5343 D(G(z)): 0.3643/0.4046\n",
            "Epoch [60/200] Batch [50/374] Loss_D: 1.2743 Loss_G: 0.9434 D(x): 0.5403 D(G(z)): 0.3438/0.3957\n",
            "Epoch [60/200] Batch [100/374] Loss_D: 1.2537 Loss_G: 1.1519 D(x): 0.6737 D(G(z)): 0.4670/0.3236\n",
            "Epoch [60/200] Batch [150/374] Loss_D: 1.2624 Loss_G: 1.0819 D(x): 0.5817 D(G(z)): 0.4006/0.3448\n",
            "Epoch [60/200] Batch [200/374] Loss_D: 1.2622 Loss_G: 1.0085 D(x): 0.6044 D(G(z)): 0.4478/0.3707\n",
            "Epoch [60/200] Batch [250/374] Loss_D: 1.2589 Loss_G: 0.9396 D(x): 0.5804 D(G(z)): 0.3825/0.3984\n",
            "Epoch [60/200] Batch [300/374] Loss_D: 1.2563 Loss_G: 0.8290 D(x): 0.5905 D(G(z)): 0.4139/0.4423\n",
            "Epoch [60/200] Batch [350/374] Loss_D: 1.2506 Loss_G: 1.0306 D(x): 0.6059 D(G(z)): 0.4196/0.3657\n",
            "Calculating FID...\n",
            "Epoch [60/200] Val Loss_D: 1.2060 Val Loss_G: 0.8806 FID: 178.2670\n",
            "Saved checkpoint to checkpoints/epoch_60_model_best.pt\n",
            "New best FID: 178.2670 at epoch 60\n",
            "Saved image grid to outputs/epoch_60_fake.png.\n",
            "Saved image grid to outputs/epoch_60_real.png.\n",
            "Epoch [61/200] Batch [0/374] Loss_D: 1.2455 Loss_G: 1.2055 D(x): 0.6265 D(G(z)): 0.4261/0.3071\n",
            "Epoch [61/200] Batch [50/374] Loss_D: 1.2920 Loss_G: 1.1098 D(x): 0.7012 D(G(z)): 0.5249/0.3350\n",
            "Epoch [61/200] Batch [100/374] Loss_D: 1.2684 Loss_G: 0.7453 D(x): 0.5352 D(G(z)): 0.3204/0.4805\n",
            "Epoch [61/200] Batch [150/374] Loss_D: 1.2684 Loss_G: 0.9249 D(x): 0.5565 D(G(z)): 0.3614/0.4018\n",
            "Epoch [61/200] Batch [200/374] Loss_D: 1.2799 Loss_G: 0.9695 D(x): 0.5647 D(G(z)): 0.3881/0.3839\n",
            "Epoch [61/200] Batch [250/374] Loss_D: 1.2666 Loss_G: 1.0659 D(x): 0.6365 D(G(z)): 0.4178/0.3511\n",
            "Epoch [61/200] Batch [300/374] Loss_D: 1.2738 Loss_G: 1.0311 D(x): 0.5156 D(G(z)): 0.3066/0.3644\n",
            "Epoch [61/200] Batch [350/374] Loss_D: 1.2629 Loss_G: 1.0462 D(x): 0.6199 D(G(z)): 0.4289/0.3573\n",
            "Calculating FID...\n",
            "Epoch [61/200] Val Loss_D: 1.2639 Val Loss_G: 1.2239 FID: 173.3727\n",
            "Saved checkpoint to checkpoints/epoch_61_model_best.pt\n",
            "New best FID: 173.3727 at epoch 61\n",
            "Saved image grid to outputs/epoch_61_fake.png.\n",
            "Saved image grid to outputs/epoch_61_real.png.\n",
            "Epoch [62/200] Batch [0/374] Loss_D: 1.3081 Loss_G: 0.8494 D(x): 0.5362 D(G(z)): 0.3098/0.4369\n",
            "Epoch [62/200] Batch [50/374] Loss_D: 1.2924 Loss_G: 0.9067 D(x): 0.5904 D(G(z)): 0.4332/0.4094\n",
            "Epoch [62/200] Batch [100/374] Loss_D: 1.2267 Loss_G: 1.0033 D(x): 0.5747 D(G(z)): 0.3759/0.3723\n",
            "Epoch [62/200] Batch [150/374] Loss_D: 1.3214 Loss_G: 0.7393 D(x): 0.5327 D(G(z)): 0.3672/0.4853\n",
            "Epoch [62/200] Batch [200/374] Loss_D: 1.2355 Loss_G: 1.1595 D(x): 0.6542 D(G(z)): 0.4353/0.3202\n",
            "Epoch [62/200] Batch [250/374] Loss_D: 1.2652 Loss_G: 0.9251 D(x): 0.5481 D(G(z)): 0.3574/0.4030\n",
            "Epoch [62/200] Batch [300/374] Loss_D: 1.2773 Loss_G: 0.9337 D(x): 0.6121 D(G(z)): 0.4472/0.3963\n",
            "Epoch [62/200] Batch [350/374] Loss_D: 1.2764 Loss_G: 1.0958 D(x): 0.6269 D(G(z)): 0.4210/0.3422\n",
            "Calculating FID...\n",
            "Epoch [62/200] Val Loss_D: 1.2551 Val Loss_G: 0.7912 FID: 188.4220\n",
            "Epoch [63/200] Batch [0/374] Loss_D: 1.2787 Loss_G: 1.0932 D(x): 0.6348 D(G(z)): 0.4558/0.3417\n",
            "Epoch [63/200] Batch [50/374] Loss_D: 1.2462 Loss_G: 0.9247 D(x): 0.5440 D(G(z)): 0.3519/0.4054\n",
            "Epoch [63/200] Batch [100/374] Loss_D: 1.2555 Loss_G: 1.0116 D(x): 0.6243 D(G(z)): 0.4049/0.3683\n",
            "Epoch [63/200] Batch [150/374] Loss_D: 1.2348 Loss_G: 1.1584 D(x): 0.6719 D(G(z)): 0.4668/0.3216\n",
            "Epoch [63/200] Batch [200/374] Loss_D: 1.2651 Loss_G: 0.9949 D(x): 0.6324 D(G(z)): 0.4143/0.3792\n",
            "Epoch [63/200] Batch [250/374] Loss_D: 1.2943 Loss_G: 0.8914 D(x): 0.5490 D(G(z)): 0.3411/0.4154\n",
            "Epoch [63/200] Batch [300/374] Loss_D: 1.2512 Loss_G: 1.1211 D(x): 0.6062 D(G(z)): 0.4219/0.3320\n",
            "Epoch [63/200] Batch [350/374] Loss_D: 1.2681 Loss_G: 1.0784 D(x): 0.5829 D(G(z)): 0.4028/0.3469\n",
            "Calculating FID...\n",
            "Epoch [63/200] Val Loss_D: 1.2141 Val Loss_G: 0.9657 FID: 189.1046\n",
            "Epoch [64/200] Batch [0/374] Loss_D: 1.2783 Loss_G: 0.7621 D(x): 0.5965 D(G(z)): 0.3995/0.4724\n",
            "Epoch [64/200] Batch [50/374] Loss_D: 1.2724 Loss_G: 0.8976 D(x): 0.5689 D(G(z)): 0.3928/0.4143\n",
            "Epoch [64/200] Batch [100/374] Loss_D: 1.2728 Loss_G: 1.1839 D(x): 0.6131 D(G(z)): 0.4550/0.3127\n",
            "Epoch [64/200] Batch [150/374] Loss_D: 1.3135 Loss_G: 1.0878 D(x): 0.6134 D(G(z)): 0.4434/0.3442\n",
            "Epoch [64/200] Batch [200/374] Loss_D: 1.2902 Loss_G: 0.7601 D(x): 0.5322 D(G(z)): 0.3586/0.4729\n",
            "Epoch [64/200] Batch [250/374] Loss_D: 1.2566 Loss_G: 1.0238 D(x): 0.6111 D(G(z)): 0.4319/0.3642\n",
            "Epoch [64/200] Batch [300/374] Loss_D: 1.2602 Loss_G: 1.0619 D(x): 0.6354 D(G(z)): 0.4533/0.3531\n",
            "Epoch [64/200] Batch [350/374] Loss_D: 1.2469 Loss_G: 1.1480 D(x): 0.6415 D(G(z)): 0.4479/0.3259\n",
            "Calculating FID...\n",
            "Epoch [64/200] Val Loss_D: 1.2524 Val Loss_G: 0.7349 FID: 185.8149\n",
            "Epoch [65/200] Batch [0/374] Loss_D: 1.2929 Loss_G: 1.2859 D(x): 0.6705 D(G(z)): 0.4926/0.2860\n",
            "Epoch [65/200] Batch [50/374] Loss_D: 1.2655 Loss_G: 0.8581 D(x): 0.5664 D(G(z)): 0.3738/0.4305\n",
            "Epoch [65/200] Batch [100/374] Loss_D: 1.2445 Loss_G: 1.0589 D(x): 0.6465 D(G(z)): 0.4551/0.3542\n",
            "Epoch [65/200] Batch [150/374] Loss_D: 1.2586 Loss_G: 0.9696 D(x): 0.6708 D(G(z)): 0.4610/0.3850\n",
            "Epoch [65/200] Batch [200/374] Loss_D: 1.2526 Loss_G: 0.8479 D(x): 0.5806 D(G(z)): 0.3696/0.4375\n",
            "Epoch [65/200] Batch [250/374] Loss_D: 1.3066 Loss_G: 0.8708 D(x): 0.5723 D(G(z)): 0.4025/0.4254\n",
            "Epoch [65/200] Batch [300/374] Loss_D: 1.2724 Loss_G: 1.0697 D(x): 0.5967 D(G(z)): 0.4048/0.3481\n",
            "Epoch [65/200] Batch [350/374] Loss_D: 1.2704 Loss_G: 0.9987 D(x): 0.5928 D(G(z)): 0.4168/0.3740\n",
            "Calculating FID...\n",
            "Epoch [65/200] Val Loss_D: 1.2360 Val Loss_G: 0.8419 FID: 194.0938\n",
            "Epoch [66/200] Batch [0/374] Loss_D: 1.2569 Loss_G: 0.8529 D(x): 0.5866 D(G(z)): 0.4305/0.4322\n",
            "Epoch [66/200] Batch [50/374] Loss_D: 1.3005 Loss_G: 0.9730 D(x): 0.5878 D(G(z)): 0.4121/0.3837\n",
            "Epoch [66/200] Batch [100/374] Loss_D: 1.2839 Loss_G: 1.0838 D(x): 0.6388 D(G(z)): 0.4543/0.3424\n",
            "Epoch [66/200] Batch [150/374] Loss_D: 1.2748 Loss_G: 1.0872 D(x): 0.5789 D(G(z)): 0.4039/0.3428\n",
            "Epoch [66/200] Batch [200/374] Loss_D: 1.2682 Loss_G: 0.9163 D(x): 0.6112 D(G(z)): 0.4396/0.4061\n",
            "Epoch [66/200] Batch [250/374] Loss_D: 1.2805 Loss_G: 0.7545 D(x): 0.5584 D(G(z)): 0.3850/0.4781\n",
            "Epoch [66/200] Batch [300/374] Loss_D: 1.2926 Loss_G: 1.0654 D(x): 0.6399 D(G(z)): 0.4640/0.3508\n",
            "Epoch [66/200] Batch [350/374] Loss_D: 1.2458 Loss_G: 0.9149 D(x): 0.5724 D(G(z)): 0.4096/0.4085\n",
            "Calculating FID...\n",
            "Epoch [66/200] Val Loss_D: 1.2471 Val Loss_G: 1.1290 FID: 185.0016\n",
            "Epoch [67/200] Batch [0/374] Loss_D: 1.3007 Loss_G: 0.6889 D(x): 0.4949 D(G(z)): 0.3318/0.5085\n",
            "Epoch [67/200] Batch [50/374] Loss_D: 1.2658 Loss_G: 1.0757 D(x): 0.5953 D(G(z)): 0.3935/0.3471\n",
            "Epoch [67/200] Batch [100/374] Loss_D: 1.2600 Loss_G: 0.9392 D(x): 0.5438 D(G(z)): 0.3696/0.3973\n",
            "Epoch [67/200] Batch [150/374] Loss_D: 1.2457 Loss_G: 0.8892 D(x): 0.6105 D(G(z)): 0.4174/0.4159\n",
            "Epoch [67/200] Batch [200/374] Loss_D: 1.2778 Loss_G: 0.8216 D(x): 0.5788 D(G(z)): 0.3996/0.4452\n",
            "Epoch [67/200] Batch [250/374] Loss_D: 1.2632 Loss_G: 0.9677 D(x): 0.6381 D(G(z)): 0.4529/0.3878\n",
            "Epoch [67/200] Batch [300/374] Loss_D: 1.2551 Loss_G: 1.0483 D(x): 0.6151 D(G(z)): 0.4365/0.3555\n",
            "Epoch [67/200] Batch [350/374] Loss_D: 1.2647 Loss_G: 1.0042 D(x): 0.5815 D(G(z)): 0.3774/0.3754\n",
            "Calculating FID...\n",
            "Epoch [67/200] Val Loss_D: 1.3041 Val Loss_G: 0.6438 FID: 189.1464\n",
            "Epoch [68/200] Batch [0/374] Loss_D: 1.3249 Loss_G: 1.1410 D(x): 0.6762 D(G(z)): 0.5371/0.3265\n",
            "Epoch [68/200] Batch [50/374] Loss_D: 1.2565 Loss_G: 0.8726 D(x): 0.5595 D(G(z)): 0.3732/0.4234\n",
            "Epoch [68/200] Batch [100/374] Loss_D: 1.2795 Loss_G: 0.7611 D(x): 0.5158 D(G(z)): 0.3378/0.4729\n",
            "Epoch [68/200] Batch [150/374] Loss_D: 1.2800 Loss_G: 0.9468 D(x): 0.5770 D(G(z)): 0.3879/0.3943\n",
            "Epoch [68/200] Batch [200/374] Loss_D: 1.2784 Loss_G: 0.9571 D(x): 0.6169 D(G(z)): 0.4618/0.3895\n",
            "Epoch [68/200] Batch [250/374] Loss_D: 1.2522 Loss_G: 0.9036 D(x): 0.5902 D(G(z)): 0.4222/0.4104\n",
            "Epoch [68/200] Batch [300/374] Loss_D: 1.2668 Loss_G: 0.8747 D(x): 0.5848 D(G(z)): 0.4177/0.4264\n",
            "Epoch [68/200] Batch [350/374] Loss_D: 1.2679 Loss_G: 0.8489 D(x): 0.5258 D(G(z)): 0.3563/0.4338\n",
            "Calculating FID...\n",
            "Epoch [68/200] Val Loss_D: 1.2596 Val Loss_G: 1.1652 FID: 180.1270\n",
            "Epoch [69/200] Batch [0/374] Loss_D: 1.2991 Loss_G: 0.7340 D(x): 0.4997 D(G(z)): 0.3215/0.4843\n",
            "Epoch [69/200] Batch [50/374] Loss_D: 1.2870 Loss_G: 0.9682 D(x): 0.5413 D(G(z)): 0.3845/0.3845\n",
            "Epoch [69/200] Batch [100/374] Loss_D: 1.2651 Loss_G: 0.9878 D(x): 0.5907 D(G(z)): 0.4210/0.3775\n",
            "Epoch [69/200] Batch [150/374] Loss_D: 1.2737 Loss_G: 0.9025 D(x): 0.5767 D(G(z)): 0.4207/0.4089\n",
            "Epoch [69/200] Batch [200/374] Loss_D: 1.2618 Loss_G: 0.8081 D(x): 0.5503 D(G(z)): 0.3542/0.4522\n",
            "Epoch [69/200] Batch [250/374] Loss_D: 1.2731 Loss_G: 0.8198 D(x): 0.5635 D(G(z)): 0.3676/0.4469\n",
            "Epoch [69/200] Batch [300/374] Loss_D: 1.2820 Loss_G: 0.9999 D(x): 0.6227 D(G(z)): 0.4776/0.3722\n",
            "Epoch [69/200] Batch [350/374] Loss_D: 1.2661 Loss_G: 0.9967 D(x): 0.5699 D(G(z)): 0.3993/0.3738\n",
            "Calculating FID...\n",
            "Epoch [69/200] Val Loss_D: 1.2305 Val Loss_G: 0.9253 FID: 175.6824\n",
            "Epoch [70/200] Batch [0/374] Loss_D: 1.2689 Loss_G: 0.9499 D(x): 0.5528 D(G(z)): 0.4048/0.3931\n",
            "Epoch [70/200] Batch [50/374] Loss_D: 1.2661 Loss_G: 1.2856 D(x): 0.6817 D(G(z)): 0.4826/0.2849\n",
            "Epoch [70/200] Batch [100/374] Loss_D: 1.2568 Loss_G: 0.8537 D(x): 0.5922 D(G(z)): 0.4163/0.4310\n",
            "Epoch [70/200] Batch [150/374] Loss_D: 1.2803 Loss_G: 1.1540 D(x): 0.6623 D(G(z)): 0.4903/0.3227\n",
            "Epoch [70/200] Batch [200/374] Loss_D: 1.2733 Loss_G: 0.9508 D(x): 0.6239 D(G(z)): 0.4356/0.3922\n",
            "Epoch [70/200] Batch [250/374] Loss_D: 1.2525 Loss_G: 0.9860 D(x): 0.5802 D(G(z)): 0.3908/0.3788\n",
            "Epoch [70/200] Batch [300/374] Loss_D: 1.2713 Loss_G: 0.8364 D(x): 0.5471 D(G(z)): 0.3638/0.4413\n",
            "Epoch [70/200] Batch [350/374] Loss_D: 1.3048 Loss_G: 1.2215 D(x): 0.6827 D(G(z)): 0.5045/0.3000\n",
            "Calculating FID...\n",
            "Epoch [70/200] Val Loss_D: 1.2924 Val Loss_G: 0.8502 FID: 176.4966\n",
            "Epoch [71/200] Batch [0/374] Loss_D: 1.3033 Loss_G: 0.9523 D(x): 0.5706 D(G(z)): 0.4369/0.3976\n",
            "Epoch [71/200] Batch [50/374] Loss_D: 1.2846 Loss_G: 0.7346 D(x): 0.5432 D(G(z)): 0.3828/0.4844\n",
            "Epoch [71/200] Batch [100/374] Loss_D: 1.2684 Loss_G: 0.9563 D(x): 0.5719 D(G(z)): 0.4022/0.3913\n",
            "Epoch [71/200] Batch [150/374] Loss_D: 1.2530 Loss_G: 0.9534 D(x): 0.5638 D(G(z)): 0.4066/0.3903\n",
            "Epoch [71/200] Batch [200/374] Loss_D: 1.2740 Loss_G: 0.9603 D(x): 0.6111 D(G(z)): 0.4094/0.3874\n",
            "Epoch [71/200] Batch [250/374] Loss_D: 1.2489 Loss_G: 1.0360 D(x): 0.6039 D(G(z)): 0.4135/0.3618\n",
            "Epoch [71/200] Batch [300/374] Loss_D: 1.2536 Loss_G: 0.9202 D(x): 0.5907 D(G(z)): 0.4366/0.4051\n",
            "Epoch [71/200] Batch [350/374] Loss_D: 1.2805 Loss_G: 1.0376 D(x): 0.6123 D(G(z)): 0.4314/0.3586\n",
            "Calculating FID...\n",
            "Epoch [71/200] Val Loss_D: 1.2461 Val Loss_G: 0.7586 FID: 175.3610\n",
            "Epoch [72/200] Batch [0/374] Loss_D: 1.2507 Loss_G: 1.2137 D(x): 0.6803 D(G(z)): 0.4850/0.3028\n",
            "Epoch [72/200] Batch [50/374] Loss_D: 1.2695 Loss_G: 0.8629 D(x): 0.5517 D(G(z)): 0.3645/0.4315\n",
            "Epoch [72/200] Batch [100/374] Loss_D: 1.2543 Loss_G: 0.8712 D(x): 0.5874 D(G(z)): 0.4007/0.4242\n",
            "Epoch [72/200] Batch [150/374] Loss_D: 1.2943 Loss_G: 0.9290 D(x): 0.5594 D(G(z)): 0.4035/0.4011\n",
            "Epoch [72/200] Batch [200/374] Loss_D: 1.2591 Loss_G: 0.9024 D(x): 0.5388 D(G(z)): 0.3700/0.4114\n",
            "Epoch [72/200] Batch [250/374] Loss_D: 1.2784 Loss_G: 0.7268 D(x): 0.5299 D(G(z)): 0.3737/0.4878\n",
            "Epoch [72/200] Batch [300/374] Loss_D: 1.2521 Loss_G: 0.9403 D(x): 0.6249 D(G(z)): 0.4567/0.3966\n",
            "Epoch [72/200] Batch [350/374] Loss_D: 1.2399 Loss_G: 0.8572 D(x): 0.6033 D(G(z)): 0.4177/0.4309\n",
            "Calculating FID...\n",
            "Epoch [72/200] Val Loss_D: 1.2785 Val Loss_G: 1.1212 FID: 183.2527\n",
            "Epoch [73/200] Batch [0/374] Loss_D: 1.2947 Loss_G: 0.8358 D(x): 0.5087 D(G(z)): 0.3421/0.4407\n",
            "Epoch [73/200] Batch [50/374] Loss_D: 1.2652 Loss_G: 0.9223 D(x): 0.5416 D(G(z)): 0.3724/0.4018\n",
            "Epoch [73/200] Batch [100/374] Loss_D: 1.2788 Loss_G: 0.9450 D(x): 0.6164 D(G(z)): 0.4464/0.3953\n",
            "Epoch [73/200] Batch [150/374] Loss_D: 1.2723 Loss_G: 0.9441 D(x): 0.5374 D(G(z)): 0.3267/0.3956\n",
            "Epoch [73/200] Batch [200/374] Loss_D: 1.2648 Loss_G: 0.8759 D(x): 0.5516 D(G(z)): 0.3733/0.4230\n",
            "Epoch [73/200] Batch [250/374] Loss_D: 1.2703 Loss_G: 0.8900 D(x): 0.6069 D(G(z)): 0.4321/0.4178\n",
            "Epoch [73/200] Batch [300/374] Loss_D: 1.2640 Loss_G: 0.7853 D(x): 0.5629 D(G(z)): 0.3833/0.4600\n",
            "Epoch [73/200] Batch [350/374] Loss_D: 1.2620 Loss_G: 1.0463 D(x): 0.6000 D(G(z)): 0.4166/0.3558\n",
            "Calculating FID...\n",
            "Epoch [73/200] Val Loss_D: 1.2038 Val Loss_G: 0.9269 FID: 186.5959\n",
            "Epoch [74/200] Batch [0/374] Loss_D: 1.2386 Loss_G: 0.9343 D(x): 0.6034 D(G(z)): 0.4022/0.3980\n",
            "Epoch [74/200] Batch [50/374] Loss_D: 1.2901 Loss_G: 0.8822 D(x): 0.5275 D(G(z)): 0.3501/0.4182\n",
            "Epoch [74/200] Batch [100/374] Loss_D: 1.2641 Loss_G: 1.0342 D(x): 0.6520 D(G(z)): 0.4438/0.3609\n",
            "Epoch [74/200] Batch [150/374] Loss_D: 1.2498 Loss_G: 0.9903 D(x): 0.5996 D(G(z)): 0.4186/0.3768\n",
            "Epoch [74/200] Batch [200/374] Loss_D: 1.2698 Loss_G: 0.9795 D(x): 0.6130 D(G(z)): 0.4379/0.3809\n",
            "Epoch [74/200] Batch [250/374] Loss_D: 1.2650 Loss_G: 0.8495 D(x): 0.5282 D(G(z)): 0.3526/0.4341\n",
            "Epoch [74/200] Batch [300/374] Loss_D: 1.2929 Loss_G: 1.1144 D(x): 0.6509 D(G(z)): 0.4747/0.3333\n",
            "Epoch [74/200] Batch [350/374] Loss_D: 1.2415 Loss_G: 1.0602 D(x): 0.6193 D(G(z)): 0.4304/0.3530\n",
            "Calculating FID...\n",
            "Epoch [74/200] Val Loss_D: 1.2470 Val Loss_G: 0.7365 FID: 187.0503\n",
            "Epoch [75/200] Batch [0/374] Loss_D: 1.2653 Loss_G: 1.1150 D(x): 0.6596 D(G(z)): 0.4793/0.3325\n",
            "Epoch [75/200] Batch [50/374] Loss_D: 1.2677 Loss_G: 0.9225 D(x): 0.6182 D(G(z)): 0.4465/0.4035\n",
            "Epoch [75/200] Batch [100/374] Loss_D: 1.2796 Loss_G: 0.8695 D(x): 0.5363 D(G(z)): 0.3987/0.4244\n",
            "Epoch [75/200] Batch [150/374] Loss_D: 1.2659 Loss_G: 0.8716 D(x): 0.5934 D(G(z)): 0.4199/0.4228\n",
            "Epoch [75/200] Batch [200/374] Loss_D: 1.2651 Loss_G: 0.9309 D(x): 0.5695 D(G(z)): 0.3855/0.4007\n",
            "Epoch [75/200] Batch [250/374] Loss_D: 1.2660 Loss_G: 1.0273 D(x): 0.5386 D(G(z)): 0.3453/0.3663\n",
            "Epoch [75/200] Batch [300/374] Loss_D: 1.2826 Loss_G: 0.9096 D(x): 0.5421 D(G(z)): 0.3833/0.4094\n",
            "Epoch [75/200] Batch [350/374] Loss_D: 1.3128 Loss_G: 0.7949 D(x): 0.4846 D(G(z)): 0.3284/0.4568\n",
            "Calculating FID...\n",
            "Epoch [75/200] Val Loss_D: 1.2864 Val Loss_G: 1.1929 FID: 172.7151\n",
            "Epoch [76/200] Batch [0/374] Loss_D: 1.3246 Loss_G: 0.7252 D(x): 0.4747 D(G(z)): 0.3148/0.4933\n",
            "Epoch [76/200] Batch [50/374] Loss_D: 1.2752 Loss_G: 0.8896 D(x): 0.5966 D(G(z)): 0.4334/0.4151\n",
            "Epoch [76/200] Batch [100/374] Loss_D: 1.2677 Loss_G: 0.9041 D(x): 0.6111 D(G(z)): 0.4474/0.4098\n",
            "Epoch [76/200] Batch [150/374] Loss_D: 1.2642 Loss_G: 0.8608 D(x): 0.6495 D(G(z)): 0.4653/0.4270\n",
            "Epoch [76/200] Batch [200/374] Loss_D: 1.2415 Loss_G: 0.9090 D(x): 0.6208 D(G(z)): 0.4236/0.4102\n",
            "Epoch [76/200] Batch [250/374] Loss_D: 1.2515 Loss_G: 0.9186 D(x): 0.6016 D(G(z)): 0.4230/0.4042\n",
            "Epoch [76/200] Batch [300/374] Loss_D: 1.2880 Loss_G: 0.9999 D(x): 0.6379 D(G(z)): 0.4786/0.3751\n",
            "Epoch [76/200] Batch [350/374] Loss_D: 1.2789 Loss_G: 0.9001 D(x): 0.6061 D(G(z)): 0.4457/0.4120\n",
            "Calculating FID...\n",
            "Epoch [76/200] Val Loss_D: 1.2604 Val Loss_G: 1.0598 FID: 179.2194\n",
            "Epoch [77/200] Batch [0/374] Loss_D: 1.3194 Loss_G: 0.8504 D(x): 0.5168 D(G(z)): 0.3547/0.4318\n",
            "Epoch [77/200] Batch [50/374] Loss_D: 1.2695 Loss_G: 1.0123 D(x): 0.6352 D(G(z)): 0.4613/0.3685\n",
            "Epoch [77/200] Batch [100/374] Loss_D: 1.2989 Loss_G: 0.9122 D(x): 0.5495 D(G(z)): 0.3805/0.4061\n",
            "Epoch [77/200] Batch [150/374] Loss_D: 1.2595 Loss_G: 1.0367 D(x): 0.5990 D(G(z)): 0.4219/0.3617\n",
            "Epoch [77/200] Batch [200/374] Loss_D: 1.2579 Loss_G: 1.1060 D(x): 0.6226 D(G(z)): 0.4376/0.3398\n",
            "Epoch [77/200] Batch [250/374] Loss_D: 1.2513 Loss_G: 0.8539 D(x): 0.5907 D(G(z)): 0.4311/0.4326\n",
            "Epoch [77/200] Batch [300/374] Loss_D: 1.2631 Loss_G: 0.8872 D(x): 0.5680 D(G(z)): 0.3717/0.4184\n",
            "Epoch [77/200] Batch [350/374] Loss_D: 1.2454 Loss_G: 0.9836 D(x): 0.6219 D(G(z)): 0.4282/0.3790\n",
            "Calculating FID...\n",
            "Epoch [77/200] Val Loss_D: 1.2471 Val Loss_G: 1.1105 FID: 182.6102\n",
            "Epoch [78/200] Batch [0/374] Loss_D: 1.3033 Loss_G: 0.7116 D(x): 0.5046 D(G(z)): 0.3352/0.4947\n",
            "Epoch [78/200] Batch [50/374] Loss_D: 1.2335 Loss_G: 0.9308 D(x): 0.5882 D(G(z)): 0.3992/0.4000\n",
            "Epoch [78/200] Batch [100/374] Loss_D: 1.2843 Loss_G: 0.8424 D(x): 0.5484 D(G(z)): 0.3965/0.4353\n",
            "Epoch [78/200] Batch [150/374] Loss_D: 1.2533 Loss_G: 0.9345 D(x): 0.6426 D(G(z)): 0.4521/0.3990\n",
            "Epoch [78/200] Batch [200/374] Loss_D: 1.2733 Loss_G: 1.0000 D(x): 0.6240 D(G(z)): 0.4549/0.3719\n",
            "Epoch [78/200] Batch [250/374] Loss_D: 1.2486 Loss_G: 0.9711 D(x): 0.5677 D(G(z)): 0.3654/0.3876\n",
            "Epoch [78/200] Batch [300/374] Loss_D: 1.2794 Loss_G: 0.8485 D(x): 0.5389 D(G(z)): 0.3692/0.4347\n",
            "Epoch [78/200] Batch [350/374] Loss_D: 1.2887 Loss_G: 1.4438 D(x): 0.6900 D(G(z)): 0.4983/0.2422\n",
            "Calculating FID...\n",
            "Epoch [78/200] Val Loss_D: 1.2515 Val Loss_G: 1.1066 FID: 180.5416\n",
            "Epoch [79/200] Batch [0/374] Loss_D: 1.2911 Loss_G: 0.8736 D(x): 0.5032 D(G(z)): 0.3395/0.4229\n",
            "Epoch [79/200] Batch [50/374] Loss_D: 1.2806 Loss_G: 0.9742 D(x): 0.5713 D(G(z)): 0.3893/0.3840\n",
            "Epoch [79/200] Batch [100/374] Loss_D: 1.2365 Loss_G: 0.9079 D(x): 0.5726 D(G(z)): 0.3833/0.4088\n",
            "Epoch [79/200] Batch [150/374] Loss_D: 1.2583 Loss_G: 0.7608 D(x): 0.5428 D(G(z)): 0.3633/0.4740\n",
            "Epoch [79/200] Batch [200/374] Loss_D: 1.2637 Loss_G: 0.9999 D(x): 0.6600 D(G(z)): 0.4763/0.3751\n",
            "Epoch [79/200] Batch [250/374] Loss_D: 1.2494 Loss_G: 0.8460 D(x): 0.6082 D(G(z)): 0.4175/0.4367\n",
            "Epoch [79/200] Batch [300/374] Loss_D: 1.2618 Loss_G: 0.9382 D(x): 0.5694 D(G(z)): 0.3960/0.3958\n",
            "Epoch [79/200] Batch [350/374] Loss_D: 1.3109 Loss_G: 0.8381 D(x): 0.5175 D(G(z)): 0.3459/0.4371\n",
            "Calculating FID...\n",
            "Epoch [79/200] Val Loss_D: 1.3737 Val Loss_G: 0.5369 FID: 176.0251\n",
            "Epoch [80/200] Batch [0/374] Loss_D: 1.3821 Loss_G: 1.3376 D(x): 0.7484 D(G(z)): 0.5987/0.2687\n",
            "Epoch [80/200] Batch [50/374] Loss_D: 1.2794 Loss_G: 0.9067 D(x): 0.6151 D(G(z)): 0.4566/0.4100\n",
            "Epoch [80/200] Batch [100/374] Loss_D: 1.2557 Loss_G: 0.8990 D(x): 0.5738 D(G(z)): 0.3649/0.4125\n",
            "Epoch [80/200] Batch [150/374] Loss_D: 1.2820 Loss_G: 0.8171 D(x): 0.5411 D(G(z)): 0.3767/0.4476\n",
            "Epoch [80/200] Batch [200/374] Loss_D: 1.2934 Loss_G: 1.0002 D(x): 0.6546 D(G(z)): 0.4827/0.3724\n",
            "Epoch [80/200] Batch [250/374] Loss_D: 1.3147 Loss_G: 1.0870 D(x): 0.6581 D(G(z)): 0.5250/0.3428\n",
            "Epoch [80/200] Batch [300/374] Loss_D: 1.2643 Loss_G: 0.8492 D(x): 0.5260 D(G(z)): 0.3625/0.4326\n",
            "Epoch [80/200] Batch [350/374] Loss_D: 1.2639 Loss_G: 0.8954 D(x): 0.5893 D(G(z)): 0.4259/0.4189\n",
            "Calculating FID...\n",
            "Epoch [80/200] Val Loss_D: 1.2479 Val Loss_G: 0.8095 FID: 165.5672\n",
            "Saved checkpoint to checkpoints/epoch_80_model_best.pt\n",
            "New best FID: 165.5672 at epoch 80\n",
            "Saved image grid to outputs/epoch_80_fake.png.\n",
            "Saved image grid to outputs/epoch_80_real.png.\n",
            "Epoch [81/200] Batch [0/374] Loss_D: 1.2822 Loss_G: 1.1474 D(x): 0.6256 D(G(z)): 0.4550/0.3252\n",
            "Epoch [81/200] Batch [50/374] Loss_D: 1.2438 Loss_G: 0.8365 D(x): 0.5769 D(G(z)): 0.3725/0.4380\n",
            "Epoch [81/200] Batch [100/374] Loss_D: 1.2696 Loss_G: 0.9237 D(x): 0.6156 D(G(z)): 0.4550/0.4029\n",
            "Epoch [81/200] Batch [150/374] Loss_D: 1.2875 Loss_G: 1.0592 D(x): 0.6301 D(G(z)): 0.4761/0.3530\n",
            "Epoch [81/200] Batch [200/374] Loss_D: 1.3018 Loss_G: 0.9753 D(x): 0.6300 D(G(z)): 0.4834/0.3828\n",
            "Epoch [81/200] Batch [250/374] Loss_D: 1.2729 Loss_G: 1.0200 D(x): 0.6366 D(G(z)): 0.4449/0.3672\n",
            "Epoch [81/200] Batch [300/374] Loss_D: 1.2815 Loss_G: 0.8745 D(x): 0.5706 D(G(z)): 0.4122/0.4213\n",
            "Epoch [81/200] Batch [350/374] Loss_D: 1.3087 Loss_G: 0.8571 D(x): 0.5915 D(G(z)): 0.4183/0.4285\n",
            "Calculating FID...\n",
            "Epoch [81/200] Val Loss_D: 1.2351 Val Loss_G: 0.8815 FID: 174.2255\n",
            "Epoch [82/200] Batch [0/374] Loss_D: 1.2732 Loss_G: 1.0043 D(x): 0.6123 D(G(z)): 0.4264/0.3723\n",
            "Epoch [82/200] Batch [50/374] Loss_D: 1.2488 Loss_G: 0.9300 D(x): 0.5430 D(G(z)): 0.3398/0.3991\n",
            "Epoch [82/200] Batch [100/374] Loss_D: 1.2827 Loss_G: 0.8463 D(x): 0.5970 D(G(z)): 0.3921/0.4358\n",
            "Epoch [82/200] Batch [150/374] Loss_D: 1.2901 Loss_G: 1.1686 D(x): 0.6775 D(G(z)): 0.4794/0.3149\n",
            "Epoch [82/200] Batch [200/374] Loss_D: 1.2600 Loss_G: 1.2195 D(x): 0.6667 D(G(z)): 0.4722/0.3035\n",
            "Epoch [82/200] Batch [250/374] Loss_D: 1.2934 Loss_G: 0.7616 D(x): 0.5833 D(G(z)): 0.3773/0.4732\n",
            "Epoch [82/200] Batch [300/374] Loss_D: 1.2807 Loss_G: 0.7809 D(x): 0.5380 D(G(z)): 0.3717/0.4626\n",
            "Epoch [82/200] Batch [350/374] Loss_D: 1.2544 Loss_G: 0.9509 D(x): 0.6039 D(G(z)): 0.4253/0.3940\n",
            "Calculating FID...\n",
            "Epoch [82/200] Val Loss_D: 1.2640 Val Loss_G: 0.7389 FID: 175.6208\n",
            "Epoch [83/200] Batch [0/374] Loss_D: 1.3287 Loss_G: 1.1664 D(x): 0.6455 D(G(z)): 0.4841/0.3179\n",
            "Epoch [83/200] Batch [50/374] Loss_D: 1.2821 Loss_G: 0.9045 D(x): 0.5500 D(G(z)): 0.3732/0.4106\n",
            "Epoch [83/200] Batch [100/374] Loss_D: 1.2636 Loss_G: 1.0242 D(x): 0.5875 D(G(z)): 0.4196/0.3666\n",
            "Epoch [83/200] Batch [150/374] Loss_D: 1.3032 Loss_G: 0.9509 D(x): 0.5732 D(G(z)): 0.4238/0.3912\n",
            "Epoch [83/200] Batch [200/374] Loss_D: 1.2869 Loss_G: 0.7545 D(x): 0.5383 D(G(z)): 0.3356/0.4784\n",
            "Epoch [83/200] Batch [250/374] Loss_D: 1.2741 Loss_G: 0.8702 D(x): 0.5615 D(G(z)): 0.3886/0.4240\n",
            "Epoch [83/200] Batch [300/374] Loss_D: 1.2755 Loss_G: 0.9306 D(x): 0.6214 D(G(z)): 0.4526/0.4000\n",
            "Epoch [83/200] Batch [350/374] Loss_D: 1.2911 Loss_G: 1.0789 D(x): 0.6163 D(G(z)): 0.4521/0.3468\n",
            "Calculating FID...\n",
            "Epoch [83/200] Val Loss_D: 1.2373 Val Loss_G: 0.8298 FID: 179.9389\n",
            "Epoch [84/200] Batch [0/374] Loss_D: 1.2575 Loss_G: 0.9849 D(x): 0.6129 D(G(z)): 0.4436/0.3792\n",
            "Epoch [84/200] Batch [50/374] Loss_D: 1.3004 Loss_G: 0.9140 D(x): 0.5709 D(G(z)): 0.4277/0.4052\n",
            "Epoch [84/200] Batch [100/374] Loss_D: 1.2777 Loss_G: 0.9168 D(x): 0.5718 D(G(z)): 0.3985/0.4068\n",
            "Epoch [84/200] Batch [150/374] Loss_D: 1.2781 Loss_G: 1.1138 D(x): 0.6001 D(G(z)): 0.4261/0.3354\n",
            "Epoch [84/200] Batch [200/374] Loss_D: 1.2626 Loss_G: 1.0144 D(x): 0.6009 D(G(z)): 0.4393/0.3679\n",
            "Epoch [84/200] Batch [250/374] Loss_D: 1.2624 Loss_G: 0.9371 D(x): 0.5728 D(G(z)): 0.3996/0.3996\n",
            "Epoch [84/200] Batch [300/374] Loss_D: 1.2567 Loss_G: 1.0218 D(x): 0.6296 D(G(z)): 0.4340/0.3681\n",
            "Epoch [84/200] Batch [350/374] Loss_D: 1.2641 Loss_G: 0.9158 D(x): 0.5566 D(G(z)): 0.3913/0.4062\n",
            "Calculating FID...\n",
            "Epoch [84/200] Val Loss_D: 1.2596 Val Loss_G: 0.7439 FID: 174.5098\n",
            "Epoch [85/200] Batch [0/374] Loss_D: 1.3304 Loss_G: 1.0884 D(x): 0.6397 D(G(z)): 0.4905/0.3423\n",
            "Epoch [85/200] Batch [50/374] Loss_D: 1.2639 Loss_G: 0.8218 D(x): 0.5283 D(G(z)): 0.3661/0.4437\n",
            "Epoch [85/200] Batch [100/374] Loss_D: 1.2387 Loss_G: 0.9296 D(x): 0.5971 D(G(z)): 0.4061/0.4010\n",
            "Epoch [85/200] Batch [150/374] Loss_D: 1.2785 Loss_G: 0.7569 D(x): 0.5282 D(G(z)): 0.3512/0.4755\n",
            "Epoch [85/200] Batch [200/374] Loss_D: 1.2541 Loss_G: 1.1139 D(x): 0.5997 D(G(z)): 0.4203/0.3355\n",
            "Epoch [85/200] Batch [250/374] Loss_D: 1.2648 Loss_G: 0.9521 D(x): 0.6042 D(G(z)): 0.4280/0.3915\n",
            "Epoch [85/200] Batch [300/374] Loss_D: 1.2845 Loss_G: 1.0052 D(x): 0.6211 D(G(z)): 0.4649/0.3706\n",
            "Epoch [85/200] Batch [350/374] Loss_D: 1.2943 Loss_G: 0.8307 D(x): 0.5306 D(G(z)): 0.3848/0.4424\n",
            "Calculating FID...\n",
            "Epoch [85/200] Val Loss_D: 1.2697 Val Loss_G: 0.7607 FID: 181.4577\n",
            "Epoch [86/200] Batch [0/374] Loss_D: 1.2923 Loss_G: 0.8953 D(x): 0.6139 D(G(z)): 0.4812/0.4147\n",
            "Epoch [86/200] Batch [50/374] Loss_D: 1.2377 Loss_G: 0.9412 D(x): 0.5402 D(G(z)): 0.3451/0.3951\n",
            "Epoch [86/200] Batch [100/374] Loss_D: 1.2651 Loss_G: 0.8703 D(x): 0.5974 D(G(z)): 0.4272/0.4262\n",
            "Epoch [86/200] Batch [150/374] Loss_D: 1.2615 Loss_G: 0.9477 D(x): 0.5879 D(G(z)): 0.4152/0.3954\n",
            "Epoch [86/200] Batch [200/374] Loss_D: 1.2891 Loss_G: 0.9466 D(x): 0.5513 D(G(z)): 0.4137/0.3926\n",
            "Epoch [86/200] Batch [250/374] Loss_D: 1.3327 Loss_G: 1.0385 D(x): 0.6722 D(G(z)): 0.5092/0.3582\n",
            "Epoch [86/200] Batch [300/374] Loss_D: 1.2426 Loss_G: 1.0207 D(x): 0.5950 D(G(z)): 0.4241/0.3654\n",
            "Epoch [86/200] Batch [350/374] Loss_D: 1.2640 Loss_G: 0.8927 D(x): 0.5910 D(G(z)): 0.4403/0.4152\n",
            "Calculating FID...\n",
            "Epoch [86/200] Val Loss_D: 1.2576 Val Loss_G: 1.0917 FID: 181.4195\n",
            "Epoch [87/200] Batch [0/374] Loss_D: 1.3070 Loss_G: 0.8201 D(x): 0.4955 D(G(z)): 0.3520/0.4467\n",
            "Epoch [87/200] Batch [50/374] Loss_D: 1.2898 Loss_G: 0.8463 D(x): 0.5371 D(G(z)): 0.3838/0.4350\n",
            "Epoch [87/200] Batch [100/374] Loss_D: 1.2537 Loss_G: 0.9585 D(x): 0.5939 D(G(z)): 0.4008/0.3910\n",
            "Epoch [87/200] Batch [150/374] Loss_D: 1.2774 Loss_G: 1.0099 D(x): 0.6218 D(G(z)): 0.4581/0.3732\n",
            "Epoch [87/200] Batch [200/374] Loss_D: 1.2574 Loss_G: 0.9836 D(x): 0.5965 D(G(z)): 0.4158/0.3775\n",
            "Epoch [87/200] Batch [250/374] Loss_D: 1.2775 Loss_G: 0.8740 D(x): 0.5130 D(G(z)): 0.3447/0.4214\n",
            "Epoch [87/200] Batch [300/374] Loss_D: 1.3066 Loss_G: 1.0178 D(x): 0.6049 D(G(z)): 0.4545/0.3667\n",
            "Epoch [87/200] Batch [350/374] Loss_D: 1.2612 Loss_G: 0.8230 D(x): 0.5689 D(G(z)): 0.3957/0.4515\n",
            "Calculating FID...\n",
            "Epoch [87/200] Val Loss_D: 1.2434 Val Loss_G: 0.9617 FID: 173.2223\n",
            "Epoch [88/200] Batch [0/374] Loss_D: 1.2789 Loss_G: 0.9152 D(x): 0.5727 D(G(z)): 0.3959/0.4056\n",
            "Epoch [88/200] Batch [50/374] Loss_D: 1.2603 Loss_G: 0.8571 D(x): 0.5578 D(G(z)): 0.3895/0.4297\n",
            "Epoch [88/200] Batch [100/374] Loss_D: 1.2524 Loss_G: 0.9526 D(x): 0.6010 D(G(z)): 0.4305/0.3909\n",
            "Epoch [88/200] Batch [150/374] Loss_D: 1.2410 Loss_G: 0.9632 D(x): 0.5725 D(G(z)): 0.3751/0.3865\n",
            "Epoch [88/200] Batch [200/374] Loss_D: 1.2584 Loss_G: 0.8516 D(x): 0.5595 D(G(z)): 0.3729/0.4338\n",
            "Epoch [88/200] Batch [250/374] Loss_D: 1.2902 Loss_G: 1.0312 D(x): 0.6041 D(G(z)): 0.4448/0.3614\n",
            "Epoch [88/200] Batch [300/374] Loss_D: 1.2583 Loss_G: 1.1013 D(x): 0.6256 D(G(z)): 0.4374/0.3373\n",
            "Epoch [88/200] Batch [350/374] Loss_D: 1.2355 Loss_G: 1.0767 D(x): 0.5978 D(G(z)): 0.4100/0.3464\n",
            "Calculating FID...\n",
            "Epoch [88/200] Val Loss_D: 1.2411 Val Loss_G: 0.7770 FID: 174.2649\n",
            "Epoch [89/200] Batch [0/374] Loss_D: 1.2598 Loss_G: 1.0185 D(x): 0.6408 D(G(z)): 0.4654/0.3666\n",
            "Epoch [89/200] Batch [50/374] Loss_D: 1.2301 Loss_G: 1.0083 D(x): 0.6018 D(G(z)): 0.4080/0.3709\n",
            "Epoch [89/200] Batch [100/374] Loss_D: 1.2703 Loss_G: 0.9820 D(x): 0.5796 D(G(z)): 0.4068/0.3834\n",
            "Epoch [89/200] Batch [150/374] Loss_D: 1.2443 Loss_G: 1.0405 D(x): 0.5966 D(G(z)): 0.4087/0.3595\n",
            "Epoch [89/200] Batch [200/374] Loss_D: 1.2902 Loss_G: 0.9173 D(x): 0.5557 D(G(z)): 0.3966/0.4039\n",
            "Epoch [89/200] Batch [250/374] Loss_D: 1.2652 Loss_G: 0.9689 D(x): 0.5166 D(G(z)): 0.3496/0.3864\n",
            "Epoch [89/200] Batch [300/374] Loss_D: 1.2637 Loss_G: 0.8120 D(x): 0.5708 D(G(z)): 0.4101/0.4503\n",
            "Epoch [89/200] Batch [350/374] Loss_D: 1.2784 Loss_G: 1.0451 D(x): 0.6024 D(G(z)): 0.4501/0.3563\n",
            "Calculating FID...\n",
            "Epoch [89/200] Val Loss_D: 1.3178 Val Loss_G: 0.6147 FID: 177.0160\n",
            "Epoch [90/200] Batch [0/374] Loss_D: 1.3580 Loss_G: 0.9770 D(x): 0.7208 D(G(z)): 0.5590/0.3835\n",
            "Epoch [90/200] Batch [50/374] Loss_D: 1.2882 Loss_G: 0.9050 D(x): 0.5275 D(G(z)): 0.3770/0.4107\n",
            "Epoch [90/200] Batch [100/374] Loss_D: 1.2808 Loss_G: 0.8639 D(x): 0.5369 D(G(z)): 0.3722/0.4262\n",
            "Epoch [90/200] Batch [150/374] Loss_D: 1.2921 Loss_G: 1.0500 D(x): 0.6214 D(G(z)): 0.4617/0.3552\n",
            "Epoch [90/200] Batch [200/374] Loss_D: 1.2394 Loss_G: 0.8597 D(x): 0.5843 D(G(z)): 0.3918/0.4277\n",
            "Epoch [90/200] Batch [250/374] Loss_D: 1.2582 Loss_G: 0.9596 D(x): 0.6065 D(G(z)): 0.4329/0.3873\n",
            "Epoch [90/200] Batch [300/374] Loss_D: 1.3088 Loss_G: 0.8643 D(x): 0.5043 D(G(z)): 0.3494/0.4259\n",
            "Epoch [90/200] Batch [350/374] Loss_D: 1.2564 Loss_G: 0.9598 D(x): 0.6106 D(G(z)): 0.4391/0.3885\n",
            "Calculating FID...\n",
            "Epoch [90/200] Val Loss_D: 1.2525 Val Loss_G: 0.9767 FID: 179.7019\n",
            "Epoch [91/200] Batch [0/374] Loss_D: 1.2856 Loss_G: 0.8585 D(x): 0.5324 D(G(z)): 0.3901/0.4307\n",
            "Epoch [91/200] Batch [50/374] Loss_D: 1.2504 Loss_G: 0.8372 D(x): 0.5883 D(G(z)): 0.3985/0.4392\n",
            "Epoch [91/200] Batch [100/374] Loss_D: 1.2487 Loss_G: 1.1144 D(x): 0.5773 D(G(z)): 0.3869/0.3327\n",
            "Epoch [91/200] Batch [150/374] Loss_D: 1.2483 Loss_G: 1.0306 D(x): 0.5678 D(G(z)): 0.3635/0.3639\n",
            "Epoch [91/200] Batch [200/374] Loss_D: 1.2618 Loss_G: 1.0737 D(x): 0.6082 D(G(z)): 0.4274/0.3482\n",
            "Epoch [91/200] Batch [250/374] Loss_D: 1.2681 Loss_G: 0.9929 D(x): 0.5808 D(G(z)): 0.4097/0.3753\n",
            "Epoch [91/200] Batch [300/374] Loss_D: 1.2785 Loss_G: 0.7815 D(x): 0.5214 D(G(z)): 0.3671/0.4638\n",
            "Epoch [91/200] Batch [350/374] Loss_D: 1.2789 Loss_G: 0.9953 D(x): 0.6118 D(G(z)): 0.4718/0.3764\n",
            "Calculating FID...\n",
            "Epoch [91/200] Val Loss_D: 1.2143 Val Loss_G: 0.8736 FID: 167.9357\n",
            "Epoch [92/200] Batch [0/374] Loss_D: 1.2397 Loss_G: 0.8515 D(x): 0.6120 D(G(z)): 0.4196/0.4336\n",
            "Epoch [92/200] Batch [50/374] Loss_D: 1.2677 Loss_G: 1.0470 D(x): 0.5834 D(G(z)): 0.4009/0.3573\n",
            "Epoch [92/200] Batch [100/374] Loss_D: 1.2625 Loss_G: 0.9695 D(x): 0.5648 D(G(z)): 0.3889/0.3880\n",
            "Epoch [92/200] Batch [150/374] Loss_D: 1.2824 Loss_G: 0.9179 D(x): 0.5373 D(G(z)): 0.3695/0.4059\n",
            "Epoch [92/200] Batch [200/374] Loss_D: 1.2801 Loss_G: 0.7430 D(x): 0.5142 D(G(z)): 0.3448/0.4795\n",
            "Epoch [92/200] Batch [250/374] Loss_D: 1.2718 Loss_G: 0.9409 D(x): 0.6331 D(G(z)): 0.4589/0.3958\n",
            "Epoch [92/200] Batch [300/374] Loss_D: 1.3107 Loss_G: 0.9342 D(x): 0.5284 D(G(z)): 0.3678/0.4005\n",
            "Epoch [92/200] Batch [350/374] Loss_D: 1.2586 Loss_G: 0.9107 D(x): 0.6017 D(G(z)): 0.4331/0.4080\n",
            "Calculating FID...\n",
            "Epoch [92/200] Val Loss_D: 1.2556 Val Loss_G: 1.0470 FID: 174.0243\n",
            "Epoch [93/200] Batch [0/374] Loss_D: 1.2620 Loss_G: 0.7270 D(x): 0.5302 D(G(z)): 0.3559/0.4892\n",
            "Epoch [93/200] Batch [50/374] Loss_D: 1.2271 Loss_G: 0.8984 D(x): 0.6031 D(G(z)): 0.4115/0.4116\n",
            "Epoch [93/200] Batch [100/374] Loss_D: 1.2999 Loss_G: 0.9806 D(x): 0.6714 D(G(z)): 0.5097/0.3803\n",
            "Epoch [93/200] Batch [150/374] Loss_D: 1.2622 Loss_G: 0.8728 D(x): 0.6027 D(G(z)): 0.4292/0.4232\n",
            "Epoch [93/200] Batch [200/374] Loss_D: 1.3086 Loss_G: 0.9496 D(x): 0.4762 D(G(z)): 0.3189/0.3913\n",
            "Epoch [93/200] Batch [250/374] Loss_D: 1.2782 Loss_G: 0.8701 D(x): 0.5834 D(G(z)): 0.4138/0.4237\n",
            "Epoch [93/200] Batch [300/374] Loss_D: 1.2703 Loss_G: 1.0629 D(x): 0.5790 D(G(z)): 0.4250/0.3512\n",
            "Epoch [93/200] Batch [350/374] Loss_D: 1.2699 Loss_G: 0.8732 D(x): 0.5383 D(G(z)): 0.3865/0.4226\n",
            "Calculating FID...\n",
            "Epoch [93/200] Val Loss_D: 1.2398 Val Loss_G: 0.8779 FID: 174.3508\n",
            "Epoch [94/200] Batch [0/374] Loss_D: 1.2675 Loss_G: 0.9921 D(x): 0.5980 D(G(z)): 0.4252/0.3777\n",
            "Epoch [94/200] Batch [50/374] Loss_D: 1.2838 Loss_G: 0.8036 D(x): 0.5046 D(G(z)): 0.3322/0.4528\n",
            "Epoch [94/200] Batch [100/374] Loss_D: 1.3060 Loss_G: 0.9595 D(x): 0.6055 D(G(z)): 0.4402/0.3881\n",
            "Epoch [94/200] Batch [150/374] Loss_D: 1.2593 Loss_G: 0.8641 D(x): 0.5661 D(G(z)): 0.3973/0.4251\n",
            "Epoch [94/200] Batch [200/374] Loss_D: 1.3056 Loss_G: 0.7256 D(x): 0.5074 D(G(z)): 0.3739/0.4891\n",
            "Epoch [94/200] Batch [250/374] Loss_D: 1.2795 Loss_G: 1.0870 D(x): 0.6322 D(G(z)): 0.4694/0.3422\n",
            "Epoch [94/200] Batch [300/374] Loss_D: 1.2641 Loss_G: 1.0195 D(x): 0.5973 D(G(z)): 0.4248/0.3687\n",
            "Epoch [94/200] Batch [350/374] Loss_D: 1.2842 Loss_G: 0.9103 D(x): 0.5657 D(G(z)): 0.3905/0.4065\n",
            "Calculating FID...\n",
            "Epoch [94/200] Val Loss_D: 1.2544 Val Loss_G: 1.0793 FID: 165.3304\n",
            "Epoch [95/200] Batch [0/374] Loss_D: 1.2948 Loss_G: 0.8165 D(x): 0.5018 D(G(z)): 0.3465/0.4467\n",
            "Epoch [95/200] Batch [50/374] Loss_D: 1.2526 Loss_G: 0.9231 D(x): 0.5741 D(G(z)): 0.3855/0.4056\n",
            "Epoch [95/200] Batch [100/374] Loss_D: 1.2706 Loss_G: 1.0809 D(x): 0.6579 D(G(z)): 0.4838/0.3444\n",
            "Epoch [95/200] Batch [150/374] Loss_D: 1.2634 Loss_G: 0.9143 D(x): 0.5934 D(G(z)): 0.4245/0.4076\n",
            "Epoch [95/200] Batch [200/374] Loss_D: 1.2620 Loss_G: 0.8761 D(x): 0.5827 D(G(z)): 0.4186/0.4224\n",
            "Epoch [95/200] Batch [250/374] Loss_D: 1.2698 Loss_G: 0.8044 D(x): 0.5663 D(G(z)): 0.3989/0.4569\n",
            "Epoch [95/200] Batch [300/374] Loss_D: 1.2778 Loss_G: 1.0702 D(x): 0.5888 D(G(z)): 0.4191/0.3479\n",
            "Epoch [95/200] Batch [350/374] Loss_D: 1.2712 Loss_G: 1.0215 D(x): 0.6451 D(G(z)): 0.4826/0.3655\n",
            "Calculating FID...\n",
            "Epoch [95/200] Val Loss_D: 1.2400 Val Loss_G: 0.8767 FID: 169.8754\n",
            "Epoch [96/200] Batch [0/374] Loss_D: 1.2829 Loss_G: 0.9227 D(x): 0.5838 D(G(z)): 0.4468/0.4037\n",
            "Epoch [96/200] Batch [50/374] Loss_D: 1.2559 Loss_G: 0.8490 D(x): 0.6270 D(G(z)): 0.4352/0.4347\n",
            "Epoch [96/200] Batch [100/374] Loss_D: 1.2631 Loss_G: 1.0829 D(x): 0.6170 D(G(z)): 0.4578/0.3424\n",
            "Epoch [96/200] Batch [150/374] Loss_D: 1.2752 Loss_G: 0.9092 D(x): 0.6256 D(G(z)): 0.4383/0.4078\n",
            "Epoch [96/200] Batch [200/374] Loss_D: 1.2700 Loss_G: 1.0286 D(x): 0.5496 D(G(z)): 0.3970/0.3617\n",
            "Epoch [96/200] Batch [250/374] Loss_D: 1.2400 Loss_G: 0.9598 D(x): 0.6186 D(G(z)): 0.4423/0.3894\n",
            "Epoch [96/200] Batch [300/374] Loss_D: 1.2823 Loss_G: 0.8666 D(x): 0.5503 D(G(z)): 0.3844/0.4259\n",
            "Epoch [96/200] Batch [350/374] Loss_D: 1.2692 Loss_G: 0.9119 D(x): 0.6569 D(G(z)): 0.4761/0.4083\n",
            "Calculating FID...\n",
            "Epoch [96/200] Val Loss_D: 1.2695 Val Loss_G: 1.0682 FID: 186.3200\n",
            "Epoch [97/200] Batch [0/374] Loss_D: 1.3298 Loss_G: 0.6939 D(x): 0.5108 D(G(z)): 0.3599/0.5091\n",
            "Epoch [97/200] Batch [50/374] Loss_D: 1.2771 Loss_G: 0.9170 D(x): 0.5248 D(G(z)): 0.3712/0.4058\n",
            "Epoch [97/200] Batch [100/374] Loss_D: 1.3070 Loss_G: 1.1380 D(x): 0.6311 D(G(z)): 0.4526/0.3270\n",
            "Epoch [97/200] Batch [150/374] Loss_D: 1.2726 Loss_G: 0.9264 D(x): 0.5985 D(G(z)): 0.4512/0.4033\n",
            "Epoch [97/200] Batch [200/374] Loss_D: 1.2692 Loss_G: 0.8484 D(x): 0.5835 D(G(z)): 0.4195/0.4321\n",
            "Epoch [97/200] Batch [250/374] Loss_D: 1.2831 Loss_G: 0.8448 D(x): 0.5747 D(G(z)): 0.4155/0.4365\n",
            "Epoch [97/200] Batch [300/374] Loss_D: 1.2477 Loss_G: 1.0180 D(x): 0.5925 D(G(z)): 0.4211/0.3659\n",
            "Epoch [97/200] Batch [350/374] Loss_D: 1.2791 Loss_G: 0.8468 D(x): 0.6565 D(G(z)): 0.4856/0.4335\n",
            "Calculating FID...\n",
            "Epoch [97/200] Val Loss_D: 1.3719 Val Loss_G: 0.5190 FID: 167.4165\n",
            "Epoch [98/200] Batch [0/374] Loss_D: 1.4059 Loss_G: 1.0545 D(x): 0.7341 D(G(z)): 0.6181/0.3531\n",
            "Epoch [98/200] Batch [50/374] Loss_D: 1.2679 Loss_G: 0.9990 D(x): 0.6126 D(G(z)): 0.4552/0.3738\n",
            "Epoch [98/200] Batch [100/374] Loss_D: 1.2911 Loss_G: 0.8893 D(x): 0.5162 D(G(z)): 0.3489/0.4162\n",
            "Epoch [98/200] Batch [150/374] Loss_D: 1.2904 Loss_G: 0.8047 D(x): 0.5320 D(G(z)): 0.3591/0.4551\n",
            "Epoch [98/200] Batch [200/374] Loss_D: 1.3160 Loss_G: 0.7329 D(x): 0.4818 D(G(z)): 0.3301/0.4843\n",
            "Epoch [98/200] Batch [250/374] Loss_D: 1.2815 Loss_G: 1.0604 D(x): 0.6513 D(G(z)): 0.4830/0.3523\n",
            "Epoch [98/200] Batch [300/374] Loss_D: 1.2806 Loss_G: 0.9202 D(x): 0.5886 D(G(z)): 0.4340/0.4054\n",
            "Epoch [98/200] Batch [350/374] Loss_D: 1.2959 Loss_G: 0.7562 D(x): 0.5317 D(G(z)): 0.3640/0.4764\n",
            "Calculating FID...\n",
            "Epoch [98/200] Val Loss_D: 1.2637 Val Loss_G: 0.7411 FID: 175.5314\n",
            "Epoch [99/200] Batch [0/374] Loss_D: 1.3149 Loss_G: 0.9807 D(x): 0.6390 D(G(z)): 0.4829/0.3825\n",
            "Epoch [99/200] Batch [50/374] Loss_D: 1.2558 Loss_G: 0.8032 D(x): 0.5686 D(G(z)): 0.3906/0.4524\n",
            "Epoch [99/200] Batch [100/374] Loss_D: 1.2485 Loss_G: 0.9732 D(x): 0.5857 D(G(z)): 0.3841/0.3860\n",
            "Epoch [99/200] Batch [150/374] Loss_D: 1.2741 Loss_G: 1.0102 D(x): 0.6265 D(G(z)): 0.4438/0.3703\n",
            "Epoch [99/200] Batch [200/374] Loss_D: 1.2753 Loss_G: 0.9495 D(x): 0.5411 D(G(z)): 0.3703/0.3921\n",
            "Epoch [99/200] Batch [250/374] Loss_D: 1.2576 Loss_G: 0.9251 D(x): 0.6035 D(G(z)): 0.4159/0.4022\n",
            "Epoch [99/200] Batch [300/374] Loss_D: 1.3032 Loss_G: 0.9503 D(x): 0.5827 D(G(z)): 0.4462/0.3950\n",
            "Epoch [99/200] Batch [350/374] Loss_D: 1.2843 Loss_G: 0.9575 D(x): 0.6207 D(G(z)): 0.4667/0.3902\n",
            "Calculating FID...\n",
            "Epoch [99/200] Val Loss_D: 1.2540 Val Loss_G: 0.7589 FID: 158.5916\n",
            "Saved checkpoint to checkpoints/epoch_99_model_best.pt\n",
            "New best FID: 158.5916 at epoch 99\n",
            "Saved image grid to outputs/epoch_99_fake.png.\n",
            "Saved image grid to outputs/epoch_99_real.png.\n",
            "Epoch [100/200] Batch [0/374] Loss_D: 1.2776 Loss_G: 0.8677 D(x): 0.6278 D(G(z)): 0.4790/0.4266\n",
            "Epoch [100/200] Batch [50/374] Loss_D: 1.2539 Loss_G: 1.0910 D(x): 0.6148 D(G(z)): 0.4262/0.3423\n",
            "Epoch [100/200] Batch [100/374] Loss_D: 1.2805 Loss_G: 1.0669 D(x): 0.6405 D(G(z)): 0.4799/0.3510\n",
            "Epoch [100/200] Batch [150/374] Loss_D: 1.2767 Loss_G: 0.8832 D(x): 0.5292 D(G(z)): 0.3908/0.4228\n",
            "Epoch [100/200] Batch [200/374] Loss_D: 1.2673 Loss_G: 0.8917 D(x): 0.5841 D(G(z)): 0.4212/0.4169\n",
            "Epoch [100/200] Batch [250/374] Loss_D: 1.2700 Loss_G: 0.9401 D(x): 0.6056 D(G(z)): 0.4564/0.3978\n",
            "Epoch [100/200] Batch [300/374] Loss_D: 1.2948 Loss_G: 0.8739 D(x): 0.5311 D(G(z)): 0.3734/0.4212\n",
            "Epoch [100/200] Batch [350/374] Loss_D: 1.2844 Loss_G: 1.0191 D(x): 0.6333 D(G(z)): 0.4566/0.3680\n",
            "Calculating FID...\n",
            "Epoch [100/200] Val Loss_D: 1.2330 Val Loss_G: 1.0279 FID: 169.1128\n",
            "Epoch [101/200] Batch [0/374] Loss_D: 1.2629 Loss_G: 0.8290 D(x): 0.5623 D(G(z)): 0.3704/0.4433\n",
            "Epoch [101/200] Batch [50/374] Loss_D: 1.2854 Loss_G: 0.7445 D(x): 0.4960 D(G(z)): 0.3332/0.4799\n",
            "Epoch [101/200] Batch [100/374] Loss_D: 1.2620 Loss_G: 0.9366 D(x): 0.5720 D(G(z)): 0.4079/0.3987\n",
            "Epoch [101/200] Batch [150/374] Loss_D: 1.2864 Loss_G: 0.9750 D(x): 0.5611 D(G(z)): 0.4008/0.3829\n",
            "Epoch [101/200] Batch [200/374] Loss_D: 1.2913 Loss_G: 1.0401 D(x): 0.5543 D(G(z)): 0.4177/0.3600\n",
            "Epoch [101/200] Batch [250/374] Loss_D: 1.2554 Loss_G: 0.8975 D(x): 0.6088 D(G(z)): 0.4333/0.4137\n",
            "Epoch [101/200] Batch [300/374] Loss_D: 1.2494 Loss_G: 0.9646 D(x): 0.6189 D(G(z)): 0.4410/0.3858\n",
            "Epoch [101/200] Batch [350/374] Loss_D: 1.2938 Loss_G: 1.0123 D(x): 0.5980 D(G(z)): 0.4316/0.3702\n",
            "Calculating FID...\n",
            "Epoch [101/200] Val Loss_D: 1.2630 Val Loss_G: 0.7103 FID: 175.9008\n",
            "Epoch [102/200] Batch [0/374] Loss_D: 1.2805 Loss_G: 0.9729 D(x): 0.6687 D(G(z)): 0.4945/0.3841\n",
            "Epoch [102/200] Batch [50/374] Loss_D: 1.2640 Loss_G: 0.9813 D(x): 0.6045 D(G(z)): 0.4354/0.3798\n",
            "Epoch [102/200] Batch [100/374] Loss_D: 1.3019 Loss_G: 0.8294 D(x): 0.5374 D(G(z)): 0.3556/0.4412\n",
            "Epoch [102/200] Batch [150/374] Loss_D: 1.2525 Loss_G: 0.8962 D(x): 0.5655 D(G(z)): 0.4039/0.4129\n",
            "Epoch [102/200] Batch [200/374] Loss_D: 1.2679 Loss_G: 1.1213 D(x): 0.5993 D(G(z)): 0.4406/0.3330\n",
            "Epoch [102/200] Batch [250/374] Loss_D: 1.3006 Loss_G: 0.7979 D(x): 0.5493 D(G(z)): 0.4007/0.4572\n",
            "Epoch [102/200] Batch [300/374] Loss_D: 1.2490 Loss_G: 0.9168 D(x): 0.6165 D(G(z)): 0.4396/0.4055\n",
            "Epoch [102/200] Batch [350/374] Loss_D: 1.2633 Loss_G: 0.9188 D(x): 0.6027 D(G(z)): 0.4160/0.4052\n",
            "Calculating FID...\n",
            "Epoch [102/200] Val Loss_D: 1.2588 Val Loss_G: 0.9889 FID: 176.8170\n",
            "Epoch [103/200] Batch [0/374] Loss_D: 1.3084 Loss_G: 0.7784 D(x): 0.5237 D(G(z)): 0.3864/0.4658\n",
            "Epoch [103/200] Batch [50/374] Loss_D: 1.2638 Loss_G: 1.0300 D(x): 0.6055 D(G(z)): 0.4393/0.3632\n",
            "Epoch [103/200] Batch [100/374] Loss_D: 1.2607 Loss_G: 0.9569 D(x): 0.6043 D(G(z)): 0.4425/0.3894\n",
            "Epoch [103/200] Batch [150/374] Loss_D: 1.2596 Loss_G: 0.7739 D(x): 0.5231 D(G(z)): 0.3638/0.4671\n",
            "Epoch [103/200] Batch [200/374] Loss_D: 1.2321 Loss_G: 0.9694 D(x): 0.5992 D(G(z)): 0.3956/0.3868\n",
            "Epoch [103/200] Batch [250/374] Loss_D: 1.2748 Loss_G: 0.9025 D(x): 0.5628 D(G(z)): 0.4217/0.4096\n",
            "Epoch [103/200] Batch [300/374] Loss_D: 1.2666 Loss_G: 0.7863 D(x): 0.5583 D(G(z)): 0.3906/0.4591\n",
            "Epoch [103/200] Batch [350/374] Loss_D: 1.2619 Loss_G: 0.8499 D(x): 0.5493 D(G(z)): 0.3905/0.4312\n",
            "Calculating FID...\n",
            "Epoch [103/200] Val Loss_D: 1.2293 Val Loss_G: 0.9373 FID: 173.5976\n",
            "Epoch [104/200] Batch [0/374] Loss_D: 1.2661 Loss_G: 0.9159 D(x): 0.5683 D(G(z)): 0.3948/0.4077\n",
            "Epoch [104/200] Batch [50/374] Loss_D: 1.2701 Loss_G: 0.8612 D(x): 0.5658 D(G(z)): 0.4043/0.4283\n",
            "Epoch [104/200] Batch [100/374] Loss_D: 1.2485 Loss_G: 1.0585 D(x): 0.6086 D(G(z)): 0.4162/0.3534\n",
            "Epoch [104/200] Batch [150/374] Loss_D: 1.2938 Loss_G: 0.7866 D(x): 0.5268 D(G(z)): 0.3815/0.4612\n",
            "Epoch [104/200] Batch [200/374] Loss_D: 1.2720 Loss_G: 0.8480 D(x): 0.5625 D(G(z)): 0.4002/0.4373\n",
            "Epoch [104/200] Batch [250/374] Loss_D: 1.2590 Loss_G: 0.9585 D(x): 0.5908 D(G(z)): 0.4177/0.3900\n",
            "Epoch [104/200] Batch [300/374] Loss_D: 1.2699 Loss_G: 1.1019 D(x): 0.6295 D(G(z)): 0.4563/0.3375\n",
            "Epoch [104/200] Batch [350/374] Loss_D: 1.2855 Loss_G: 1.0614 D(x): 0.6467 D(G(z)): 0.4834/0.3522\n",
            "Calculating FID...\n",
            "Epoch [104/200] Val Loss_D: 1.2621 Val Loss_G: 1.1301 FID: 177.3528\n",
            "Epoch [105/200] Batch [0/374] Loss_D: 1.2898 Loss_G: 0.8864 D(x): 0.4974 D(G(z)): 0.3399/0.4191\n",
            "Epoch [105/200] Batch [50/374] Loss_D: 1.2725 Loss_G: 0.9150 D(x): 0.5714 D(G(z)): 0.4281/0.4075\n",
            "Epoch [105/200] Batch [100/374] Loss_D: 1.2794 Loss_G: 0.8237 D(x): 0.5129 D(G(z)): 0.3425/0.4438\n",
            "Epoch [105/200] Batch [150/374] Loss_D: 1.2632 Loss_G: 0.8291 D(x): 0.5713 D(G(z)): 0.4000/0.4425\n",
            "Epoch [105/200] Batch [200/374] Loss_D: 1.2984 Loss_G: 1.0142 D(x): 0.5667 D(G(z)): 0.3941/0.3696\n",
            "Epoch [105/200] Batch [250/374] Loss_D: 1.2637 Loss_G: 0.8592 D(x): 0.5637 D(G(z)): 0.4034/0.4296\n",
            "Epoch [105/200] Batch [300/374] Loss_D: 1.2758 Loss_G: 0.9613 D(x): 0.5599 D(G(z)): 0.3941/0.3867\n",
            "Epoch [105/200] Batch [350/374] Loss_D: 1.2565 Loss_G: 0.9424 D(x): 0.6193 D(G(z)): 0.4123/0.3942\n",
            "Calculating FID...\n",
            "Epoch [105/200] Val Loss_D: 1.2412 Val Loss_G: 0.8765 FID: 168.0455\n",
            "Epoch [106/200] Batch [0/374] Loss_D: 1.2954 Loss_G: 1.1264 D(x): 0.6057 D(G(z)): 0.4275/0.3297\n",
            "Epoch [106/200] Batch [50/374] Loss_D: 1.2498 Loss_G: 1.0304 D(x): 0.6099 D(G(z)): 0.4137/0.3616\n",
            "Epoch [106/200] Batch [100/374] Loss_D: 1.2816 Loss_G: 0.8539 D(x): 0.5764 D(G(z)): 0.4119/0.4302\n",
            "Epoch [106/200] Batch [150/374] Loss_D: 1.3140 Loss_G: 0.6971 D(x): 0.4638 D(G(z)): 0.3233/0.5024\n",
            "Epoch [106/200] Batch [200/374] Loss_D: 1.2786 Loss_G: 1.2294 D(x): 0.5987 D(G(z)): 0.4312/0.2996\n",
            "Epoch [106/200] Batch [250/374] Loss_D: 1.3213 Loss_G: 0.8047 D(x): 0.5206 D(G(z)): 0.3810/0.4517\n",
            "Epoch [106/200] Batch [300/374] Loss_D: 1.2853 Loss_G: 0.9174 D(x): 0.5899 D(G(z)): 0.4242/0.4050\n",
            "Epoch [106/200] Batch [350/374] Loss_D: 1.2550 Loss_G: 1.1798 D(x): 0.6372 D(G(z)): 0.4400/0.3123\n",
            "Calculating FID...\n",
            "Epoch [106/200] Val Loss_D: 1.2668 Val Loss_G: 1.0022 FID: 172.8582\n",
            "Epoch [107/200] Batch [0/374] Loss_D: 1.2867 Loss_G: 0.8406 D(x): 0.5174 D(G(z)): 0.3724/0.4366\n",
            "Epoch [107/200] Batch [50/374] Loss_D: 1.2473 Loss_G: 0.8691 D(x): 0.5798 D(G(z)): 0.4012/0.4245\n",
            "Epoch [107/200] Batch [100/374] Loss_D: 1.2917 Loss_G: 0.8310 D(x): 0.5355 D(G(z)): 0.3736/0.4412\n",
            "Epoch [107/200] Batch [150/374] Loss_D: 1.2809 Loss_G: 0.8476 D(x): 0.4987 D(G(z)): 0.3321/0.4343\n",
            "Epoch [107/200] Batch [200/374] Loss_D: 1.2683 Loss_G: 0.8206 D(x): 0.5725 D(G(z)): 0.4127/0.4444\n",
            "Epoch [107/200] Batch [250/374] Loss_D: 1.2638 Loss_G: 0.9102 D(x): 0.5790 D(G(z)): 0.4328/0.4106\n",
            "Epoch [107/200] Batch [300/374] Loss_D: 1.2768 Loss_G: 0.9141 D(x): 0.5917 D(G(z)): 0.4355/0.4072\n",
            "Epoch [107/200] Batch [350/374] Loss_D: 1.2711 Loss_G: 0.8441 D(x): 0.5742 D(G(z)): 0.4095/0.4350\n",
            "Calculating FID...\n",
            "Epoch [107/200] Val Loss_D: 1.2356 Val Loss_G: 0.8489 FID: 173.9298\n",
            "Epoch [108/200] Batch [0/374] Loss_D: 1.2540 Loss_G: 0.9093 D(x): 0.6046 D(G(z)): 0.4332/0.4087\n",
            "Epoch [108/200] Batch [50/374] Loss_D: 1.2704 Loss_G: 0.9534 D(x): 0.6223 D(G(z)): 0.4498/0.3911\n",
            "Epoch [108/200] Batch [100/374] Loss_D: 1.2906 Loss_G: 0.7879 D(x): 0.5647 D(G(z)): 0.3879/0.4613\n",
            "Epoch [108/200] Batch [150/374] Loss_D: 1.2642 Loss_G: 0.8733 D(x): 0.5441 D(G(z)): 0.3558/0.4217\n",
            "Epoch [108/200] Batch [200/374] Loss_D: 1.2578 Loss_G: 0.8395 D(x): 0.6053 D(G(z)): 0.4372/0.4363\n",
            "Epoch [108/200] Batch [250/374] Loss_D: 1.2843 Loss_G: 0.9628 D(x): 0.5710 D(G(z)): 0.4226/0.3859\n",
            "Epoch [108/200] Batch [300/374] Loss_D: 1.2555 Loss_G: 1.0746 D(x): 0.5974 D(G(z)): 0.4096/0.3481\n",
            "Epoch [108/200] Batch [350/374] Loss_D: 1.2896 Loss_G: 0.8639 D(x): 0.5433 D(G(z)): 0.3833/0.4273\n",
            "Calculating FID...\n",
            "Epoch [108/200] Val Loss_D: 1.2300 Val Loss_G: 0.8497 FID: 172.2542\n",
            "Epoch [109/200] Batch [0/374] Loss_D: 1.2475 Loss_G: 1.0862 D(x): 0.6031 D(G(z)): 0.4351/0.3468\n",
            "Epoch [109/200] Batch [50/374] Loss_D: 1.2809 Loss_G: 0.8992 D(x): 0.6002 D(G(z)): 0.4304/0.4114\n",
            "Epoch [109/200] Batch [100/374] Loss_D: 1.2671 Loss_G: 0.9405 D(x): 0.6303 D(G(z)): 0.4547/0.3952\n",
            "Epoch [109/200] Batch [150/374] Loss_D: 1.2558 Loss_G: 0.7390 D(x): 0.5896 D(G(z)): 0.4214/0.4865\n",
            "Epoch [109/200] Batch [200/374] Loss_D: 1.2742 Loss_G: 1.0507 D(x): 0.6313 D(G(z)): 0.4851/0.3549\n",
            "Epoch [109/200] Batch [250/374] Loss_D: 1.2872 Loss_G: 0.9551 D(x): 0.5923 D(G(z)): 0.4279/0.3897\n",
            "Epoch [109/200] Batch [300/374] Loss_D: 1.2989 Loss_G: 0.9352 D(x): 0.6170 D(G(z)): 0.4689/0.3984\n",
            "Epoch [109/200] Batch [350/374] Loss_D: 1.2646 Loss_G: 1.0589 D(x): 0.6267 D(G(z)): 0.4295/0.3527\n",
            "Calculating FID...\n",
            "Epoch [109/200] Val Loss_D: 1.2695 Val Loss_G: 1.1147 FID: 169.0817\n",
            "Epoch [110/200] Batch [0/374] Loss_D: 1.2889 Loss_G: 0.7846 D(x): 0.4908 D(G(z)): 0.3392/0.4633\n",
            "Epoch [110/200] Batch [50/374] Loss_D: 1.2918 Loss_G: 0.9895 D(x): 0.6294 D(G(z)): 0.4667/0.3786\n",
            "Epoch [110/200] Batch [100/374] Loss_D: 1.2741 Loss_G: 0.9082 D(x): 0.5918 D(G(z)): 0.4391/0.4076\n",
            "Epoch [110/200] Batch [150/374] Loss_D: 1.2542 Loss_G: 1.0355 D(x): 0.5509 D(G(z)): 0.3791/0.3610\n",
            "Epoch [110/200] Batch [200/374] Loss_D: 1.2507 Loss_G: 0.9582 D(x): 0.5805 D(G(z)): 0.3875/0.3909\n",
            "Epoch [110/200] Batch [250/374] Loss_D: 1.2846 Loss_G: 0.9293 D(x): 0.6021 D(G(z)): 0.4284/0.4005\n",
            "Epoch [110/200] Batch [300/374] Loss_D: 1.2846 Loss_G: 0.8075 D(x): 0.5798 D(G(z)): 0.4297/0.4514\n",
            "Epoch [110/200] Batch [350/374] Loss_D: 1.2838 Loss_G: 0.9342 D(x): 0.6047 D(G(z)): 0.4438/0.3978\n",
            "Calculating FID...\n",
            "Epoch [110/200] Val Loss_D: 1.3265 Val Loss_G: 0.6234 FID: 175.4323\n",
            "Epoch [111/200] Batch [0/374] Loss_D: 1.3894 Loss_G: 1.2388 D(x): 0.6925 D(G(z)): 0.5467/0.2973\n",
            "Epoch [111/200] Batch [50/374] Loss_D: 1.2615 Loss_G: 0.9437 D(x): 0.5896 D(G(z)): 0.4202/0.3965\n",
            "Epoch [111/200] Batch [100/374] Loss_D: 1.3292 Loss_G: 1.0369 D(x): 0.6536 D(G(z)): 0.4839/0.3614\n",
            "Epoch [111/200] Batch [150/374] Loss_D: 1.2664 Loss_G: 0.9809 D(x): 0.6303 D(G(z)): 0.4599/0.3826\n",
            "Epoch [111/200] Batch [200/374] Loss_D: 1.2449 Loss_G: 0.9287 D(x): 0.5914 D(G(z)): 0.3966/0.4022\n",
            "Epoch [111/200] Batch [250/374] Loss_D: 1.2791 Loss_G: 0.9645 D(x): 0.5845 D(G(z)): 0.4075/0.3865\n",
            "Epoch [111/200] Batch [300/374] Loss_D: 1.3090 Loss_G: 0.9851 D(x): 0.5616 D(G(z)): 0.4223/0.3803\n",
            "Epoch [111/200] Batch [350/374] Loss_D: 1.2848 Loss_G: 0.9835 D(x): 0.6301 D(G(z)): 0.4791/0.3813\n",
            "Calculating FID...\n",
            "Epoch [111/200] Val Loss_D: 1.2747 Val Loss_G: 0.6912 FID: 175.5814\n",
            "Epoch [112/200] Batch [0/374] Loss_D: 1.3167 Loss_G: 1.3650 D(x): 0.6801 D(G(z)): 0.5136/0.2605\n",
            "Epoch [112/200] Batch [50/374] Loss_D: 1.2532 Loss_G: 1.1018 D(x): 0.6288 D(G(z)): 0.4468/0.3418\n",
            "Epoch [112/200] Batch [100/374] Loss_D: 1.2578 Loss_G: 0.7889 D(x): 0.5874 D(G(z)): 0.4098/0.4605\n",
            "Epoch [112/200] Batch [150/374] Loss_D: 1.2672 Loss_G: 1.0600 D(x): 0.6349 D(G(z)): 0.4523/0.3544\n",
            "Epoch [112/200] Batch [200/374] Loss_D: 1.2751 Loss_G: 0.9019 D(x): 0.5616 D(G(z)): 0.3920/0.4126\n",
            "Epoch [112/200] Batch [250/374] Loss_D: 1.2867 Loss_G: 0.9638 D(x): 0.5233 D(G(z)): 0.3961/0.3883\n",
            "Epoch [112/200] Batch [300/374] Loss_D: 1.2711 Loss_G: 0.8057 D(x): 0.5986 D(G(z)): 0.4357/0.4516\n",
            "Epoch [112/200] Batch [350/374] Loss_D: 1.2636 Loss_G: 1.1466 D(x): 0.6349 D(G(z)): 0.4482/0.3245\n",
            "Calculating FID...\n",
            "Epoch [112/200] Val Loss_D: 1.2638 Val Loss_G: 0.9049 FID: 178.7756\n",
            "Epoch [113/200] Batch [0/374] Loss_D: 1.3009 Loss_G: 0.9215 D(x): 0.5691 D(G(z)): 0.4322/0.4061\n",
            "Epoch [113/200] Batch [50/374] Loss_D: 1.2665 Loss_G: 0.8383 D(x): 0.5727 D(G(z)): 0.4252/0.4413\n",
            "Epoch [113/200] Batch [100/374] Loss_D: 1.2579 Loss_G: 0.9193 D(x): 0.6035 D(G(z)): 0.4335/0.4044\n",
            "Epoch [113/200] Batch [150/374] Loss_D: 1.2708 Loss_G: 0.9153 D(x): 0.5810 D(G(z)): 0.3939/0.4065\n",
            "Epoch [113/200] Batch [200/374] Loss_D: 1.2697 Loss_G: 1.0573 D(x): 0.6033 D(G(z)): 0.4442/0.3536\n",
            "Epoch [113/200] Batch [250/374] Loss_D: 1.3053 Loss_G: 1.2817 D(x): 0.6785 D(G(z)): 0.5040/0.2834\n",
            "Epoch [113/200] Batch [300/374] Loss_D: 1.2758 Loss_G: 0.7986 D(x): 0.5092 D(G(z)): 0.3687/0.4549\n",
            "Epoch [113/200] Batch [350/374] Loss_D: 1.2806 Loss_G: 0.7481 D(x): 0.5292 D(G(z)): 0.3680/0.4796\n",
            "Calculating FID...\n",
            "Epoch [113/200] Val Loss_D: 1.3057 Val Loss_G: 0.6471 FID: 175.5618\n",
            "Epoch [114/200] Batch [0/374] Loss_D: 1.3342 Loss_G: 1.2167 D(x): 0.6861 D(G(z)): 0.5397/0.3041\n",
            "Epoch [114/200] Batch [50/374] Loss_D: 1.2494 Loss_G: 1.0444 D(x): 0.5929 D(G(z)): 0.4090/0.3592\n",
            "Epoch [114/200] Batch [100/374] Loss_D: 1.2670 Loss_G: 0.8499 D(x): 0.5470 D(G(z)): 0.3846/0.4330\n",
            "Epoch [114/200] Batch [150/374] Loss_D: 1.2689 Loss_G: 0.8042 D(x): 0.5568 D(G(z)): 0.3911/0.4527\n",
            "Epoch [114/200] Batch [200/374] Loss_D: 1.2650 Loss_G: 0.9127 D(x): 0.5828 D(G(z)): 0.4200/0.4081\n",
            "Epoch [114/200] Batch [250/374] Loss_D: 1.3027 Loss_G: 0.9343 D(x): 0.5760 D(G(z)): 0.4182/0.3994\n",
            "Epoch [114/200] Batch [300/374] Loss_D: 1.2914 Loss_G: 1.0561 D(x): 0.6297 D(G(z)): 0.4502/0.3527\n",
            "Epoch [114/200] Batch [350/374] Loss_D: 1.2583 Loss_G: 0.8517 D(x): 0.6354 D(G(z)): 0.4674/0.4310\n",
            "Calculating FID...\n",
            "Epoch [114/200] Val Loss_D: 1.2469 Val Loss_G: 0.9463 FID: 167.2630\n",
            "Epoch [115/200] Batch [0/374] Loss_D: 1.2810 Loss_G: 0.9224 D(x): 0.5507 D(G(z)): 0.3996/0.4046\n",
            "Epoch [115/200] Batch [50/374] Loss_D: 1.2601 Loss_G: 0.8758 D(x): 0.6245 D(G(z)): 0.4483/0.4238\n",
            "Epoch [115/200] Batch [100/374] Loss_D: 1.2758 Loss_G: 0.9428 D(x): 0.6087 D(G(z)): 0.4452/0.3948\n",
            "Epoch [115/200] Batch [150/374] Loss_D: 1.2525 Loss_G: 0.8875 D(x): 0.5949 D(G(z)): 0.4199/0.4185\n",
            "Epoch [115/200] Batch [200/374] Loss_D: 1.2793 Loss_G: 0.9289 D(x): 0.5844 D(G(z)): 0.4244/0.3994\n",
            "Epoch [115/200] Batch [250/374] Loss_D: 1.2408 Loss_G: 0.9665 D(x): 0.6304 D(G(z)): 0.4478/0.3893\n",
            "Epoch [115/200] Batch [300/374] Loss_D: 1.3261 Loss_G: 0.7493 D(x): 0.5309 D(G(z)): 0.3530/0.4779\n",
            "Epoch [115/200] Batch [350/374] Loss_D: 1.2730 Loss_G: 0.8378 D(x): 0.6057 D(G(z)): 0.4508/0.4364\n",
            "Calculating FID...\n",
            "Epoch [115/200] Val Loss_D: 1.3022 Val Loss_G: 0.6336 FID: 168.9535\n",
            "Epoch [116/200] Batch [0/374] Loss_D: 1.3093 Loss_G: 0.8908 D(x): 0.6815 D(G(z)): 0.5408/0.4161\n",
            "Epoch [116/200] Batch [50/374] Loss_D: 1.2649 Loss_G: 1.1055 D(x): 0.6304 D(G(z)): 0.4524/0.3382\n",
            "Epoch [116/200] Batch [100/374] Loss_D: 1.2568 Loss_G: 0.9089 D(x): 0.6015 D(G(z)): 0.4299/0.4084\n",
            "Epoch [116/200] Batch [150/374] Loss_D: 1.3116 Loss_G: 0.9266 D(x): 0.6148 D(G(z)): 0.4615/0.4041\n",
            "Epoch [116/200] Batch [200/374] Loss_D: 1.2647 Loss_G: 1.0329 D(x): 0.6020 D(G(z)): 0.4450/0.3617\n",
            "Epoch [116/200] Batch [250/374] Loss_D: 1.2936 Loss_G: 0.9787 D(x): 0.6239 D(G(z)): 0.4749/0.3818\n",
            "Epoch [116/200] Batch [300/374] Loss_D: 1.2856 Loss_G: 0.7999 D(x): 0.5604 D(G(z)): 0.4219/0.4563\n",
            "Epoch [116/200] Batch [350/374] Loss_D: 1.3017 Loss_G: 1.1087 D(x): 0.6197 D(G(z)): 0.4714/0.3364\n",
            "Calculating FID...\n",
            "Epoch [116/200] Val Loss_D: 1.2378 Val Loss_G: 0.8983 FID: 171.1377\n",
            "Epoch [117/200] Batch [0/374] Loss_D: 1.2853 Loss_G: 0.8857 D(x): 0.5632 D(G(z)): 0.4058/0.4180\n",
            "Epoch [117/200] Batch [50/374] Loss_D: 1.2848 Loss_G: 0.9640 D(x): 0.5813 D(G(z)): 0.4426/0.3871\n",
            "Epoch [117/200] Batch [100/374] Loss_D: 1.3061 Loss_G: 1.1255 D(x): 0.6466 D(G(z)): 0.4853/0.3314\n",
            "Epoch [117/200] Batch [150/374] Loss_D: 1.2968 Loss_G: 0.8885 D(x): 0.5067 D(G(z)): 0.3456/0.4163\n",
            "Epoch [117/200] Batch [200/374] Loss_D: 1.2435 Loss_G: 0.9256 D(x): 0.5927 D(G(z)): 0.4196/0.4022\n",
            "Epoch [117/200] Batch [250/374] Loss_D: 1.2762 Loss_G: 0.8876 D(x): 0.5399 D(G(z)): 0.3605/0.4160\n",
            "Epoch [117/200] Batch [300/374] Loss_D: 1.2870 Loss_G: 0.7630 D(x): 0.5488 D(G(z)): 0.4126/0.4719\n",
            "Epoch [117/200] Batch [350/374] Loss_D: 1.2600 Loss_G: 1.0223 D(x): 0.5957 D(G(z)): 0.4294/0.3648\n",
            "Calculating FID...\n",
            "Epoch [117/200] Val Loss_D: 1.2617 Val Loss_G: 0.7931 FID: 171.4860\n",
            "Epoch [118/200] Batch [0/374] Loss_D: 1.2932 Loss_G: 0.9686 D(x): 0.5976 D(G(z)): 0.4668/0.3858\n",
            "Epoch [118/200] Batch [50/374] Loss_D: 1.2725 Loss_G: 0.9432 D(x): 0.6187 D(G(z)): 0.4662/0.3928\n",
            "Epoch [118/200] Batch [100/374] Loss_D: 1.2793 Loss_G: 0.8473 D(x): 0.6253 D(G(z)): 0.4665/0.4342\n",
            "Epoch [118/200] Batch [150/374] Loss_D: 1.2949 Loss_G: 0.7188 D(x): 0.5305 D(G(z)): 0.3773/0.4931\n",
            "Epoch [118/200] Batch [200/374] Loss_D: 1.2739 Loss_G: 0.9972 D(x): 0.6306 D(G(z)): 0.4744/0.3735\n",
            "Epoch [118/200] Batch [250/374] Loss_D: 1.2995 Loss_G: 1.0917 D(x): 0.6039 D(G(z)): 0.4604/0.3422\n",
            "Epoch [118/200] Batch [300/374] Loss_D: 1.3224 Loss_G: 0.7014 D(x): 0.4699 D(G(z)): 0.3134/0.5003\n",
            "Epoch [118/200] Batch [350/374] Loss_D: 1.3054 Loss_G: 0.9816 D(x): 0.6663 D(G(z)): 0.5089/0.3798\n",
            "Calculating FID...\n",
            "Epoch [118/200] Val Loss_D: 1.2719 Val Loss_G: 0.7098 FID: 165.8962\n",
            "Epoch [119/200] Batch [0/374] Loss_D: 1.3233 Loss_G: 1.2050 D(x): 0.6579 D(G(z)): 0.4980/0.3048\n",
            "Epoch [119/200] Batch [50/374] Loss_D: 1.2602 Loss_G: 1.0253 D(x): 0.6169 D(G(z)): 0.4477/0.3650\n",
            "Epoch [119/200] Batch [100/374] Loss_D: 1.2548 Loss_G: 0.8722 D(x): 0.5844 D(G(z)): 0.4055/0.4232\n",
            "Epoch [119/200] Batch [150/374] Loss_D: 1.3570 Loss_G: 0.6894 D(x): 0.4637 D(G(z)): 0.3139/0.5066\n",
            "Epoch [119/200] Batch [200/374] Loss_D: 1.2616 Loss_G: 0.9608 D(x): 0.6156 D(G(z)): 0.4348/0.3893\n",
            "Epoch [119/200] Batch [250/374] Loss_D: 1.2572 Loss_G: 0.9285 D(x): 0.5794 D(G(z)): 0.4266/0.4040\n",
            "Epoch [119/200] Batch [300/374] Loss_D: 1.2846 Loss_G: 0.8960 D(x): 0.5707 D(G(z)): 0.4083/0.4137\n",
            "Epoch [119/200] Batch [350/374] Loss_D: 1.3075 Loss_G: 0.6240 D(x): 0.4966 D(G(z)): 0.3508/0.5398\n",
            "Calculating FID...\n",
            "Epoch [119/200] Val Loss_D: 1.2533 Val Loss_G: 0.9649 FID: 160.3252\n",
            "\n",
            "Early stopping triggered after 20 epochs without improvement.\n",
            "Best FID: 158.5916 at epoch 99\n",
            "Saved checkpoint to checkpoints/epoch_119_model_final.pt\n",
            "Saved image grid to outputs/epoch_119_fake_final.png.\n",
            "Saved image grid to outputs/epoch_119_real_final.png.\n",
            "Saved training curves to outputs/training_curves.png.\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "!python train.py --config configs/final.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJDELkXiZq5X"
      },
      "source": [
        "## Viewing Training Progress with Tensorboard (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eePcvpKuZq5X"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q80BY58Zq5Y"
      },
      "source": [
        "## Evaluating the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Zsbb23LZq5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545e2b24-030b-4e1c-ef0e-1a748d96de5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set random seed to 42.\n",
            "Using device: cuda\n",
            "Loading checkpoint from checkpoints/epoch_99_model_best.pt\n",
            "Loaded model from epoch 99\n",
            "Found 2655 images in data/pokemon-dataset-1000/test\n",
            "Test dataset size: 2655\n",
            "Collecting real images...\n",
            "Using 1000 real images for evaluation\n",
            "Generating 1000 samples...\n",
            "Saving sample images...\n",
            "Saved image grid to eval_outputs/real_eval_samples.png.\n",
            "Saved image grid to eval_outputs/fake_eval_samples.png.\n",
            "Calculating metrics...\n",
            "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Metrics:\n",
            "  FID: 78.9190 (lower is better)\n",
            "  Inception Score: 2.9500 ± 0.1216 (higher is better)\n",
            "  Diversity Score: 59.8087 (higher is better)\n",
            "Evaluating discriminator...\n",
            "Discriminator Stats:\n",
            "  Real images - Mean score: 0.6446, Std: 0.1046\n",
            "  Fake images - Mean score: 0.4750, Std: 0.0767\n",
            "Plotting confusion matrix...\n",
            "Saved confusion matrix to eval_outputs/confusion_matrix_eval.png.\n",
            "Accuracy statistics:\n",
            "  Accuracy: 0.629\n",
            "  Precision: 0.584\n",
            "  Recall: 0.902\n",
            "  F1 Score: 0.709\n",
            "\n",
            "Evaluation complete! Outputs saved to eval_outputs.\n"
          ]
        }
      ],
      "source": [
        "!python eval.py --checkpoint checkpoints/epoch_99_model_best.pt --config configs/final.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG8mO3OrZq5Y"
      },
      "source": [
        "## Download the Collected Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Xa3VMVz7Zq5Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6bca297d-9997-42be-eb79-9ae0ad852aeb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_398f547e-60db-479e-b39c-e69a517081c7\", \"outputs.zip\", 13220254)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_887f2c32-17e8-49ce-92f5-39451489de76\", \"eval_outputs.zip\", 855767)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "shutil.make_archive(\"outputs\", 'zip', \"outputs\")\n",
        "files.download(\"outputs.zip\")\n",
        "shutil.make_archive(\"eval_outputs\", 'zip', \"eval_outputs\")\n",
        "files.download(\"eval_outputs.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Best Model Checkpoint"
      ],
      "metadata": {
        "id": "47wKhYi7A8ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"checkpoints/epoch_99_model_best.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jLR322BFAVXy",
        "outputId": "3c95740f-44d3-428e-bc95-bf6e688d73b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5a919f17-212a-4002-901d-7f37980d0aae\", \"epoch_99_model_best.pt\", 76679283)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Clear all the data to restart training"
      ],
      "metadata": {
        "id": "mBr9ZKw4BAyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MMcttN-NDXu_"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/CSC487-Project/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}